{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SCHOOL\\WSS\\pedagogical-agent\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_gpt = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer_gpt.encode(text))\n",
    "\n",
    "# Step 1: Extract text from webpage\n",
    "def read_webpage(url: str) -> str:\n",
    "    \"\"\"Extract text from a web page and return it as a single string.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch page: {url}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    main_content = soup.find(\"div\", class_=\"h3-wrap-list\")\n",
    "\n",
    "    if main_content:\n",
    "        return main_content.get_text(separator=\"\\n\", strip=True)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Dictionary to store processed web data\n",
    "processed_web_data = {}\n",
    "\n",
    "def process_webpage(url: str, page_name: str):\n",
    "    \"\"\"Process webpage text and split it into chunks using RecursiveCharacterTextSplitter.\"\"\"\n",
    "    \n",
    "    # Extract text from webpage\n",
    "    text = read_webpage(url)\n",
    "    \n",
    "    if not text:\n",
    "        print(f\"No content extracted from {url}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=256,  # Size of chunks (in tokens)\n",
    "        chunk_overlap=24,  # Tokens overlap between chunks\n",
    "        length_function=count_tokens,  # Function to count tokens in each chunk\n",
    "    )\n",
    "\n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.create_documents([text])\n",
    "    chunks = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "    print(f\"Processed {page_name}: {len(chunks)} chunks\")\n",
    "\n",
    "    # Store processed data in dictionary\n",
    "    processed_web_data[page_name] = chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed pandas_cheatsheet: 6 chunks\n",
      "{'pandas_cheatsheet': [\"#\\nIntroduction\\nYouÃ¢\\x80\\x99ll need to import pandas to get started:\\nimport\\npandas\\nas\\npd\\n#\\nCreating DataFrames\\n-\\n-\\npd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\\nFrom a dictionary\\npd.DataFrame(data=[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}])\\nFrom a list of dictionaries\\npd.read_csv('file.csv')\\nFrom a CSV file\\npd.read_excel('file.xlsx')\\nFrom an Excel file\\n#\\nInspecting Data\\n-\\n-\\ndf.head()\\nFirst 5 rows\\ndf.tail()\\nLast 5 rows\\ndf.shape\\nNumber of rows and columns\\ndf.info()\\nInfo on DataFrame\\ndf.describe()\\nSummary statistics\\ndf.columns\\nColumn names\\ndf.index\\nIndex\\ndf.dtypes\\nData types of columns\\n#\\nSelecting Data\\n-\\n-\\ndf['col1']\\nSelect column\", \"Data types of columns\\n#\\nSelecting Data\\n-\\n-\\ndf['col1']\\nSelect column\\ndf[['col1', 'col2']]\\nSelect multiple columns\\ndf.loc[0]\\nSelect row by index\\ndf.loc[:, 'col1']\\nSelect all rows for 'col1'\\ndf.iloc[0]\\nSelect row by position\\ndf.iloc[0, 1]\\nSelect specific value\\ndf[df['col1'] > 2]\\nSelect rows based on condition\\n#\\nData Cleaning\\n-\\n-\\ndf.dropna()\\nDrop rows with any missing values\\ndf.dropna(axis=1)\\nDrop columns with any missing values\\ndf.fillna(0)\\nReplace missing values with 0\\ndf.drop_duplicates()\\nDrop duplicate rows\\ndf.rename(columns={'old_name': 'new_name'})\\nRename columns\\ndf.astype('int')\\nChange data type\\n#\\nAdding/Removing Data\\n-\\n-\\ndf['col3'] = df['col1'] + df['col2']\\nAdd new column\", \"-\\ndf['col3'] = df['col1'] + df['col2']\\nAdd new column\\ndf.drop('col1', axis=1)\\nDrop column\\ndf.append(new_row)\\nAdd new row\\ndf.insert(2, 'new_col', new_data)\\nInsert new column at position 2\\n#\\nCombining Data\\n-\\n-\\npd.concat([df1, df2])\\nConcatenate rows\\npd.concat([df1, df2], axis=1)\\nConcatenate columns\\npd.merge(df1, df2, on='key')\\nMerge DataFrames on key\\npd.merge(df1, df2, left_on='key1', right_on='key2')\\nMerge on different keys\\ndf1.join(df2, lsuffix='_left', rsuffix='_right')\\nJoin DataFrames\\n#\\nAggregating Data\\n-\\n-\\ndf['col1'].sum()\\nSum of values in column\\ndf['col1'].mean()\\nMean of values in column\", \"Sum of values in column\\ndf['col1'].mean()\\nMean of values in column\\ndf['col1'].count()\\nCount of values in column\\ndf['col1'].min()\\nMinimum value in column\\ndf['col1'].max()\\nMaximum value in column\\ndf['col1'].std()\\nStandard deviation\\ndf['col1'].var()\\nVariance\\ndf.groupby('col1').sum()\\nGroup by and sum\\ndf.groupby('col1').mean()\\nGroup by and mean\\ndf.groupby(['col1', 'col2']).count()\\nGroup by multiple columns\\n#\\nApplying Functions\\n-\\n-\\ndf.apply(np.sqrt)\\nApply function to all values\\ndf['col1'].apply(lambda x: x ** 2)\\nApply function to column\\ndf.applymap(str)\\nApply function to DataFrame elements\\ndf['col1'].map({'a': 1, 'b': 2})\\nMap values\\ndf['col1'].replace('a', 1)\\nReplace values\\n#\\nHandling Dates\\n-\\n-\", \"Replace values\\n#\\nHandling Dates\\n-\\n-\\ndf['date'] = pd.to_datetime(df['date'])\\nConvert to datetime\\ndf['year'] = df['date'].dt.year\\nExtract year\\ndf['month'] = df['date'].dt.month\\nExtract month\\ndf['day'] = df['date'].dt.day\\nExtract day\\ndf.set_index('date', inplace=True)\\nSet date as index\\n#\\nInput/Output\\n-\\n-\\ndf.to_csv('file.csv')\\nSave DataFrame to CSV\\ndf = pd.read_csv('file.csv')\\nLoad DataFrame from CSV\\ndf.to_excel('file.xlsx')\\nSave DataFrame to Excel\\ndf = pd.read_excel('file.xlsx')\\nLoad DataFrame from Excel\\nfrom sqlalchemy import create_engine\\nImport SQLAlchemy for SQL operations\\nengine = create_engine('sqlite:///:memory:')\\nCreate SQL engine\\ndf.to_sql('table_name', engine)\\nSave to SQL table\", \"Create SQL engine\\ndf.to_sql('table_name', engine)\\nSave to SQL table\\ndf = pd.read_sql('table_name', engine)\\nLoad from SQL table\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "url = \"https://cheatsheets.zip/pandas\"\n",
    "process_webpage(url, \"pandas_cheatsheet\")\n",
    "\n",
    "# Now you can inspect processed_web_data\n",
    "# pprint.pprint(processed_web_data)\n",
    "print(processed_web_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
