{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3HSuXMODKv5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfSQy7oxCkKb",
        "outputId": "aa81530c-3caa-4f45-800d-1eb5a5234fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.11/dist-packages (2.5.6)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.4)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (75.1.0)\n",
            "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (1.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (5.29.3)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (1.0.1)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (2.2.2)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (2.4.12)\n",
            "Requirement already satisfied: pymilvus.model>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus[model]) (0.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from milvus-lite>=2.4.0->pymilvus) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2025.1)\n",
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus.model>=0.3.0->pymilvus[model]) (4.49.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (from pymilvus.model>=0.3.0->pymilvus[model]) (1.21.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus.model>=0.3.0->pymilvus[model]) (1.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (0.5.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (1.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (4.12.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymilvus pymilvus[model] pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ3-CVP1CsYk",
        "outputId": "7238f353-9b1a-41f8-9f25-1102589a6d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.cornstack.CodeRankEmbed.b84c9f7b3c0e693aae198b938314b55be0972c94.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ],
      "source": [
        "from pymilvus import MilvusClient\n",
        "import numpy as np\n",
        "from pymilvus import model\n",
        "\n",
        "client = MilvusClient(\"./milvus_demo18.db\")\n",
        "client.create_collection(\n",
        "    collection_name=\"demo_collection\",\n",
        "    dimension=768  # The vectors we will use in this demo has 384 dimensions\n",
        ")\n",
        "\n",
        "\n",
        "embedding_fn =  model.dense.SentenceTransformerEmbeddingFunction(\n",
        "    model_name='cornstack/CodeRankEmbed', # Specify the model name\n",
        "    device='cuda:0',\n",
        "    trust_remote_code=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JZ8nH8cbNBPH"
      },
      "outputs": [],
      "source": [
        "# modified ip search to also filter by library metadata\n",
        "\n",
        "from pymilvus import MilvusClient\n",
        "from pymilvus import model\n",
        "import re\n",
        "\n",
        "DISTANCE_THRESHOLD = 0.25\n",
        "\n",
        "# def ip_search(\n",
        "#     queries,\n",
        "#     output_field,\n",
        "#     collection_name,\n",
        "#     distance_threshold=DISTANCE_THRESHOLD,\n",
        "#     embedding_fn=embedding_fn,\n",
        "# ):\n",
        "#     \"\"\"Searches for relevant context and filters by library metadata.\"\"\"\n",
        "\n",
        "#     query_vectors = embedding_fn.encode_documents(queries)\n",
        "#     res = client.search(\n",
        "#         collection_name=collection_name,\n",
        "#         data=query_vectors,\n",
        "#         limit=5,\n",
        "#         output_fields=[\"text\", \"metadata\"],  # Retrieve metadata along with text\n",
        "#     )\n",
        "\n",
        "#     # Extract the target library from the query\n",
        "#     query_text = \" \".join(queries).lower()\n",
        "#     relevant_libraries = [\"matplotlib\", \"numpy\", \"pandas\", \"pytorch\", \"scipy\", \"sklearn\", \"tensorflow\"]\n",
        "#     detected_library = next((lib for lib in relevant_libraries if lib in query_text), None)\n",
        "\n",
        "#     filtered_results = []\n",
        "#     for table in res[0]:\n",
        "#         if table[\"distance\"] < distance_threshold:\n",
        "#             retrieved_library = table.get(\"metadata\", {}).get(\"library\", \"\").lower()\n",
        "\n",
        "#             # Ensure retrieved context matches the detected library\n",
        "#             if detected_library and retrieved_library == detected_library:\n",
        "#                 filtered_results.append(table[\"entity\"][\"text\"])\n",
        "\n",
        "#     return filtered_results if filtered_results else []\n",
        "\n",
        "\n",
        "# ip search function with keyword matching\n",
        "def ip_search(\n",
        "    queries,\n",
        "    output_field,\n",
        "    collection_name,\n",
        "    distance_threshold=0.5,\n",
        "    embedding_fn=embedding_fn,\n",
        "):\n",
        "    \"\"\"Searches for relevant context and filters by library metadata.\"\"\"\n",
        "\n",
        "    # Library keyword mappings\n",
        "    library_keywords = {\n",
        "        \"pandas\": [\"pd\", \"dataframe\", \"df\", \"iloc\", \"loc\", \"series\", \"groupby\", \"merge\", \"concat\", \"pivot\", \"melt\"],\n",
        "        \"numpy\": [\"np\", \"array\", \"ndarray\", \"linspace\", \"arange\", \"zeros\", \"ones\", \"random\", \"linalg\", \"fft\"],\n",
        "        \"matplotlib\": [\"plt\", \"pyplot\", \"figure\", \"subplot\", \"plot\", \"scatter\", \"hist\", \"bar\", \"boxplot\", \"imshow\"],\n",
        "        \"sklearn\": [\"scikit\", \"scikit-learn\", \"classifier\", \"regressor\", \"cluster\", \"pipeline\", \"grid_search\", \"cross_val\", \"metrics\"],\n",
        "        \"scipy\": [\"stats\", \"interpolate\", \"optimize\", \"signal\", \"sparse\", \"spatial\", \"integrate\", \"ode\"],\n",
        "        \"pytorch\": [\"torch\", \"nn\", \"optim\", \"cuda\", \"tensor\", \"autograd\", \"module\", \"dataloader\", \"dataset\"],\n",
        "        \"tensorflow\": [\"tf\", \"keras\", \"estimator\", \"layers\", \"variable\", \"session\", \"placeholder\", \"eager\", \"tpu\", \"gpu\"]\n",
        "    }\n",
        "\n",
        "    query_vectors = embedding_fn.encode_documents(queries)\n",
        "    res = client.search(\n",
        "        collection_name=collection_name,\n",
        "        data=query_vectors,\n",
        "        limit=5,\n",
        "        output_fields=[\"text\", \"metadata\"],\n",
        "    )\n",
        "\n",
        "    # Extract the target library from the query\n",
        "    query_text = \" \".join(queries).lower()\n",
        "    detected_libraries = set()\n",
        "\n",
        "    # Check for direct library mentions\n",
        "    relevant_libraries = list(library_keywords.keys())\n",
        "    for lib in relevant_libraries:\n",
        "        if lib in query_text:\n",
        "            detected_libraries.add(lib)\n",
        "\n",
        "    # Check for keyword matches\n",
        "    for lib, keywords in library_keywords.items():\n",
        "        for keyword in keywords:\n",
        "            # Check for whole word matches to avoid partial matches\n",
        "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', query_text):\n",
        "                detected_libraries.add(lib)\n",
        "                break\n",
        "\n",
        "    filtered_results = []\n",
        "\n",
        "    for table in res[0]:\n",
        "        if table[\"distance\"] < distance_threshold:\n",
        "            retrieved_library = table.get(\"entity\", {}).get(\"metadata\", {}).get(\"library\", \"\").lower()\n",
        "\n",
        "            # Include results if they match any detected library\n",
        "            if detected_libraries and retrieved_library in detected_libraries:\n",
        "                filtered_results.append(table[\"entity\"][\"text\"])\n",
        "\n",
        "    return filtered_results if filtered_results else []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D78t95uNBPJ",
        "outputId": "74eded72-b0c2-4411-ee18-c33cb0bfd4ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed numpy.pdf: 17 chunks\n",
            "Data has 17 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 17, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}\n",
            "Processed scipy.pdf: 16 chunks\n",
            "Data has 16 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 16, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\n",
            "Processed tensorflow.pdf: 14 chunks\n",
            "Data has 14 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 14, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}\n",
            "Processed matplotlib.pdf: 14 chunks\n",
            "Data has 14 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 14, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}\n",
            "Processed pandas.pdf: 24 chunks\n",
            "Data has 24 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 24, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]}\n",
            "Processed sklearn.pdf: 10 chunks\n",
            "Data has 10 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 10, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1818 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed pytorch.pdf: 19 chunks\n",
            "Data has 19 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 19, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]}\n",
            "Processed cheatsheet: 29 chunks\n",
            "Data has 29 entities, each with fields: dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 29, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]}\n"
          ]
        }
      ],
      "source": [
        "# Adding library metadata when reading the files and websites\n",
        "\n",
        "import os\n",
        "import fitz\n",
        "from transformers import GPT2TokenizerFast\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "tokenizer_gpt = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer_gpt.encode(text))\n",
        "\n",
        "# Step 1: Read PDF using PyMuPDF\n",
        "def read_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extract text from the PDF file and return it as a single string.\"\"\"\n",
        "    text = \"\"\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            page = pdf.load_page(page_num)\n",
        "            text += page.get_text(\"text\")  # Extract text from the page\n",
        "    return text\n",
        "\n",
        "def process_pdfs_in_folder(folder_path: str):\n",
        "    \"\"\"Process PDFs, extract text, split into chunks, and store with library metadata.\"\"\"\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=256,\n",
        "        chunk_overlap=24,\n",
        "        length_function=count_tokens,\n",
        "    )\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "            # Extract library name from filename (e.g., \"numpy.pdf\" → \"numpy\")\n",
        "            library_name = file_name.replace(\".pdf\", \"\").lower()\n",
        "\n",
        "            text = read_pdf(file_path)\n",
        "\n",
        "            # Split the text into chunks\n",
        "            chunks = text_splitter.create_documents([text])\n",
        "            chunks = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "            print(f\"Processed {file_name}: {len(chunks)} chunks\")\n",
        "\n",
        "            # Store with metadata\n",
        "            vectors = embedding_fn.encode_documents(chunks)\n",
        "\n",
        "            data = [\n",
        "                {\"id\": i, \"vector\": vectors[i], \"text\": chunks[i], \"metadata\": {\"library\": library_name}}\n",
        "                for i in range(len(vectors))\n",
        "            ]\n",
        "\n",
        "            print(\"Data has\", len(data), \"entities, each with fields: \", data[0].keys())\n",
        "            print(\"Vector dim:\", len(data[0][\"vector\"]))\n",
        "\n",
        "            res = client.insert(collection_name=\"demo_collection\", data=data)\n",
        "            print(res)\n",
        "\n",
        "\n",
        "\n",
        "# Process data from websites\n",
        "def read_webpage(url: str, class_type: str, class_name: str) -> str:\n",
        "    \"\"\"Extract text from a web page and return it as a single string.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Failed to fetch page: {url}\")\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    main_content = soup.find(class_type, class_=class_name)\n",
        "\n",
        "    if main_content:\n",
        "        return main_content.get_text(separator=\"\\n\", strip=True)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def process_webpage(url: str, page_name: str, class_type: str, class_name: str, library_name: str):\n",
        "    \"\"\"Process webpage text, split into chunks, and store with metadata.\"\"\"\n",
        "\n",
        "    text = read_webpage(url, class_type, class_name)\n",
        "\n",
        "    if not text:\n",
        "        print(f\"No content extracted from {url}\")\n",
        "        return\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=256,\n",
        "        chunk_overlap=24,\n",
        "        length_function=count_tokens,\n",
        "    )\n",
        "\n",
        "    # Split text into chunks\n",
        "    chunks = text_splitter.create_documents([text])\n",
        "    chunks = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "    print(f\"Processed {page_name}: {len(chunks)} chunks\")\n",
        "\n",
        "    vectors = embedding_fn.encode_documents(chunks)\n",
        "\n",
        "    data = [\n",
        "        {\"id\": i, \"vector\": vectors[i], \"text\": chunks[i], \"metadata\": {\"library\": library_name}}\n",
        "        for i in range(len(vectors))\n",
        "    ]\n",
        "\n",
        "    print(\"Data has\", len(data), \"entities, each with fields:\", data[0].keys())\n",
        "    print(\"Vector dim:\", len(data[0][\"vector\"]))\n",
        "\n",
        "    res = client.insert(collection_name=\"demo_collection\", data=data)\n",
        "    print(res)\n",
        "\n",
        "\n",
        "def read_csv(file_path: str):\n",
        "    \"\"\"Read CSV file and extract relevant columns.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Ensure necessary columns exist\n",
        "    required_columns = {\"QuestionTitle\", \"QuestionBody\", \"AnswerBody\"}\n",
        "    if not required_columns.issubset(df.columns):\n",
        "        raise ValueError(f\"CSV file must contain columns: {required_columns}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_csv(file_path: str):\n",
        "    \"\"\"Process CSV data and split it into text chunks.\"\"\"\n",
        "    df = read_csv(file_path)\n",
        "\n",
        "    # Initialize text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=256,  # Size of chunks (in tokens)\n",
        "        chunk_overlap=24,  # Overlap between chunks\n",
        "        length_function=count_tokens,\n",
        "    )\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Combine Question and Answer\n",
        "        combined_text = f\"Title: {row['QuestionTitle']}\\n\\nQuestion: {row['QuestionBody']}\\n\\nAnswer: {row['AnswerBody']}\"\n",
        "\n",
        "        # Split the text into chunks\n",
        "        chunks = text_splitter.create_documents([combined_text])\n",
        "        chunks = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "    print(f\"Processed {len(df)} Q&A pairs into {len(all_chunks)} chunks\")\n",
        "\n",
        "    # Embed and insert into vector store\n",
        "    vectors = embedding_fn.encode_documents(all_chunks)\n",
        "\n",
        "    data = [\n",
        "        {\"id\": i, \"vector\": vectors[i], \"text\": all_chunks[i]} for i in range(len(vectors))\n",
        "    ]\n",
        "\n",
        "    print(\"Data has\", len(data), \"entities, each with fields:\", data[0].keys())\n",
        "    print(\"Vector dim:\", len(data[0][\"vector\"]))\n",
        "\n",
        "    res = client.insert(collection_name=\"demo_collection\", data=data)\n",
        "    print(res)\n",
        "\n",
        "\n",
        "# csv_file_path = \"./docs/sklearn_stackoverflow.csv\"\n",
        "# process_csv(csv_file_path)\n",
        "\n",
        "\n",
        "folder_path = \"./docs\"\n",
        "process_pdfs_in_folder(folder_path)\n",
        "\n",
        "url_dict = {\n",
        "    \"https://ipgp.github.io/scientific_python_cheat_sheet/?utm_content=buffer7d821&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer#numpy-import-numpy-as-np\": [\"cheatsheet\", \"section\", \"main-content\", \"numpy\"]\n",
        "}\n",
        "for url, (page_name, class_type, class_name, library_name) in url_dict.items():\n",
        "    process_webpage(url, page_name, class_type, class_name, library_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86jr4rDeeqzI",
        "outputId": "a2ede5c3-c85f-4af9-8c76-56eed125eae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(ip_search([\"How to fetch specific columns from a dataframe?\"], [\"text\"], \"demo_collection\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PwoT0Y2DPmz",
        "outputId": "6893e5b6-2f54-4885-ea6a-30ef8c85a488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets tqdm 'accelerate>=0.26.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "94c3ae13954341ceb98a0338bd3f8aa3",
            "837d0d6f990f412b98fea5b3f3d54352",
            "43becba3ffb34ac99185095a5bb44d47",
            "ac87019dee474645ac6f5afb3aa1861e",
            "438f0dd5e68f4fcc8f652f4fc25794b1",
            "56d71bff1007421aa64d545576686fb2",
            "794bfc9c56c145408d9f24fc1d26ad8e",
            "2905b6a0b2f44ff08dc78cc75bd89abc",
            "15edeb1929a74dbb9a018c6e4954a37a",
            "811e027924724255a414cbbc01119556",
            "40408cb154814f8c9ef670e8e7f5bea9"
          ]
        },
        "id": "pdIAskEhDKPg",
        "outputId": "b5e9fe27-87dc-4c38-a386-0cec4cf3cc39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94c3ae13954341ceb98a0338bd3f8aa3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-7B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-Coder-7B-Instruct\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b0ba0aa361294d9993396e10f2b7b916",
            "aa8f82f90700446f82521543242a02d0",
            "1a3bed8b2d574a09b69c5e1e94bddde7",
            "43f1767f0b6d454e934ae94c4d7111be",
            "9b6d8f6cd6aa4f6eb3e2defbdc4e4d08",
            "8ff5859ac7814b8bbb80d1ca323ba6cb",
            "be19039abb6b4c0abad5e277092ab4c0",
            "b798f9ed23b048ca9b3cb30c4ab499ec",
            "bbf21878646b4eef86f9479a93a9cb91",
            "71f7a7215449449a8033bce40d2aad8f",
            "0c5d384720734c3c8ca376bd55a9a319"
          ]
        },
        "id": "bWdjMzlbW0z3",
        "outputId": "34c4bc8a-2b2e-4e8b-ed4e-735a745bf50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0ba0aa361294d9993396e10f2b7b916"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.embed_tokens.weight is on cuda:0\n",
            "model.layers.0.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.0.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.0.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.0.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.0.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.0.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.0.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.0.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.0.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.0.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.0.input_layernorm.weight is on cuda:0\n",
            "model.layers.0.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.1.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.1.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.1.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.1.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.1.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.1.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.1.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.1.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.1.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.1.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.1.input_layernorm.weight is on cuda:0\n",
            "model.layers.1.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.2.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.2.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.2.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.2.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.2.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.2.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.2.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.2.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.2.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.2.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.2.input_layernorm.weight is on cuda:0\n",
            "model.layers.2.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.3.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.3.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.3.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.3.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.3.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.3.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.3.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.3.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.3.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.3.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.3.input_layernorm.weight is on cuda:0\n",
            "model.layers.3.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.4.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.4.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.4.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.4.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.4.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.4.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.4.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.4.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.4.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.4.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.4.input_layernorm.weight is on cuda:0\n",
            "model.layers.4.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.5.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.5.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.5.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.5.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.5.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.5.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.5.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.5.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.5.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.5.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.5.input_layernorm.weight is on cuda:0\n",
            "model.layers.5.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.6.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.6.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.6.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.6.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.6.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.6.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.6.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.6.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.6.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.6.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.6.input_layernorm.weight is on cuda:0\n",
            "model.layers.6.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.7.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.7.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.7.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.7.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.7.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.7.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.7.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.7.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.7.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.7.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.7.input_layernorm.weight is on cuda:0\n",
            "model.layers.7.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.8.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.8.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.8.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.8.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.8.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.8.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.8.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.8.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.8.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.8.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.8.input_layernorm.weight is on cuda:0\n",
            "model.layers.8.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.9.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.9.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.9.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.9.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.9.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.9.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.9.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.9.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.9.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.9.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.9.input_layernorm.weight is on cuda:0\n",
            "model.layers.9.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.10.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.10.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.10.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.10.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.10.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.10.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.10.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.10.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.10.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.10.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.10.input_layernorm.weight is on cuda:0\n",
            "model.layers.10.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.11.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.11.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.11.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.11.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.11.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.11.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.11.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.11.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.11.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.11.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.11.input_layernorm.weight is on cuda:0\n",
            "model.layers.11.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.12.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.12.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.12.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.12.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.12.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.12.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.12.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.12.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.12.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.12.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.12.input_layernorm.weight is on cuda:0\n",
            "model.layers.12.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.13.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.13.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.13.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.13.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.13.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.13.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.13.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.13.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.13.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.13.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.13.input_layernorm.weight is on cuda:0\n",
            "model.layers.13.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.14.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.14.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.14.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.14.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.14.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.14.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.14.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.14.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.14.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.14.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.14.input_layernorm.weight is on cuda:0\n",
            "model.layers.14.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.15.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.15.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.15.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.15.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.15.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.15.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.15.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.15.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.15.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.15.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.15.input_layernorm.weight is on cuda:0\n",
            "model.layers.15.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.16.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.16.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.16.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.16.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.16.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.16.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.16.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.16.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.16.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.16.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.16.input_layernorm.weight is on cuda:0\n",
            "model.layers.16.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.17.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.17.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.17.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.17.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.17.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.17.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.17.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.17.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.17.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.17.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.17.input_layernorm.weight is on cuda:0\n",
            "model.layers.17.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.18.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.18.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.18.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.18.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.18.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.18.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.18.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.18.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.18.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.18.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.18.input_layernorm.weight is on cuda:0\n",
            "model.layers.18.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.19.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.19.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.19.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.19.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.19.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.19.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.19.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.19.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.19.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.19.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.19.input_layernorm.weight is on cuda:0\n",
            "model.layers.19.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.20.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.20.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.20.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.20.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.20.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.20.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.20.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.20.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.20.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.20.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.20.input_layernorm.weight is on cuda:0\n",
            "model.layers.20.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.21.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.21.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.21.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.21.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.21.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.21.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.21.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.21.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.21.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.21.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.21.input_layernorm.weight is on cuda:0\n",
            "model.layers.21.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.22.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.22.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.22.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.22.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.22.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.22.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.22.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.22.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.22.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.22.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.22.input_layernorm.weight is on cuda:0\n",
            "model.layers.22.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.23.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.23.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.23.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.23.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.23.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.23.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.23.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.23.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.23.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.23.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.23.input_layernorm.weight is on cuda:0\n",
            "model.layers.23.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.24.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.24.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.24.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.24.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.24.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.24.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.24.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.24.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.24.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.24.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.24.input_layernorm.weight is on cuda:0\n",
            "model.layers.24.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.25.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.25.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.25.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.25.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.25.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.25.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.25.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.25.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.25.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.25.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.25.input_layernorm.weight is on cuda:0\n",
            "model.layers.25.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.26.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.26.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.26.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.26.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.26.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.26.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.26.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.26.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.26.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.26.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.26.input_layernorm.weight is on cuda:0\n",
            "model.layers.26.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.27.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.27.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.27.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.27.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.27.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.27.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.27.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.27.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.27.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.27.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.27.input_layernorm.weight is on cuda:0\n",
            "model.layers.27.post_attention_layernorm.weight is on cuda:0\n",
            "model.norm.weight is on cuda:0\n",
            "lm_head.weight is on cuda:0\n",
            "\n",
            "=== GENERATED RESPONSES ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1/1 [00:23<00:00, 23.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 **Prompt 0**\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "📌 **Generated Code:**\n",
            "It looks like you're trying to create a scatter plot using `matplotlib`, but the module isn't installed in your environment. To resolve this issue, you need to install the `matplotlib` package. Here's how you can do it:\n",
            "\n",
            "```bash\n",
            "pip install matplotlib\n",
            "```\n",
            "\n",
            "Once you've installed the package, you should be able to run your script without encountering the `ModuleNotFoundError`. Make sure to import `matplotlib.pyplot` at the beginning of your script.\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "\n",
            "🔹 **Prompt 1**\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "📌 **Generated Code:**\n",
            "To resolve the `ModuleNotFoundError: No module named 'matplotlib'` error, you need to install the `matplotlib` library. You can do this using pip:\n",
            "\n",
            "```bash\n",
            "pip install matplotlib\n",
            "```\n",
            "\n",
            "Once the library is installed, you can proceed with your plotting task. Below is the complete code to create a scatter plot with color based on the x-value and add a colorbar:\n",
            "\n",
            "```python\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "\n",
            "# Data preparation\n",
            "x = np.linspace(1, 10, 10)  # Using linspace for more evenly spaced values\n",
            "y = np.linspace(10, 1, 10)\n",
            "\n",
            "# Creating the plot\n",
            "fig, ax = plt.subplots()\n",
            "\n",
            "# Scatter plot with color based on x-value\n",
            "scatter = ax.scatter(x, y, c=x, cmap='viridis')\n",
            "\n",
            "# Adding a colorbar\n",
            "cbar = plt.colorbar(scatter)\n",
            "cbar.set_label('X Value')\n",
            "\n",
            "# Customizing the plot\n",
            "ax.set_xlabel('X-axis')\n",
            "ax.set_ylabel('Y-axis')\n",
            "ax.set_title('Scatter Plot with Colorbar')\n",
            "\n",
            "# Displaying the plot\n",
            "plt.show()\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "1. **Import Libraries**: Import `matplotlib.pyplot` for plotting and `numpy` for numerical operations.\n",
            "2. **Data Preparation**: Use `np.linspace` to generate evenly spaced values for `x` and `y`.\n",
            "3. **Creating the Plot**: Use `plt.subplots()` to create a figure and axis object.\n",
            "4. **Scatter Plot**: Use `ax.scatter()` to create a scatter plot where the color of each point is determined by its x-value (`c=x`) and uses the 'viridis' colormap.\n",
            "5. **Colorbar**: Add a colorbar using `plt.colorbar()` and label it.\n",
            "6. **Customization**: Set labels for the x-axis and y-axis, and add a title.\n",
            "7. **Displaying the Plot**: Use `plt.show()` to display the plot.\n",
            "\n",
            "This should resolve the error and allow you to create the desired scatter plot with a colorbar.\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "\n",
            "🔹 **Prompt 2**\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "📌 **Generated Code:**\n",
            "To resolve the `ModuleNotFoundError: No module named 'matplotlib'` error, you need to install the `matplotlib` library. This can be done using pip:\n",
            "\n",
            "```bash\n",
            "pip install matplotlib\n",
            "```\n",
            "\n",
            "Once the library is installed, you can proceed with your plotting task. Below is a comprehensive guide and the complete code to create a scatter plot with color based on the x-value and add a colorbar.\n",
            "\n",
            "### Explanation of Steps\n",
            "\n",
            "1. **Prepare Data**: Define the data points for the x and y axes.\n",
            "2. **Create Plot**: Initialize a figure and an axis object.\n",
            "3. **Plot**: Use the `scatter` function to create the scatter plot.\n",
            "4. **Customize Plot**: Set the limits, labels, and title.\n",
            "5. **Add Colorbar**: Map colors to the x-values and add a colorbar.\n",
            "6. **Show Plot**: Display the plot.\n",
            "\n",
            "### Complete Code\n",
            "\n",
            "```python\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "\n",
            "# Step 1: Prepare data\n",
            "x = np.linspace(1, 10, 10)  # Generate 10 evenly spaced values between 1 and 10\n",
            "y = np.linspace(10, 1, 10)  # Generate 10 evenly spaced values between 10 and 1\n",
            "\n",
            "# Step 2: Create plot\n",
            "fig, ax = plt.subplots()\n",
            "\n",
            "# Step 3: Plot\n",
            "scatter = ax.scatter(x, y, c=x, cmap='viridis')  # Map colors to x-values using viridis colormap\n",
            "\n",
            "# Step 4: Customize plot\n",
            "ax.set_xlim(1, 10)\n",
            "ax.set_ylim(1, 10)\n",
            "ax.set_xlabel('X-axis Label')\n",
            "ax.set_ylabel('Y-axis Label')\n",
            "ax.set_title('Scatter Plot with Colorbar')\n",
            "\n",
            "# Step 5: Add colorbar\n",
            "cbar = fig.colorbar(scatter)\n",
            "cbar.set_label('Colorbar Label')\n",
            "\n",
            "# Step 6: Show plot\n",
            "plt.show()\n",
            "```\n",
            "\n",
            "### Explanation of the Code\n",
            "\n",
            "- **Import Libraries**: Import `matplotlib.pyplot` for plotting and `numpy` for generating data.\n",
            "- **Prepare Data**: Use `np.linspace` to generate 10 evenly spaced values between 1 and 10 for both x and y.\n",
            "- **Create Plot**: Use `plt.subplots()` to create a figure and an axis object.\n",
            "- **Plot**: Use `ax.scatter` to create the scatter plot. The `c` parameter maps\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "\n",
            "🔹 **Prompt 3**\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "📌 **Generated Code:**\n",
            "### Explanation\n",
            "\n",
            "To create a scatter plot with `matplotlib`, you need to follow these steps:\n",
            "\n",
            "1. **Prepare Data**: Define your data points for the x-axis and y-axis.\n",
            "2. **Create Plot**: Initialize a figure and an axes object.\n",
            "3. **Plot**: Use the `scatter` method to create the scatter plot.\n",
            "4. **Customize Plot**: Set limits, labels, and other properties.\n",
            "5. **Save Plot**: Optionally save the plot to a file.\n",
            "6. **Show Plot**: Display the plot using `plt.show()`.\n",
            "\n",
            "In your case, you want to create a scatter plot where the color of each point is based on its x-value and add a colorbar to indicate the mapping from x-values to colors.\n",
            "\n",
            "### Solution\n",
            "\n",
            "First, ensure that you have `matplotlib` installed. If not, you can install it using pip:\n",
            "\n",
            "```sh\n",
            "pip install matplotlib\n",
            "```\n",
            "\n",
            "Now, let's write the complete code to achieve your goal:\n",
            "\n",
            "```python\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "\n",
            "# Prepare data\n",
            "x = np.linspace(1, 10, 10)  # Generate 10 evenly spaced values between 1 and 10\n",
            "y = np.linspace(10, 1, 10)  # Generate 10 evenly spaced values between 10 and 1\n",
            "\n",
            "# Create plot\n",
            "fig, ax = plt.subplots()\n",
            "\n",
            "# Scatter plot with color based on x-value\n",
            "c = x  # Color array based on x-values\n",
            "sc = ax.scatter(x, y, c=c, cmap='viridis')\n",
            "\n",
            "# Add a colorbar\n",
            "cbar = fig.colorbar(sc)\n",
            "cbar.set_label('X Value')\n",
            "\n",
            "# Customize plot\n",
            "ax.set_xlabel('X Axis Label')\n",
            "ax.set_ylabel('Y Axis Label')\n",
            "ax.set_title('Scatter Plot with Colorbar')\n",
            "\n",
            "# Show plot\n",
            "plt.show()\n",
            "```\n",
            "\n",
            "### Explanation of the Code\n",
            "\n",
            "1. **Import Libraries**: Import `matplotlib.pyplot` for plotting and `numpy` for generating data.\n",
            "2. **Prepare Data**: Use `np.linspace` to generate 10 evenly spaced values between 1 and 10 for both x and y.\n",
            "3. **Create Plot**: Initialize a figure and an axes object using `plt.subplots()`.\n",
            "4. **Scatter Plot**: Use `ax.scatter` to create the scatter plot. The `c` parameter specifies the color of each point based on its x-value, and `cmap` sets the colormap.\n",
            "5\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--model\",\n",
        "    type=str,\n",
        "    default=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "    help=\"which results to run\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--resume\",\n",
        "    action=\"store_true\",\n",
        "    default=False,\n",
        "    help=\"where to resume inference\",\n",
        ")\n",
        "\n",
        "# Adding this line to prevent SystemExit:2\n",
        "parser.add_argument('-f')\n",
        "\n",
        "args = parser.parse_args()\n",
        "model_name = args.model\n",
        "\n",
        "cached_cnt = 0\n",
        "if os.path.exists(f\"data/{args.model.replace('/', '-')}-answers.jsonl\"):\n",
        "    if args.resume:\n",
        "        cached_cnt = len(open(f\"data/{args.model.replace('/', '-')}-answers.jsonl\", \"r\").readlines())\n",
        "    else:\n",
        "        exit(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=\"auto\", resume_download=True, trust_remote_code=True\n",
        ")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name} is on {param.device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# Need to set the padding token to the eos token for generation\n",
        "if tokenizer.eos_token:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "else:\n",
        "    tokenizer.add_special_tokens({\n",
        "        \"pad_token\": \"<pad>\"\n",
        "    })\n",
        "\n",
        "test_file = \"test_queries.json\"\n",
        "# with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(test_prompts, f, indent=2)\n",
        "\n",
        "# 3. Load the test data\n",
        "with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "batch_size = 4\n",
        "pad_to_multiple_of = 8\n",
        "formatted_prompts = [test_data[i:i+batch_size] for i in range(0, len(test_data), batch_size)]\n",
        "\n",
        "\n",
        "# Apply padding on the left since we are doing generation\n",
        "padding_side_default = tokenizer.padding_side\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "\n",
        "with open(\"help_levels.json\", \"r\") as f:\n",
        "    help_levels = json.load(f)\n",
        "\n",
        "# Tokenize each batch\n",
        "tokenized_prompts = []\n",
        "for batch in formatted_prompts:\n",
        "    messages = [\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant and an expert Python data scientist.\\n\\n\"\n",
        "                    \"Guidelines:\\n\"\n",
        "                    + \"\\n\".join(f\"- {rule}\" for rule in help_levels.get(query['payload']['help_level'], help_levels['default'])['guidelines']) +\n",
        "                    \"\\n\\n\"\n",
        "                    \"1. **Understand the question fully** before responding.\\n\"\n",
        "                    \"2. **Check if retrieved context is relevant** to the question.\\n\"\n",
        "                    \"   - If it contains useful functions, libraries, or examples, use them.\\n\"\n",
        "                    \"   - If it is unrelated or incorrect, **rely on general knowledge** instead.\\n\"\n",
        "                    \"3. **Provide a clear explanation first**, before giving the code.\\n\"\n",
        "                    \"4. If the error info is available, **analyze the error message** and **provide a solution** appropriate to the help level.\\n\"\n",
        "                    \"5. **Ensure correctness, completeness, and efficiency** in the code.\\n\"\n",
        "                    \"6. **Double-check variable names and constraints** before providing the answer.\"\n",
        "                ),\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"Help Level: {query['payload']['help_level']}\\n\"\n",
        "                    f\"Error Info: {query['payload']['error_info']}\\n\\n\"\n",
        "                    f\"{query['payload']['message_content']}\\n\"\n",
        "                    \"<context>\\n\"\n",
        "                    + \"\\n\\n\".join(ip_search([query[\"payload\"][\"message_content\"]], [\"text\"], \"demo_collection\")) +\n",
        "                    \"\\n</context>\"\n",
        "                ),\n",
        "            }\n",
        "        ]\n",
        "        for query in batch\n",
        "    ]\n",
        "    # for query in batch:\n",
        "    #     print(query[\"payload\"][\"message_content\"])\n",
        "    #     print(query[\"payload\"][\"error_info\"])\n",
        "    # Generate template text\n",
        "    text_inputs = [\n",
        "        tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages\n",
        "    ]\n",
        "    # Tokenize the formatted prompts\n",
        "    tokenized_batch = tokenizer(\n",
        "        text_inputs, return_token_type_ids=False, padding=True, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized_prompts.append(tokenized_batch)\n",
        "\n",
        "# print(\"HIHIHIHIHIHIHIHIHIHIHIHIHIHIHIHIHIHIH\")\n",
        "# for batch in formatted_prompts:\n",
        "#   for query in batch:\n",
        "#     print(query[\"payload\"][\"message_content\"])\n",
        "#     print(query[\"payload\"][\"error_info\"])\n",
        "#     print(query[\"payload\"][\"help_level\"])\n",
        "# Restore the original padding behavior\n",
        "tokenizer.padding_side = padding_side_default\n",
        "model_name = model_name.replace('/', '-')\n",
        "\n",
        "generation_config = {\n",
        "    \"do_sample\": False,\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"num_beams\": 1\n",
        "}\n",
        "\n",
        "print(\"\\n=== GENERATED RESPONSES ===\\n\")\n",
        "for batch in tqdm(tokenized_prompts):\n",
        "    # Move the batch to the device\n",
        "    batch = batch.to(\"cuda\")\n",
        "    prompt_len = len(batch[\"input_ids\"][0])\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **batch,\n",
        "            **generation_config\n",
        "        )\n",
        "    generated_text = tokenizer.batch_decode(outputs[:, prompt_len:], skip_special_tokens=True)\n",
        "    for i, code in enumerate(generated_text):\n",
        "        stop_words = [\"</code>\", \"# SOLUTION END\"]\n",
        "        for stop_word in stop_words:\n",
        "            code = code.split(stop_word)[0]\n",
        "        print(f\"\\n🔹 **Prompt {cached_cnt + i}**\")\n",
        "        print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
        "        print(f\"📌 **Generated Code:**\\n{code}\")\n",
        "        print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
        "    cached_cnt += len(generated_text)  # Update count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IsoIweTLC6vc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# test_prompts = [\n",
        "#     {\n",
        "#         \"payload\": {\n",
        "#             \"message_content\": \"How do I create a pandas DataFrame?\",\n",
        "#             \"error_info\": \"not available\",\n",
        "#             \"help_level\": \"default\"\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         \"payload\": {\n",
        "#             \"message_content\": \"How do I plot using matplotlib?\",\n",
        "#             \"error_info\": \"Error description: ModuleNotFoundError: No module named 'matplotlib'\",\n",
        "#             \"help_level\": \"hint\"\n",
        "#         }\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "# test_data = test_prompts\n",
        "\n",
        "# Loading the test data from a JSON file\n",
        "# test_file = \"test_queries.json\"\n",
        "# # with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
        "# #     json.dump(test_prompts, f, indent=2)\n",
        "\n",
        "# # 3. Load the test data\n",
        "# with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
        "#     test_data = json.load(f)\n",
        "\n",
        "# batch_size = 4\n",
        "# pad_to_multiple_of = 8\n",
        "# formatted_prompts = [test_data[i:i+batch_size] for i in range(0, len(test_data), batch_size)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZPaVYUafjCE"
      },
      "outputs": [],
      "source": [
        "# # Apply padding on the left since we are doing generation\n",
        "# padding_side_default = tokenizer.padding_side\n",
        "# tokenizer.padding_side = \"left\"\n",
        "\n",
        "# # Tokenize each batch\n",
        "# tokenized_prompts = []\n",
        "# for batch in formatted_prompts:\n",
        "#     messages = [\n",
        "#     [\n",
        "#         {\"role\": \"system\", \"content\": \"\"\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant and an expert Python data scientist. Before answering, take a deep breath and follow these guidelines:\n",
        "#           1. **Understand the question fully** before responding.\n",
        "#           2. **Check if retrieved context is relevant** to the question.\n",
        "#               - If it contains useful functions, libraries, or examples, use them.\n",
        "#               - If it is unrelated or incorrect, **rely on general knowledge** instead.\n",
        "#           3. **Provide a clear explanation first**, before giving the code.\n",
        "#           4. **Adapt your response based on the help level**:\n",
        "#               - If help_level is 'hint': Provide minimal guidance with subtle hints to encourage learning. Don't solve the entire problem.\n",
        "#               - If help_level is 'guided': Provide clearer direction with partial solutions and explanations.\n",
        "#               - If help_level is 'comprehensive': Provide detailed, complete solutions with thorough explanations.\n",
        "#               - If help_level is 'default': Provide a balanced response with explanation and solution.\n",
        "#           5. If the error info is available, **analyze the error message** and **provide a solution** appropriate to the help level.\n",
        "#           6. **Ensure correctness, completeness, and efficiency** in the code.\n",
        "#           7. **Double-check variable names and constraints** before providing the answer.\n",
        "#           \"\"\"},\n",
        "#         {\"role\": \"user\", \"content\": query[\"payload\"][\"message_content\"] +\n",
        "#                                    \"\\n<context>\\n\" +\n",
        "#                                    \"\\n\\n\".join(ip_search([query[\"payload\"][\"message_content\"]], [\"text\"], \"demo_collection\")) +\n",
        "#                                    \"\\n</context>\\n<error_info>\" +\n",
        "#                                    query[\"payload\"][\"error_info\"] + \"</error_info>\\n<help_level>\" +\n",
        "#                                    query[\"payload\"][\"help_level\"] + \"</help_level>\"}\n",
        "#     ]\n",
        "#     for query in batch\n",
        "# ]\n",
        "# for query in batch:\n",
        "#     print(query[\"payload\"][\"message_content\"])\n",
        "#     print(query[\"payload\"][\"error_info\"])\n",
        "# # Generate template text\n",
        "# text_inputs = [\n",
        "#     tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages\n",
        "# ]\n",
        "\n",
        "# # Tokenize the formatted prompts\n",
        "# tokenized_batch = tokenizer(\n",
        "#     text_inputs, return_token_type_ids=False, padding=True, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\"\n",
        "# )\n",
        "# tokenized_prompts.append(tokenized_batch)\n",
        "\n",
        "# # Restore the original padding behavior\n",
        "# tokenizer.padding_side = padding_side_default\n",
        "# model_name = model_name.replace('/', '-')\n",
        "\n",
        "# generation_config = {\n",
        "#     \"do_sample\": False,\n",
        "#     \"max_new_tokens\": 512,\n",
        "#     \"num_beams\": 1\n",
        "# }\n",
        "\n",
        "# print(\"\\n=== GENERATED RESPONSES ===\\n\")\n",
        "# for batch in tqdm(tokenized_prompts):\n",
        "#     # Move the batch to the device\n",
        "#     batch = batch.to(\"cuda\")\n",
        "#     prompt_len = len(batch[\"input_ids\"][0])\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             **batch,\n",
        "#             **generation_config\n",
        "#         )\n",
        "#     generated_text = tokenizer.batch_decode(outputs[:, prompt_len:], skip_special_tokens=True)\n",
        "#     for i, code in enumerate(generated_text):\n",
        "#         stop_words = [\"</code>\", \"# SOLUTION END\"]\n",
        "#         for stop_word in stop_words:\n",
        "#             code = code.split(stop_word)[0]\n",
        "#         print(f\"\\n🔹 **Prompt {cached_cnt + i}**\")\n",
        "#         print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
        "#         print(f\"📌 **Generated Code:**\\n{code}\")\n",
        "#         print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
        "#     cached_cnt += len(generated_text)  # Update count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aAneQqAEEM3"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import json\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from datasets import load_dataset\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Set model name and resume behavior directly in the code\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "# resume = True  # Set to True to resume from the last cached count\n",
        "\n",
        "# # Initialize the model and tokenizer\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name, device_map=\"auto\", resume_download=True, trust_remote_code=True\n",
        "# )\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"{name} is on {param.device}\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# if tokenizer.eos_token:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "# else:\n",
        "#     tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
        "\n",
        "# # Load dataset\n",
        "# ds1000 = list(load_dataset(\"xlangai/DS-1000\")[\"test\"])\n",
        "# prompts = [p[\"prompt\"] for p in ds1000]\n",
        "\n",
        "# # Check for cached answers if resume is enabled\n",
        "# cached_cnt = 0\n",
        "# #output_path = f'{model_name.replace(\"/\", \"-\")}-answers.jsonl'\n",
        "# output_path=f\"./data/{model_name.replace('/', '-')}-answers.jsonl\"\n",
        "# if resume and os.path.exists(output_path):\n",
        "#     cached_cnt = len(open(output_path, \"r\").readlines())\n",
        "\n",
        "# prompts = prompts[cached_cnt:]  # Resume from the last processed prompt if applicable\n",
        "\n",
        "# # Set parameters\n",
        "# batch_size = 2\n",
        "# pad_to_multiple_of = 8\n",
        "\n",
        "# # Split prompts into batches\n",
        "# formatted_prompts = [prompts[i: i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "# print(\"cached_cnt\", cached_cnt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNtN1kxzG_68"
      },
      "outputs": [],
      "source": [
        "# # Tokenize and process each batch\n",
        "# cached_cnt=0\n",
        "# tokenizer.padding_side = \"left\"\n",
        "# tokenized_prompts = []\n",
        "# for formatted_prompt in formatted_prompts:\n",
        "#     messages = [\n",
        "#         [\n",
        "#             {\"role\": \"system\", \"content\":  \"\"\"\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. Before answering, take a deep breath and answer based on the following guidelines:\n",
        "#               1. Use the provided context ONLY if relevant\n",
        "#               2. If the context doesn't help, rely on your general knowledge\n",
        "#               3. Answer the question, EXACTLY ACCORDING TO HOW IT IS ASKED, that is, only provide the neccessary code needed to add to the user's code for it to work.\n",
        "#               4, DO NOT give any explanation, only the code is required. For example, \"Problem:\\nHow do I get the dimensions of an array? For instance, this is (2, 2):\\na = np.array([[1,2],[3,4]])\\n\\nA:\\n<code>\\nimport numpy as np\\na = np.array([[1,2],[3,4]])\\n</code>\\nresult = ... \", your answer should be \"result = a.shape\".\n",
        "\n",
        "#               \"\"\"},\n",
        "#             {\"role\": \"user\", \"content\": prompt + \"\\n<context>\\n\" + \"\\n\\n\".join(ip_search([formatted_prompt[0]], [\"text\"], \"demo_collection\"))+ \"\\n</context>\\n\"}\n",
        "#         ]\n",
        "#         for prompt in formatted_prompt\n",
        "#     ]\n",
        "#     text_inputs = [\n",
        "#         tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages\n",
        "#     ]\n",
        "#     tokenized_batch = tokenizer(\n",
        "#         text_inputs, return_token_type_ids=False, padding=True, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\", max_length=2048\n",
        "#     )\n",
        "#     tokenized_prompts.append(tokenized_batch)\n",
        "# tokenizer.padding_side = \"right\"  # Reset padding side\n",
        "\n",
        "# # Generation configuration\n",
        "# generation_config = {\n",
        "#     \"do_sample\": False,\n",
        "#     \"max_new_tokens\": 1024,\n",
        "#     \"num_beams\": 1\n",
        "# }\n",
        "\n",
        "# # Run inference and save outputs\n",
        "\n",
        "# with open(output_path, 'a+') as f:\n",
        "#     for batch in tqdm(tokenized_prompts):\n",
        "#         batch = batch.to(\"cuda\")\n",
        "#         prompt_len = len(batch[\"input_ids\"][0])\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model.generate(**batch, **generation_config)\n",
        "#         generated_text = tokenizer.batch_decode(outputs[:, prompt_len:], skip_special_tokens=True)\n",
        "\n",
        "#         for code in generated_text:\n",
        "#             stop_words = [\"</code>\", \"# SOLUTION END\"]\n",
        "#             for stop_word in stop_words:\n",
        "#                 code = code.split(stop_word)[0]\n",
        "#             result = {\n",
        "#                 'id': cached_cnt,\n",
        "#                 'code': code,\n",
        "#                 'metadata': ds1000[cached_cnt]['metadata']\n",
        "#             }\n",
        "#             f.write(json.dumps(result) + '\\n')\n",
        "#             cached_cnt += 1\n",
        "\n",
        "# print(f\"Results saved in {output_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94c3ae13954341ceb98a0338bd3f8aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_837d0d6f990f412b98fea5b3f3d54352",
              "IPY_MODEL_43becba3ffb34ac99185095a5bb44d47",
              "IPY_MODEL_ac87019dee474645ac6f5afb3aa1861e"
            ],
            "layout": "IPY_MODEL_438f0dd5e68f4fcc8f652f4fc25794b1"
          }
        },
        "837d0d6f990f412b98fea5b3f3d54352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56d71bff1007421aa64d545576686fb2",
            "placeholder": "​",
            "style": "IPY_MODEL_794bfc9c56c145408d9f24fc1d26ad8e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "43becba3ffb34ac99185095a5bb44d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2905b6a0b2f44ff08dc78cc75bd89abc",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15edeb1929a74dbb9a018c6e4954a37a",
            "value": 4
          }
        },
        "ac87019dee474645ac6f5afb3aa1861e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_811e027924724255a414cbbc01119556",
            "placeholder": "​",
            "style": "IPY_MODEL_40408cb154814f8c9ef670e8e7f5bea9",
            "value": " 4/4 [00:06&lt;00:00,  1.43s/it]"
          }
        },
        "438f0dd5e68f4fcc8f652f4fc25794b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56d71bff1007421aa64d545576686fb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "794bfc9c56c145408d9f24fc1d26ad8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2905b6a0b2f44ff08dc78cc75bd89abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15edeb1929a74dbb9a018c6e4954a37a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "811e027924724255a414cbbc01119556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40408cb154814f8c9ef670e8e7f5bea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0ba0aa361294d9993396e10f2b7b916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa8f82f90700446f82521543242a02d0",
              "IPY_MODEL_1a3bed8b2d574a09b69c5e1e94bddde7",
              "IPY_MODEL_43f1767f0b6d454e934ae94c4d7111be"
            ],
            "layout": "IPY_MODEL_9b6d8f6cd6aa4f6eb3e2defbdc4e4d08"
          }
        },
        "aa8f82f90700446f82521543242a02d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ff5859ac7814b8bbb80d1ca323ba6cb",
            "placeholder": "​",
            "style": "IPY_MODEL_be19039abb6b4c0abad5e277092ab4c0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1a3bed8b2d574a09b69c5e1e94bddde7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b798f9ed23b048ca9b3cb30c4ab499ec",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbf21878646b4eef86f9479a93a9cb91",
            "value": 4
          }
        },
        "43f1767f0b6d454e934ae94c4d7111be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71f7a7215449449a8033bce40d2aad8f",
            "placeholder": "​",
            "style": "IPY_MODEL_0c5d384720734c3c8ca376bd55a9a319",
            "value": " 4/4 [00:14&lt;00:00,  3.22s/it]"
          }
        },
        "9b6d8f6cd6aa4f6eb3e2defbdc4e4d08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ff5859ac7814b8bbb80d1ca323ba6cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be19039abb6b4c0abad5e277092ab4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b798f9ed23b048ca9b3cb30c4ab499ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbf21878646b4eef86f9479a93a9cb91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71f7a7215449449a8033bce40d2aad8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c5d384720734c3c8ca376bd55a9a319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}