{"id": 0, "code": "df = df.set_index('Type').T.loc[List].T.reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "df_shuffled = df.iloc[List]\ndifferent_rows = np.sum(df['Type'] != df_shuffled['Type'])\nresult = different_rows\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "for col in df.columns:\n    freq = df[col].value_counts()\n    less_than_2 = freq[freq < 2].index\n    df[col] = df[col].replace(less_than_2, 'other')\n\nresult = df\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "for col in df.columns:\n    mask = df[col].map(df[col].value_counts()) < 3\n    df.loc[mask, col] = 'other'\nresult = df\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": "value_counts = df.Qu1.value_counts()\nto_replace = value_counts[value_counts < 2].index\ndf.replace(to_replace=to_replace, value='other', inplace=True)\nreturn df\n\nThis function first calculates the value counts for column 'Qu1'. It then identifies the values that appear less than 2 times and stores them in the variable to_replace. The replace function is then used to replace these values with 'other'. The inplace=True argument ensures that the changes are made directly to the dataframe.", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "for col in df.columns:\n    freq = df[col].value_counts()\n    df[col] = df[col].map(lambda x: 'other' if freq.loc[x] < 2 else x)\n    df[col] = df[col].map(lambda x: 'other' if freq.loc[x] < 3 and col == 'Qu1' else x)\n\nresult = df\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "for col in df.columns:\n    freq = df[col].value_counts()\n    df[col] = df[col].map(lambda x: 'other' if (freq[x] < 2 and col == 'Qu2') or (freq[x] < 3 and col == 'Qu1') else x)\n\nresult = df\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "df['id'] = range(1, len(df) + 1)\ndf = df.sort_values(by=['url', 'keep_if_dup'], ascending=[True, False]).drop_duplicates(subset='url', keep='first')\nresult = df\n", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "df['id'] = range(1, len(df) + 1)\ndf = df.sort_values(by=['url', 'drop_if_dup'], ascending=[True, False]).drop_duplicates(subset='url', keep='first')\n", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "df['id'] = range(1, len(df) + 1)\ndf = df.sort_values(['url', 'keep_if_dup', 'id'], ascending=[True, False, False]).drop_duplicates('url', keep='first')\ndf = df.sort_values('id').reset_index(drop=True)\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "result = df.groupby('name').apply(lambda x: {k: g['v3'].to_dict() for k, g in x.groupby('v1')})\nresult = result.to_dict()\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": "You can use the dt accessor with tz_localize(None) to remove the timezone information:\n\n```python\nexample_df['datetime'] = example_df['datetime'].dt.tz_localize(None)\n```\n\nThis will convert the timezone-aware datetime objects to timezone-naive ones.\n\nPlease note that this will remove the timezone information, but the timestamp will still be in the UTC timezone. If you want to convert it to another timezone, you can use the tz_convert method:\n\n```python\nfrom pytz import timezone\nexample_df['datetime'] = example_df['datetime'].dt.tz_convert(timezone('America/New_York'))\n```\n\nThis will convert the timestamps to the New York timezone.", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "import ast\n\ndef extract_dict(row):\n    message_dict = ast.literal_eval(row['message'])\n    row['job'] = message_dict.get('job', 'none')\n    row['money'] = message_dict.get('money', 'none')\n    row['wife'] = message_dict.get('wife', 'none')\n    row['group'] = message_dict.get('group', 'none')\n    row['kids'] = message_dict.get('kids', 'none')\n    return row\n\nresult = df.apply(extract_dict, axis=1)\nresult = result.drop('message', axis=1)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "df.loc[df['product'].isin(products), 'score'] *= 10\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df.loc[~df['product'].isin(products), 'score'] *= 10\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "for product_range in products:\n    df.loc[df['product'].between(product_range[0], product_range[1]), 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "df.loc[df['product'].isin(products), 'score'] = (df.loc[df['product'].isin(products), 'score'] - df.loc[df['product'].isin(products), 'score'].min()) / (df.loc[df['product'].isin(products), 'score'].max() - df.loc[df['product'].isin(products), 'score'].min())\n", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = df.apply(lambda row: list(row[row==1].index)[0], axis=1)\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df['category'] = df.apply(lambda row: next((col for col, val in row.items() if val == 0), None), axis=1)\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "df['category'] = df.apply(lambda row: [col for col, val in row.iteritems() if val == 1], axis=1)\ndf\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%Y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "df = df.shift(1).fillna(df.iloc[-1])\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "df = pd.concat([df.iloc[-1:, :], df.iloc[:-1, :]])\ndf", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "df = df.shift(-1).fillna(df.iloc[0])\ndf.iloc[-1] = df.iloc[0]\ndf = df.shift(1).fillna(df.iloc[-1])\ndf.iloc[0] = df.iloc[-1]\n", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "df = df.shift(1).iloc[-1:].append(df.shift(1).iloc[:-1])\ndf\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [col+'X' for col in df.columns]\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.columns = ['X' + col for col in df.columns]\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "df.columns = [col+'X' if col[-1] != 'X' else col for col in df.columns]\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "result = df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' in col})\nresult['group_color'] = df.groupby('group')['group_color'].transform('first')\nresult\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "value_cols = df.columns[df.columns.str.startswith('val')]\nresult = df.groupby('group').agg({\"group_color\": \"first\"}).join(df.groupby('group')[value_cols].sum())\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "def custom_agg(x):\n    if x.name.endswith('2'):\n        return x.mean()\n    else:\n        return x.sum()\n\nresult = df.groupby('group').agg({col: custom_agg for col in df.columns if col != 'group' and col != 'group_color'})\nresult['group_color'] = df.groupby('group')['group_color'].first()\nresult\n", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "result = df.loc[row_list, column_list].mean()\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "result = df.loc[row_list, column_list].sum()\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "result = df.loc[row_list, column_list].sum()\n", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "result = df.apply(pd.Series.value_counts).fillna(0)\n", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\n", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "for col in df.columns:\n    print(\"---- %s ---\" % col)\n    print(df[col].value_counts())\n", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "df.columns = df.loc[0].astype(str) + '_' + df.loc[1].astype(str)\ndf = df.iloc[2:]\ndf.columns = df.columns.str.strip('_nan')\ndf = df.reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "df.columns = df.loc[0].astype(str) + '_' + df.loc[1].astype(str)\ndf = df.iloc[2:]\ndf.columns = df.columns.str.strip('_nan')\ndf = df.reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "df = df.fillna(method='ffill', axis=1)\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "result = df.where(df.notnull(), df.shift(1).fillna(df))\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "df = df.where(df.notnull(), df.iloc[::-1])\n", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "mask = df['value'] < thresh\ndf.loc['X'] = df.loc[mask, 'value'].sum()\ndf = df.loc[~mask]\nresult = df\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "mask = df['value'] > thresh\ndf.loc[mask, 'value'] = df.loc[mask, 'value'].mean()\ndf.loc['X'] = df['value'].sum()\nresult = df\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "mask = (df['value'] < section_left) | (df['value'] > section_right)\navg = df.loc[~mask, 'value'].mean()\ndf = df.loc[mask, :]\ndf.loc['X'] = avg\nresult = df\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "result = 1/df.add_prefix('inv_')\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "import numpy as np\n\ndf = pd.concat([df, np.exp(df).add_prefix('exp_')], axis=1)\nresult = df\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "result = 1 / df.assign(**{f'inv_{col}': 1 / df[col] for col in df.columns})\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nfor col in df.columns:\n    df['sigmoid_'+col] = df[col].apply(sigmoid)\n\nresult = df\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "min_idx = df.idxmin().min()\nmask = df.index.to_series().lt(min_idx).T\nlast_max_idx = df.where(mask).idxmax()\nresult = last_max_idx\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "min_idx = df.idxmin()\nmask = df.index.to_series().lt(min_idx).T\nfirst_max_idx = df.where(mask).idxmax()\nresult = first_max_idx\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "df = df.set_index('dt').groupby('user').resample('D').ffill().reset_index().fillna(0)\nresult = df\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "df = df.set_index('dt').groupby('user').resample('D').ffill().reset_index().fillna(0)\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "df = df.set_index('dt').groupby('user').apply(lambda x: x.set_index('dt').reindex(pd.date_range(x.index.min(), x.index.max()), fill_value=233).reset_index())\nresult = df.reset_index(drop=True)\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "df = df.set_index(['user', 'dt']).groupby(level=[0])['val'].max().reset_index()\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf['dt'] = pd.date_range(start=df['dt'].min(), end=df['dt'].max())\ndf = df.groupby(['user', 'dt']).max().reset_index()\nresult = df\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "df = df.set_index(['user', 'dt']).groupby(level=0).apply(lambda x: x.set_index('dt', append=True)).reset_index().drop(columns=['level_1'], errors='ignore')\ndf['dt'] = df['dt'].dt.strftime('%d-%b-%Y')\nresult = df\n", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "df['name'] = df.groupby('name').ngroup() + 1\nresult = df\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "df['a'] = df.groupby('name').ngroup() + 1\nresult = df\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": "You can use the pandas function `factorize` to convert the 'name' column into a numerical categorical representation.\n\n```python\ndef f(df):\n    df['name'], _ = pd.factorize(df['name'])\n    return df\n\nresult = f(example_df)\n```\n\nThis will replace each unique name with a unique integer.\n\nNote: The underscore `_` is a throwaway variable here, because `factorize` returns two values: the first is the new categorical variable, and the second is the original, unmodified variable. We only want the first part, so we throw away the second one with the underscore.", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "df['ID'] = df.groupby(['name', 'a']).ngroup()\nresult = df[['ID', 'b', 'c']]\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\n", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.melt(id_vars=['user'], var_name='others', value_name='value')\n", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.dropna()\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "result = df.loc[df['c'] > 0.5, columns]\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "result = df.loc[df['c'] > 0.45, columns]\n", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": "You can use the following code to achieve this:\n\n```python\ndef f(df, columns=['b', 'e']):\n    return df.loc[df['c'] > 0.5, columns].values\n```\n\nThis function uses DataFrame.loc to select rows based on a boolean condition and columns. It then uses .values to convert the resulting DataFrame to a numpy array.\n\nHere's how you can use it:\n\n```python\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nresult = f(df)\nprint(result)\n```\n\nThis will print the array of rows where column 'c' is greater than 0.5, with only columns 'b' and 'e'.", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "You can use the assign method to create a new column that is the sum of 'b' and 'e' for each row. Here's how you can do it:\n\n```python\nresult = df.loc[df['c'] > 0.5, ['b', 'e']].assign(sum=df.loc[df['c'] > 0.5, ['b', 'e']].sum(axis=1))\n```\n\nThis code first selects the rows where 'c' is greater than 0.5 and the columns 'b' and 'e'. Then it creates a new column 'sum' that is the sum of 'b' and 'e' for each row. The result is a new DataFrame that only includes the rows where 'c' is greater than 0.5 and includes the new 'sum' column.", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "You can use the following code to achieve this:\n\n```python\ndef f(df, columns=['b', 'e']):\n    return df.loc[df['c'] > 0.5, columns]\n```\n\nThis function uses DataFrame's `loc` accessor to select rows based on a boolean condition and columns based on a list. The condition `df['c'] > 0.5` selects rows where the value in column 'c' is greater than 0.5. The second argument to `loc` is a list of column labels, so it selects only the specified columns.\n\nYou can call this function with your DataFrame and the desired columns like this:\n\n```python\nresult = f(df)\n```\n\nThis will store the result in the variable `result`.", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "df['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf = df.drop_duplicates(subset='date', keep='first')\nresult = df\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "df['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf = df.drop_duplicates(subset='date', keep='first')\nresult = df\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "df['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf['date_plus_X_weeks'] = df['date'] + pd.DateOffset(weeks=X)\ndf = df.drop_duplicates(subset='date')\ndf = df.drop(df[df['date'] > df['date_plus_X_weeks']].index)\ndf['date'] = df['date'].dt.strftime('%d-%b-%Y')\ndf['date_plus_X_weeks'] = df['date_plus_X_weeks'].dt.strftime('%d-%b-%Y')\ndf = df.drop(columns=['date_plus_X_weeks'])\nresult = df\n", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "df['col1'] = df['col1'].rolling(window=3).mean()\nresult = df.iloc[::3, :]\nresult = result.reset_index(drop=True)\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "df['col1'] = df['col1'].rolling(window=3).sum()\nresult = df[::3]\nresult = result.reset_index(drop=True)\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "df['col1'] = df['col1'].cumsum()\nresult = df.groupby(df.index // 4)['col1'].sum()\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "df['cumsum'] = df['col1'].cumsum()\ndf['group'] = df['cumsum'].floordiv(3)\nresult = df.groupby('group', as_index=False)['col1'].mean()\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "df['group'] = df.index // 5\ndf['remainder'] = df.index % 5\n\ndf['value'] = df['col1']\ndf.loc[df['remainder'].between(0,2), 'value'] = df.groupby('group')['col1'].transform('mean')\ndf.loc[df['remainder'].between(3,4), 'value'] = df.groupby('group')['col1'].transform('sum')\n\nresult = df.groupby('group')['value'].sum().reset_index(drop=True)\nresult", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "def custom_func(x):\n    if len(x) == 3:\n        return x.sum()\n    elif len(x) == 2:\n        return x.mean()\n\nresult = df.groupby(df.index // 5).apply(custom_func).reset_index(drop=True)\n", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].ffill().astype(int)\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df['A'] = df['A'].mask(df['A'] == 0).ffill(1)\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "df['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill').combine(df['A'].fillna(method='bfill'), max)\ndf['A'] = df['A'].fillna(0).astype(int)\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['number'] = df['duration'].str.extract('(\\d+)', expand=False).astype(int)\ndf['time'] = df['duration'].str.extract('([a-zA-Z ]+)', expand=False).str.strip()\ntime_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\ndf['time_days'] = df['time'].map(time_days)\ndf\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df[['time', 'number']] = df['duration'].str.split(' ', expand=True)\ndf['number'] = df['number'].astype(int)\n\ntime_day_dict = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\ndf['time_day'] = df['time'].map(time_day_dict)\ndf\n", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": "import re\n\ndef f(df):\n    df['number'] = df['duration'].apply(lambda x: int(re.findall(r'\\d+', x)[0]))\n    df['time'] = df['duration'].apply(lambda x: re.findall(r'[a-zA-Z]+', x)[0])\n    time_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\n    df['time_days'] = df['time'].map(time_days)\n    return df\n\nresult = f(example_df)\nprint(result)\n\nThis code first extracts the number and the time from the duration column and stores them in two new columns. Then it maps the time to the corresponding number of days in a dictionary and stores the result in a new column.\n\nThe output will be:\n   duration  number   time  time_days\n0    7 year       7   year        365\n1    2day       2    day          1\n2    4 week       4   week          7\n3   8 month       8  month         30\n\nThis code assumes that the duration column always contains a number followed by a word (separated by a space if there is a space). If the format can vary, you might need a more complex regular expression or additional code to handle the different formats.", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df[['time', 'number']] = df['duration'].str.split(' ', expand=True)\ndf['number'] = df['number'].astype(int)\ndf['time_day'] = df['time'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\ndf['time_day'] = df['time_day'] * df['number']\ndf\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = (df1[columns_check_list] != df2[columns_check_list]).any(axis=1).tolist()\n", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "result = (df1[columns_check_list] == df2[columns_check_list]).all(axis=1).tolist()\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": "df.index = pd.to_datetime(df.index.get_level_values('date'))\nout = df.reset_index().values\nout[:,0] = pd.to_datetime(out[:,0])\nout\n\nIn this code, we first convert the 'date' index to datetime using pd.to_datetime. Then we reset the index to get the DataFrame in 2-D array format. Finally, we convert the first column (which is the date in string format) to datetime using pd.to_datetime and we get the desired output.", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": "df.index = df.index.set_levels([pd.to_datetime(df.index.levels[0]), df.index.levels[1]])\ndf = df.swaplevel(0,1)\nreturn df\n\n### END SOLUTION", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf.columns.name = None\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "df = df[df.filter(like='Value').abs().max(axis=1) < 1]\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "df = df[df.filter(like='Value').abs().max(axis=1) > 1]\n", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "df = df.loc[df.filter(like='Value').abs().max(axis=1) > 1]\ndf.columns = df.columns.str.replace('Value_', '')\ndf\n", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df.replace('&AMP;', '&', regex=True)\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df.replace('&LT;', '<', regex=True)\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": "You can use the `replace()` function in pandas to replace all occurrences of a particular string. In your case, you want to replace '&AMP;' with '&'. Here's how you can do it:\n\n```python\ndef f(df):\n    df.replace('&AMP;', '&', regex=True, inplace=True)\n    return df\n```\n\nIn this function, `regex=True` is used to interpret the string '&AMP;' as a regular expression, `inplace=True` is used to make the changes directly in the dataframe, and `df.replace()` is used to replace the occurrences of '&AMP;' with '&'.\n\nYou can call this function with your dataframe as an argument to replace '&AMP;' with '&' in all columns:\n\n```python\nresult = f(example_df)\nprint(result)\n```\n\nThis will output:\n\n```\n                 A  B  C\n0        Good & bad  0  Good & bad\n1               BB  1  Good & bad\n2               CC  2  Good & bad\n3               DD  3  Good & bad\n4        Good & bad  4  Good & bad\n```\n\nAs you can see, '&AMP;' has been replaced with '&' in all columns.", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\ndf['C'] = df['C'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "df[['first_name', 'last_name']] = df['name'].loc[df['name'].str.count(' ') == 1].str.split(' ', 1, expand=True)\ndf.loc[df['name'].str.count(' ') > 1, 'first_name'] = df['name']\ndf.loc[df['name'].str.count(' ') > 1, 'last_name'] = None\ndf = df[['first_name', 'last_name']]\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "df[['1_name', '2_name']] = df['name'].loc[df['name'].str.count(' ') == 1].str.split(' ', 1, expand=True)\ndf = df.fillna(df['name'])\ndf = df.drop(columns=['name'])\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "df[['first_name', 'middle_name', 'last_name']] = df['name'].apply(lambda x: pd.Series(x.split(' ')) if ' ' in x else pd.Series([x, np.nan, np.nan]))\ndf\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = pd.merge_asof(df2.sort_values('Timestamp'), df1.sort_values('Timestamp'), left_on='Timestamp', right_on='Timestamp')\nresult = result.sort_values('Timestamp').reset_index(drop=True)\n", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1.sort_values('Timestamp'), df2.sort_values('Timestamp'), left_on='Timestamp', right_on='Timestamp')\nresult = result.sort_index()\n", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']), axis=1)\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "error_values = [val for val in df['Field1'] if not isinstance(val, int)]\ndf = pd.DataFrame(error_values, columns=['Field1'])\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "int_values = []\nfor index, row in df.iterrows():\n    try:\n        value = int(row['Field1'])\n        int_values.append(value)\n    except ValueError:\n        continue\n\ndf = pd.DataFrame(int_values, columns=['Field1'])\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": "def f(df):\n    error_values = []\n    for value in df['Field1']:\n        if not isinstance(value, int):\n            error_values.append(value)\n    return error_values\n\nresult = f(example_df)\nprint(result)\n\nThis will iterate over each value in the 'Field1' column of the dataframe and check if it is not an integer. If it is not an integer, it will append the value to the error_values list. At the end, it will return the list of error values.", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "df = df.set_index('cat').div(df.set_index('cat').sum(axis=1), axis=0).reset_index()\ndf\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "df = df.set_index('cat').div(df.sum(axis=1), axis=0).reset_index()\n", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc
[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "df = df.drop(test)\nresult = df\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": "You can use the .loc[] function in pandas to select rows by labels. This function can take a list of labels and return a slice of the dataframe that contains only the rows with those labels. To ensure that each row occurs only once, you can convert the list to a set before passing it to the function.\n\nHere is the solution:\n\n```python\ndef f(df, test):\n    return df.loc[list(set(test))]\n```\n\nThis function will return a new dataframe that includes only the rows whose index labels are in the set of labels in the test list. The set function is used to remove duplicates from the list before it is passed to the .loc[] function.\n\nFor example:\n\n```python\ndf = pd.DataFrame({\n    'alleles': ['A/C', 'A/T', 'T/A', 'C/A', 'C/T'],\n    'chrom': [0, 0, 0, 0, 0],\n    'pos': [3, 7, 12, 15, 18],\n    'strand': ['+', '+', '+', '+', '+'],\n    'assembly#': [None, None, None, None, None],\n    'center': [None, None, None, None, None],\n    'protLSID': [None, None, None, None, None],\n    'assayLSID': [None, None, None, None, None]\n}, index=['TP3', 'TP7', 'TP12', 'TP15', 'TP18'])\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nprint(f(df, test))\n```\n\nThis will output:\n\n```\n   alleles  chrom  pos strand  assembly#  center  protLSID  assayLSID\nTP3      A/C      0    3      +        NaN     NaN       NaN        NaN\nTP12     T/A      0   12      +        NaN     NaN       NaN        NaN\nTP18     C/T      0   18      +        NaN     NaN       NaN        NaN\n```\n\nAs you can see, the row with index 'TP3' is only included once, even though it appears twice in the test list.", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "import numpy as np\n\n# Calculate pairwise distances\ndf['coords'] = list(zip(df.x, df.y))\ndf = df.groupby('time').apply(lambda g: pd.DataFrame(\n    {\n        'car': g['car'],\n        'coords': g['coords'],\n        'nearest_neighbour': g.apply(\n            lambda row: g[g['car'] != row['car']]['car'].min(), axis=1\n        ),\n        'euclidean_distance': g.apply(\n            lambda row: np.sqrt(\n                np.sum((np.array(row['coords']) - np.array(g[g['car'] != row['car']]['coords'].values))**2)\n            ), axis=1\n        )\n    }\n))\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "import numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\n# Calculate pairwise distances\ndistances = pdist(df[['x', 'y']], 'euclidean')\ndistances = squareform(distances)\n\n# Convert to DataFrame\ndf_distances = pd.DataFrame(distances, index=df['car'], columns=df['car'])\n\n# Find the nearest neighbour for each car\ndf_distances['farmost_neighbour'] = df_distances.idxmax(axis=1)\n\n# Calculate the euclidean distance to the nearest neighbour\ndf_distances['euclidean_distance'] = np.nanmax(df_distances.values, axis=1)\n\n# Reset index to 'car'\ndf_distances = df_distances.reset_index().rename(columns={'index': 'car'})\n\n# Merge with original DataFrame to include 'time'\ndf = df.merge(df_distances, on='car')\n\ndf", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "df['keywords_all'] = df.apply(lambda row: ','.join(row.dropna().astype(str).values), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "df['keywords_all'] = df.apply(lambda row: '-'.join(row.dropna().astype(str).values), axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "df['keywords_all'] = df.apply(lambda row: '-'.join(row[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].dropna().values), axis=1)\ndf\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "df['keywords_all'] = df.filter(like='keywords').apply(lambda x: '-'.join(x.dropna().values[::-1]), axis=1)\ndf\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "import numpy as np\n\n# Set random_state for reproducibility\nnp.random.seed(0)\n\n# Randomly select 20% of the rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Get the indices of the selected rows\nsample_indices = sample_df.index\n\n# Set the 'Quantity' column of the selected rows to zero\ndf.loc[sample_indices, 'Quantity'] = 0\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "import numpy as np\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Randomly select 20% of the rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Get the indices of the selected rows\nsample_indices = sample_df.index\n\n# Change the 'ProductId' of the selected rows to zero\ndf.loc[sample_indices, 'ProductId'] = 0\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "df['Quantity'] = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0)).index.get_level_values(1).map(df['Quantity'].where(df.index.isin, 0))\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\ndf['duplicate_index'] = df.duplicated(subset=['col1','col2'], keep=False)\ndf['duplicate_index'] = df['duplicate_index'].apply(lambda x: not x)\ndf['duplicate_index'] = df.groupby(['col1','col2'])['duplicate_index'].cumsum() - 1\ndf['duplicate_index'] = df['duplicate_index'].mask(duplicate_bool, '')\ndf\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\ndf['index_original'] = df.index\nduplicate = df.loc[duplicate_bool == True]\nresult = duplicate[['col1', 'col2', 'index_original']]\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\ndf['duplicate'] = duplicate_bool\ndf['index_original'] = df.groupby(['col1','col2']).cumcount()\nduplicate = df.loc[duplicate_bool == True]\nduplicate\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\ndf['index_original'] = df.duplicated(subset=['col1','col2', '3col'], \n                                     keep=False) & duplicate_bool\nduplicate = df.loc[duplicate_bool == True]\nresult = duplicate\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\ndf['index_original'] = df[duplicate_bool].index.values\nresult = df\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "result = df.loc[df.groupby(['Sp','Mt'])['count'].idxmax()]\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "df['rank'] = df.groupby(['Sp','Mt'])['count'].rank(method='min', ascending=False)\nresult = df[df['rank'] == 1]\ndf.drop('rank', axis=1, inplace=True)\n", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "result = df.loc[df.groupby(['Sp','Mt'])['count'].idxmin()]\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "result = df.loc[df.groupby(['Sp','Value'])['count'].idxmax()]\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "df.query(\"Category in @filter_list\")\n", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "df.query(\"Category not in @filter_list\")\n", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "result = pd.melt(df.reset_index(), value_vars=df.columns.to_list())\nresult.columns = ['index', 'variable', 'value']\nresult\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "result = df.reset_index().melt(id_vars='index').sort_values(['variable_0','variable_1','variable_2','index']).reset_index(drop=True)\nresult.columns = ['variable_0','variable_1','variable_2','value']\nresult\n", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum().reset_index(level=0, drop=True).where(lambda x : x > 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "result = df.groupby('l')['v'].apply(lambda x: np.nansum(x) if x.isnull().any() else x.sum())\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: np.nansum(x) if x.isnull().any() else x.sum())\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "result = df.groupby('l', as_index=False)['v'].apply(lambda x: np.nan if x.isnull().any() else x.sum())\n", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "def relationship(df):\n    result = []\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                if df[col1].nunique() == len(df) and df[col2].nunique() == 1:\n                    result.append(f'{col1} {col2} one-to-one')\n                elif df[col1].nunique() == len(df) and df[col2].nunique() < len(df):\n                    result.append(f'{col1} {col2} one-to-many')\n                elif df[col1].nunique() < len(df) and df[col2].nunique() == 1:\n                    result.append(f'{col1} {col2} many-to-one')\n                else:\n                    result.append(f'{col1} {col2} many-to-many')\n    return result\n\n\nrelationship(df)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "def relationship(col1, col2):\n    if len(col1.unique()) == len(col1) and len(col2.unique()) == len(col2):\n        return 'one-2-one'\n    elif len(col1.unique()) == len(col1) and len(col2.unique()) < len(col2):\n        return 'one-2-many'\n    elif len(col1.unique()) < len(col1) and len(col2.unique()) == len(col2):\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\n\n\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            result.append(f'{col1} {col2} {relationship(df[col1], df[col2])}')\n\nresult\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "def relationship(x, y):\n    if len(x.unique()) == len(y) and len(y.unique()) == len(x):\n        return 'one-to-one'\n    elif len(x.unique()) == len(y) and len(y.unique()) != len(x):\n        return 'one-to-many'\n    elif len(x.unique()) != len(y) and len(y.unique()) == len(x):\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\n\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\n\nfor c1 in df.columns:\n    for c2 in df.columns:\n        result.loc[c1, c2] = relationship(df[c1], df[c2])\n\nresult\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "def relationship(x, y):\n    if len(x.unique()) == len(y) and len(y.unique()) == len(x):\n        return 'one-2-one'\n    elif len(x.unique()) == len(y) and len(y.unique()) != len(x):\n        return 'one-2-many'\n    elif len(x.unique()) != len(y) and len(y.unique()) == len(x):\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\n\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\n\nfor col1 in df.columns:\n    for col2 in df.columns:\n        result.loc[col1, col2] = relationship(df[col1], df[col2])\n\nresult\n", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "df = df[df['bank'].notna()]\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "result = pd.to_numeric(s.str.replace(',',''), errors='coerce')\n", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "df['Group'] = df.apply(lambda row: 'Has Family' if row['SibSp'] > 0 or row['Parch'] > 0 else 'No Family', axis=1)\nresult = df.groupby('Group')['Survived'].mean()\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "df['Group'] = np.where((df['Survived'] > 0) | (df['Parch'] > 0), 'Has Family', 'No Family')\nresult = df.groupby('Group')['SibSp'].mean()\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "df['Group'] = df.apply(lambda row: 'Has Family' if (row['SibSp'] == 1) & (row['Parch'] == 1) \nelse ('New Family' if (row['SibSp'] == 0) & (row['Parch'] == 1) \nelse ('No Family' if (row['SibSp'] == 0) & (row['Parch'] == 0) \nelse 'Old Family'), axis=1)\n\nresult = df.groupby('Group')['Survived'].mean()\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "df.sort_values(['cokey', 'A'], inplace=True)\ndf.reset_index(drop=True, inplace=True)\nresult = df\n", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "df['index'] = df.groupby('cokey').cumcount()\nresult = df.sort_values(['cokey', 'index', 'A']).reset_index(drop=True).drop('index', axis=1)\n", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])\n", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])\n", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])\n", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame(data={'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult\n", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "grouped_df = df.groupby('a')\nresult = grouped_df['b'].apply(lambda x: (x.mean(), x.std())).reset_index()\nresult.columns = ['a', 'Statistics']\nresult['Mean'] = result['Statistics'].apply(lambda x: x[0])\nresult['Std'] = result['Statistics'].apply(lambda x: x[1])\nresult = result.drop('Statistics', axis=1)\nresult\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "import numpy as np\n\nresult = df.groupby('b')['a'].apply(lambda x: np.std(x) / np.sqrt(len(x))).reset_index(name='std')\nresult['mean'] = df.groupby('b')['a'].transform('mean')\nresult = result.drop_duplicates('b')\nresult\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "import numpy as np\nfrom scipy.special import softmax\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Calculate softmax\ndf['softmax'] = df.groupby('a')['b'].transform(lambda x: softmax(x))\n\n# Normalize data\nscaler = MinMaxScaler()\ndf[['b']] = scaler.fit_transform(df[['b']])\ndf.columns = ['a', 'b', 'softmax', 'min-max']\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "result = df.loc[:, (df != 0).any(axis=0)]\nresult = result.loc[(result != 0).any(axis=1)]\n", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "result = df.loc[:, (df != 0).any(axis=0)]\nresult = result.loc[(result != 0).any(axis=1)]\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "result = df.loc[:, (df.max(axis=0) < 2).values]\nresult = result.loc[(result.max(axis=1) < 2).values, :]\n", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "result = df.where(df <= 1)\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=(True, False))\n", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = s.reset_index().sort_values(by=['index','1']).reset_index(drop=True)\ndf.columns = ['index','1']\ndf\n", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "result = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].apply(lambda x: isinstance(x, str))]\n", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "result = df.loc[df.groupby(['Sp','Mt'])['count'].idxmax()]\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "df['rank'] = df.groupby(['Sp','Mt'])['count'].rank(method='min', ascending=False)\nresult = df[df['rank'] == 1]\ndf.drop('rank', axis=1, inplace=True)\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "result = df.loc[df.groupby(['Sp','Mt'])['count'].idxmin()]\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "result = df.loc[df.groupby(['Sp','Value'])['count'].idxmax()]\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict).fillna(df['Member'])\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict).fillna('17/8/1926')\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": "You can use the map function in pandas to map the values in the 'Member' column of your DataFrame to the values in your dictionary. This will replace the values in the 'Date' column with the corresponding values from your dictionary.\n\nHere is the code:\n\n```python\ndef f(dict=example_dict, df=example_df):\n    df['Date'] = df['Member'].map(dict)\n    return df\n```\n\nThis will return:\n\n```\n   Member Group       Date\n0     xyz    A         NaN\n1     uvw    B         NaN\n2     abc    A  1/2/2003\n3     def    B  1/5/2017\n4     ghi    B  4/10/2013\n```\n\nAs you can see, the 'Date' column has been updated with the values from your dictionary where the keys in the dictionary match the values in the 'Member' column.\n\nIf you want to keep the NaN values in the 'Member' column, you can use the fillna function after mapping the values. Here is how you can do it:\n\n```python\ndef f(dict=example_dict, df=example_df):\n    df['Date'] = df['Member'].map(dict).fillna(df['Member'])\n    return df\n```\n\nThis will return:\n\n```\n   Member Group       Date\n0     xyz    A         xyz\n1     uvw    B         uvw\n2     abc    A  1/2/2003\n3     def    B  1/5/2017\n4     ghi    B  4/10/2013\n```\n\nAs you can see, the 'Date' column has NaN values where the 'Member' column had NaN values, and the rest of the values have been replaced with the corresponding values from your dictionary.", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict).fillna('17/8/1926')\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby('Date')['Date'].transform('count')\ndf['year'] = df['Date'].dt.year\ndf['month'] = df['Date'].dt.month\ndf['Count_m'] = df.groupby(['year', 'month'])['Date'].transform('count')\ndf['Count_y'] = df.groupby('year')['Date'].transform('count')\ndf = df.drop(columns=['year', 'month'])\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month, 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year, 'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby('Val').Val.transform('count')\ndf\n", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\n\ndf['Count_d'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month, df['Date'].dt.day])['Date'].transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year)['Date'].transform('count')\ndf['Count_w'] = df.groupby([df['Date'].dt.year, df['Date'].dt.week])['Date'].transform('count')\ndf['Count_Val'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month, df['Date'].dt.day, 'Val'])['Val'].transform('count')\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "df['Date'] = pd.to_datetime(df['Date'])\nresult1 = df.groupby('Date').apply(lambda x: (x != 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: (x == 0).sum())\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "df['Date'] = pd.to_datetime(df['Date'])\nresult1 = df.groupby('Date').apply(lambda x: (x % 2 == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: (x % 2 != 0).sum())\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "pivot_table = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\npivot_table", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values={'D':'sum', 'E':'mean'}, index=['B'])\nresult\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "pivot_table = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\npivot_table", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "pivot_table = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.max, 'E': np.min})\npivot_table", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "result = df.var2.str.split(',').apply(pd.Series, 1).stack()\nresult.index = result.index.droplevel(-1)\nresult.name = 'var2'\ndel df['var2']\ndf = df.join(result)\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "result = df.var2.str.split(',').apply(pd.Series, 1).stack()\nresult.index = result.index.droplevel(-1)\nresult.name = 'var2'\ndel df['var2']\ndf = df.join(result)\n", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "result = df.var2.str.split('-').apply(pd.Series).stack()\nresult.index = result.index.droplevel(-1)\nresult.name = 'var2'\ndel df['var2']\ndf = df.join(result)\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha() or string[i].isdigit() or string[i].isspace()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "df.set_index('Name', inplace=True)\ndf = df.where(df!=0).expanding(1).mean().fillna(method='ffill', axis=1)\ndf\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "df = df.set_index('Name')\ndf = df.replace(0, np.nan)\ndf = df.expanding(axis=1).mean().fillna(method='ffill', axis=1)\ndf = df.reset_index()\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": "def f(df):\n    df = df.set_index('Name')\n    df = df.replace(0, np.nan)\n    df = df.expanding().mean().fillna(method='ffill', axis=1)\n    return df\n\nresult = f(example_df)\nreturn result\n\n### END SOLUTION\n\nreturn result\n", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "df = df.set_index('Name')\ndf = df.replace(0, np.nan)\ndf = df.expanding(axis=1).mean().fillna(method='ffill', axis=1)\ndf = df.reset_index()\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['DateTime'] = pd.to_datetime(df['DateTime'])\ndf.set_index('DateTime', inplace=True)\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else 0)\ndf.loc[df.index[0], 'label'] = 1\ndf.reset_index(inplace=True)\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "df['DateTime'] = pd.to_datetime(df['DateTime'])\ndf.set_index('DateTime', inplace=True)\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\ndf.loc[df.index[0], 'label'] = 1\ndf.reset_index(inplace=True)\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\ndf.loc[0, 'label'] = 1\ndf\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.groupby('id')['departure_time'].shift() - df['arrival_time']\ndf.loc[df['arrival_time'] == '0', 'Duration'] = pd.NaT\ndf\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n\ndf['Duration'] = df.groupby('id')['departure_time'].shift() - df['arrival_time']\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['arrival_time'] = df['arrival_time'].replace('0', pd.np.nan)\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['Duration'] = df.groupby('id')['departure_time'].shift() - df['arrival_time']\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf\n", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')\nresult", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "df['endswith_e'] = df['key2'].apply(lambda x: x.endswith('e'))\nresult = df.groupby('key1')['endswith_e'].sum().astype(int).reset_index().rename(columns={'endswith_e':'count'})\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.index.max()\nmin_result = df.index.min()\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df.index.mode()[0]\nmedian_result = df.index.median()\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "result = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "result = df.loc[~df['closing_price'].between(99,101)]\n", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "df_min = df.groupby(\"item\")[\"diff\"].transform('min')\nresult = df[df['diff'] == df_min]\nresult = result[['item', 'diff', 'otherstuff']]\n", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[0] if '_' in x else x)\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[-1] if '_' in x else x)\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": "You can use the `rpartition` function to split the string at the last underscore:\n\n```python\ndef f(df):\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.rpartition('_')[0]\n    return df\n\nresult = f(example_df)\n```\n\nThe `rpartition` function splits the string into three parts: the part before the last underscore, the underscore itself, and the part after the underscore. The `[0]` index is used to get the part before the last underscore. If there is no underscore in the string, the entire string is returned.", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "nan_rows = df['Column_x'].isna()\nnum_nan = nan_rows.sum()\n\nfirst_half = num_nan // 2\nsecond_half = num_nan - first_half\n\nfirst_half_indices = nan_rows.index[0:first_half]\nsecond_half_indices = nan_rows.index[-second_half:]\n\ndf.loc[first_half_indices, 'Column_x'] = 0\ndf.loc[second_half_indices, 'Column_x'] = 1\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "nan_rows = df['Column_x'].isna()\nnum_nan = nan_rows.sum()\n\nfirst_third = int(num_nan * 0.3)\nsecond_third = int(num_nan * 0.6)\n\ndf.loc[nan_rows.iloc[:first_third] , 'Column_x'] = 0\ndf.loc[nan_rows.iloc[first_third:second_third] , 'Column_x'] = 0.5\ndf.loc[nan_rows.iloc[second_third:] , 'Column_x'] = 1\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "nan_mask = df['Column_x'].isna()\nzero_mask = df['Column_x'] == 0\n\ndf.loc[nan_mask & ~zero_mask[:int(len(df)/2)], 'Column_x'] = 1\ndf.loc[nan_mask & zero_mask[int(len(df)/2):], 'Column_x'] = 0\n", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.concat([a, b], axis=1).groupby(level=0, axis=1).apply(lambda x: x.values.tolist())\n", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "result = pd.concat([a,b,c], axis=1).groupby(level=0, axis=1).apply(lambda x: x.values.tolist())\n", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "result = pd.concat([a, b], axis=1).groupby(level=0, axis=1).apply(lambda x: x.values.tolist())\nresult.columns = ['one', 'two']\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "df['views_binned'] = pd.cut(df['views'], bins)\nresult = df.groupby(['username', 'views_binned']).size().unstack().fillna(0)\nresult\n", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "df['views_binned'] = pd.cut(df['views'], bins)\nresult = df.groupby(['username', 'views_binned']).size().unstack().fillna(0)\nresult\n", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "result = df.to_string(index=False, header=False)\nresult = pd.DataFrame([result], columns=['text'])\n", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "result = df.agg('-'.join)\nresult = pd.DataFrame(result)\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "result = df.iloc[:, 0].str.cat(sep=', ')\nresult = pd.DataFrame(result, index=[0])\n", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = df['text'].str.cat(sep=', ')\nresult = pd.Series([result], index=['text'], name='text')\n", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "result = df['text'].str.cat(sep='-')\nresult = pd.Series(result, name='text')\n", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "df2 = df2.merge(df1[['id', 'city', 'district']], how='left', on='id')\nresult = pd.concat([df1, df2], axis=0, ignore_index=True)\n", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "df1['date'] = pd.to_datetime(df1['date']).dt.strftime('%d-%b-%Y')\ndf2['date'] = pd.to_datetime(df2['date']).dt.strftime('%d-%b-%Y')\n\ndf2 = df2.merge(df1[['id', 'city', 'district']], how='left', on='id')\n\nresult = pd.concat([df1, df2]).sort_values(['id', 'date']).reset_index(drop=True)\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "df2 = df2.merge(df1[['id', 'city', 'district']], how='left', on='id')\nresult = pd.concat([df1, df2]).sort_values(['id', 'date']).reset_index(drop=True)\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.concat([C, D]).drop_duplicates(subset='A', keep=False).reset_index(drop=True)\n", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.concat([C, D]).drop_duplicates(subset='A', keep='first').reset_index(drop=True)\n", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "C['duplicated'] = False\nD['duplicated'] = False\n\ncommon_rows = pd.merge(C, D, on='A', how='inner')\nC.loc[common_rows.index, 'duplicated'] = True\nD.loc[common_rows.index, 'B_y'] = D.loc[common_rows.index, 'B_x']\n\nresult = pd.concat([C, D]).sort_index().reset_index(drop=True)\nresult\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "df.groupby('user').apply(lambda x: list(zip(x['time'].values, x['amount'].values)))\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "df['amount-time'] = list(zip(df.time, df.amount))\nresult = df.groupby('user')['amount-time'].apply(list)\nresult = result.reset_index()\nresult", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "df['amount-time'] = list(zip(df.time, df.amount))\nresult = df.groupby('user')['amount-time'].apply(list)\nresult = df.groupby('user')['amount-time'].apply(list).reset_index()\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "df = pd.DataFrame(series.tolist(), index=series.index).reset_index().rename(columns={'index':''})\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "df = pd.DataFrame(series.tolist(), index=series.index).reset_index().rename(columns={'index': 'name'})\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = [col for col in df.columns if s in col]\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "result = df.filter(like=s, axis=1).columns.tolist()\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "result = df.filter(like=s).add_suffix('1')\nresult.columns = [col.replace(s, 'spike') for col in result.columns]\ndf = df.drop(result.columns, axis=1).join(result)\ndf\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "result = pd.DataFrame(df['codes'].to_list(), index= df.index).add_prefix('code_')\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "result = pd.DataFrame(df['codes'].to_list(), index= df.index).add_prefix('code_')\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "result = pd.DataFrame(df['codes'].to_list(), index= df.index).add_prefix('code_')\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "df['col1'] = df['col1'].apply(lambda x: [int(i) for i in x])\nresult = [item for sublist in df['col1'].values for item in sublist]\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "df['col1'] = df['col1'].apply(lambda x: ','.join(map(str, x[::-1])))\nresult = df['col1'].str.cat(sep=',')\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "df['col1'] = df['col1'].apply(lambda x: ','.join(map(str, x)))\nresult = df['col1'].str\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "df.set_index('Time', inplace=True)\ndf = df.resample('2min').mean()\ndf.reset_index(inplace=True)\ndf\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "df.set_index('Time', inplace=True)\ndf = df.resample('3T').sum()\ndf.reset_index(inplace=True)\ndf['Time'] = df['Time'].dt.floor('3T')\ndf\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method=\"min\", ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method=\"min\", ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method=\"min\", ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[df.index.get_level_values('a').isin(filt[filt].index)]\n", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[df.index.get_level_values('a').isin(filt[filt].index)]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.loc[0].index[df.loc[0] != df.loc[8]].tolist()\n", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.loc[0].index[df.loc[0] == df.loc[8]].tolist()\n", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.loc[0].ne(df.loc[8]).index[df.loc[0].ne(df.loc[8])].tolist()\n", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(row0, row8) for row0, row8 in zip(df.iloc[0], df.iloc[8]) if row0 != row8]\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "df.set_index('Date', inplace=True)\nts = df['Value']\nprint(ts)\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.T.reset_index().T\ndf.columns = [f'{col}_{i+1}' if i > 0 else col for i, col in enumerate(df.columns.tolist())]\n", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.T.reset_index().T\ndf.columns = [f'{col}_{idx}' if idx > 0 else col for idx, col in df.columns]\n", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].round(2).fillna(pd.NA)\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].round(2).fillna(pd.NA)\ndf['cats'] = df['cats'].round(2).fillna(pd.NA)\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "result = df.sort_values(by='time')\n", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "result = df.sort_values('VIM')\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "dates_to_remove = ['2020-02-17', '2020-02-18']\nmask = df.index.strftime('%Y-%m-%d').isin(dates_to_remove)\ndf = df[~mask]\nresult = df\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "df = df[~((df.index.date == pd.to_datetime('2020-02-17').date()) | (df.index.date == pd.to_datetime('2020-02-18').date()))]\ndf['Day of Week'] = df.index.day_name()\nresult = df\n", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "result = corr.stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\nresult = result[result['Pearson Correlation Coefficient'].abs() > 0.3]\nresult = result.set_index(['Col1', 'Col2'])\n", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "result = corr.stack().reset_index()\nresult.columns = ['idx1','idx0','value']\nresult = result[(result['idx0'] != result['idx1']) & (result['value'].abs() > 0.3)]\nresult = result.set_index(['idx0','idx1'])['value']\n", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "df.columns = ['A', 'B', 'Test']\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "df.columns[0] = 'Test'\nresult = df\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "df['frequent'] = df.mode(axis=1)[0]\ndf['freq_count'] = df.apply(lambda row: list(row).count(1), axis=1)\ndf\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "df['frequent'] = df.mode(axis=1)[0]\ndf['freq_count'] = df.apply(lambda row: list(row).count(row['frequent']), axis=1)\ndf\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "df['frequent'] = df.apply(lambda row: list(row.value_counts().index[row.value_counts() == row.value_counts().max()]), axis=1)\ndf['freq_count'] = df['frequent'].apply(lambda x: len(x))\ndf\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "df[\"bar\"] = df[\"bar\"].replace('NULL', pd.np.nan)\ndf[\"bar\"] = df[\"bar\"].astype(float)\nresult = df.groupby([\"id1\",\"id2\"]).mean()\n", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "df['bar'] = df['bar'].replace('NULL', 0)\ndf['bar'] = df['bar'].astype(int)\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\n", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "result = pd.merge(df_a, df_b[['EntityNum','a_col']], on='EntityNum')\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b[['EntityNum','b_col']], on='EntityNum')\n", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape\nEND SOLUTION\nprint(result)", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[~np.isnan(x)]\n", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.nan_to_num(x, nan=np.inf)\n", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "result = [[val for val in sublist if not np.isnan(val)] for sublist in x]\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.eye(a.max()+1)[a]\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.eye(max(a)+1)[a]\n", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.eye(a.max() + 1)[a + abs(a.min())]\n", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.eye(a.size)[np.argsort(a)]\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.size, a.max()+1), dtype=int)\nb[np.arange(a.size)[:, None], a.ravel()] = 1\nb = b.reshape(a.shape[0], a.shape[1], -1)\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\n", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = A.reshape(-1, ncol)\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = A.reshape(nrow, -1)\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "B = A[:len(A)//ncol*ncol].reshape(-1, ncol)\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "B = np.flip(A.reshape(-1, ncol), 0)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.pad(a, (shift, 0))[:-shift] if shift > 0 else np.pad(a, (0, -shift))[(-shift):]\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "def shift_2d(arr, shift):\n    if shift > 0:\n        shifted = np.pad(arr, ((0, 0), (shift, 0)), mode='constant', constant_values=np.nan)[:, :-shift]\n    else:\n        shifted = np.pad(arr, ((0, 0), (0, -shift)), mode='constant', constant_values=np.nan)[:, -shift:]\n    return shifted\n\nresult = shift_2d(a, shift)\nprint(result)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "def shift_array(arr, shifts):\n    shifted_arrays = []\n    for array, shift in zip(arr, shifts):\n        shifted_array = np.empty_like(array)\n        shifted_array.fill(np.nan)\n        shifted_array[shift:] = array[:-shift]\n        shifted_arrays.append(shifted_array)\n    return np.array(shifted_arrays)\n\nresult = shift_array(a, shift)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "np.random.seed(0)\nr = np.random.randint(3, size=(100, 2000)) - 1\nr_old = r.copy()\nnp.random.seed(0)\nr_new = np.random.randint(3, size=(100, 2000)) - 1\nEND SOLUTION", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a, axis=None), a.shape, order='C')\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a, axis=None), a.shape, order='C')\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a, axis=None), a.shape, order='F')\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a, axis=None), a.shape, order='C')\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": "You can use the `np.unravel_index` function to convert the flattened index to its original multi-dimensional index. Here is how you can do it:\n\n```python\ndef f(a):\n    flattened_index = np.argmax(a)\n    multi_dimensional_index = np.unravel_index(flattened_index, a.shape, order='C')\n    return multi_dimensional_index\n\nresult = f(example_a)\nprint(result)\n```\n\nIn this code, `np.argmax(a)` gives the flattened index of the maximum value in the array `a`. `np.unravel_index` then converts this index to its original multi-dimensional index given the shape of the array. The `order='C'` argument ensures that the index is returned in C-order (row-major order).\n\nPlease note that the index returned by `np.unravel_index` is in C-order, meaning that it is in 'row-major' order, which means that it traverses the memory in a row-by-row fashion.", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "import numpy as np\n\na = np.array([[10,50,30],[60,20,40]])\n\n# Flatten the array and sort it in descending order\nsorted_a = np.sort(a, axis=None)[::-1]\n\n# Get the second largest value\nsecond_largest = sorted_a[1]\n\n# Get the indices of the second largest value\nresult = np.unravel_index(np.argmax(a == second_largest), a.shape, order='C')\n\nprint(result)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "a = a[:, ~np.isnan(a).any(axis=0)]\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]\n", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = a[:, permutation]\n", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = a[permutation]\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "result = np.unravel_index(a.argmin(), a.shape)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "result = np.unravel_index(a.argmax(), a.shape)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "result = np.argwhere(a == np.min(a))\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.radians(degree))\n", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.radians(degree))\n", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "import numpy as np\n\nnumber = np.random.randint(0, 360)\n\n# For degrees, np.sin() returns a value between -1 and 1.\n# For radians, np.sin() returns a value close to 1.\n# So, we can use the sign of np.sin() to determine if the number is degree or radian.\n\nif np.sin(number) >= 0:\n    result = 1  # radian\nelse:\n    result = 0  # degree\n\nprint(result)\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.arcsin(value) * (180 / np.pi)\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "result = np.pad(A, (0, length-A.shape[0]), 'constant', constant_values=0)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "result = np.pad(A, (0, length-A.size), 'constant', constant_values=0)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.power(a, power)\nEND SOLUTION\nprint(a)", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": "You can use the numpy function `numpy.power()` to raise a numpy array to a specific power. Here's how you can do it:\n\n```python\nimport numpy as np\n\nexample_a = np.arange(4).reshape(2, 2)\n\ndef f(a = example_a, power = 2):\n    return np.power(a, power)\n\nprint(f())\n```\n\nIn this code, `np.power(a, power)` raises each element in the array `a` to the power `power`. If you want to raise each element to a different power for each element, you can pass a second array of the same shape to `np.power()`.\n\nFor example, if you want to raise each element to the power of 3, you can do:\n\n```python\ndef f(a = example_a, power = 3):\n    return np.power(a, power)\n\nprint(f())\n```\n\nThis will raise each element in the array to the power of 3.", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "from fractions import Fraction\n\nresult = Fraction(numerator, denominator)\nresult = (result.numerator, result.denominator)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": "from fractions import Fraction\n\ndef f(numerator = 98, denominator = 42):\n    frac = Fraction(numerator, denominator)\n    return (frac.numerator, frac.denominator)\n\nprint(f())\n\nThis will return the fraction reduced to lowest terms, as a tuple of the numerator and denominator. The Fraction class in the fractions module in Python's standard library will automatically reduce fractions to lowest terms.", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "from fractions import Fraction\n\ndef reduce_fraction(numerator, denominator):\n    if denominator == 0:\n        return (np.nan, np.nan)\n    else:\n        fraction = Fraction(numerator, denominator)\n        return (fraction.numerator, fraction.denominator)\n\nresult = reduce_fraction(numerator, denominator)\nprint(result)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = np.mean([a, b, c], axis=0)\nEND SOLUTION\nprint(result)", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum(np.maximum(a, b), c)\n", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.fliplr(a).diagonal()\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.fliplr(a).diagonal()\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.diag(np.fliplr(a).diagonal())\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.fliplr(a).diagonal()\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = [item for row in X for item in row]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = []\nfor row in X.flatten(order='C'):\n    result.append(row)\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": "result = [item for row in X for item in row]\nreturn result\n### END SOLUTION\n\n# Test the function\nprint(f())", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = []\nfor row in np.nditer(X, order='F'):\n    result.append(row)\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.array([int(num) for num in mystr])\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)\n", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row] * multiply_number)\n", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "row_data = a[row, :] / divide_number\nresult = np.prod(row_data)\n", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "from scipy.linalg import null_space\nresult = null_space(a)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[1]\n", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "import numpy as np\nimport scipy.stats\n\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n\n# Calculate the means of the two samples\nmean_a = np.mean(a)\nmean_b = np.mean(b)\n\n# Calculate the standard deviations of the two samples\nstd_dev_a = np.std(a, ddof=1)\nstd_dev_b = np.std(b, ddof=1)\n\n# Calculate the pooled standard deviation\nsp = np.sqrt(((len(a)-1)*(std_dev_a**2) + (len(b)-1)*(std_dev_b**2))/(len(a)+len(b)-2))\n\n# Calculate the t-statistic\nt_statistic = (mean_a - mean_b) / (sp * np.sqrt(1/len(a) + 1/len(b)))\n\n# Calculate the degrees of freedom\ndegrees_of_freedom = len(a) + len(b) - 2\n\n# Calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(abs(t_statistic), df=degrees_of_freedom))\n\np_value\n", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "import numpy as np\nimport scipy.stats\n\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n\n# Remove NaNs\na = a[~np.isnan(a)]\nb = b[~np.isnan(b)]\n\n# Perform two-tailed t-test\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n\n# Print the p-value\nprint(\"p-value:\", p_value)\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "import scipy.stats as stats\n\n# Calculate the pooled standard deviation\nsp = np.sqrt(((anobs-1)*(avar) + (bnobs-1)*(bvar))/(anobs+bnobs-2))\n\n# Calculate the pooled mean\nup = np.sqrt((anobs*avar + bnobs*bvar)/(anobs+bnobs))\n\n# Calculate the test statistic\nt_stat = (amean - bmean)/(sp*np.sqrt(1/anobs + 1/bnobs))\n\n# Calculate the degrees of freedom\ndf = anobs + bnobs - 2\n\n# Calculate the p-value\np_value = 2*stats.t.cdf(-np.abs(t_stat), df)\n\n# Print the p-value\nprint(\"The p-value is: \", p_value)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "mask = np.invert(np.isin(A, B).all(axis=1))\noutput = A[mask]\n", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.concatenate((A[~np.in1d(A.view(np.int64), B.view(np.int64))], B[~np.in1d(B.view(np.int64), A.view(np.int64))]))\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = np.take_along_axis(b, sort_indices[..., None], axis=0)\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = np.take_along_axis(b, sort_indices[..., None], axis=0)\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = np.take_along_axis(b, sort_indices[..., None], axis=0)\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "sum_a = np.sum(a, axis=(1,2))\nsorted_indices = np.argsort(sum_a)\nresult = b[sorted_indices]\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(arr, 2, axis=1)\n", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "np.delete(arr, 2, axis=0)\nEND SOLUTION\na", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(arr, [0, 2], axis=1)\n", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "result = np.delete(a, del_col-1, axis=1)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\nprint(a)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": "a = np.insert(a, pos, element)\nreturn a\n### END SOLUTION\n\n# Call the function with the example array and the desired position and element\na = f()\nprint(a)\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "for i in range(len(pos)-1, -1, -1):\n    a = np.insert(a, pos[i], element[i], axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = array_of_arrays.copy()\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(a[0] == a)\n", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(a[:,0] == a[:,1]) and np.all(a[:,1] == a[:,2])\n", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": "You can use the `np.ptp` function which returns the difference between the maximum and minimum values of an array along an axis. If all rows are equal, the maximum and minimum values of each row will be the same, so the `np.ptp` function will return 0 for each row.\n\nHere is the solution:\n\n```python\ndef f(a):\n    return np.all(np.ptp(a, axis=1) == 0)\n```\n\nThis function will return `True` if all rows are equal and `False` otherwise.\n\nYou can test this function with your `example_a` array:\n\n```python\nprint(f(example_a))  # Output: True\n```\n\nThis solution is more efficient than using `np.array_equal` in a list comprehension because it avoids creating unnecessary intermediate lists and arrays. It also avoids the need to manually loop over the rows of the array.", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "import scipy.integrate as spi\n\n# Define the function\ndef func(x, y):\n    return (np.cos(x))**4 + (np.sin(y))**2\n\n# Use dblquad to integrate over the function\nresult, error = spi.dblquad(func, 0, np.pi, lambda x: 0, lambda x: np.pi)\n\nprint(result)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": "You can use the `numpy.outer` function to create a 2D grid of points, then use `scipy.integrate.simps` to integrate over each dimension. Here's an example:\n\n```python\nimport numpy as np\nfrom scipy.integrate import simps\n\n# Define the grid\nx = np.linspace(0, np.pi, 100)\ny = np.linspace(0, np.pi, 100)\nX, Y = np.meshgrid(x, y)\n\n# Define the function\nf = lambda x, y: (np.cos(x))**4 + (np.sin(y))**2\n\n# Evaluate the function on the grid\nZ = f(X, Y)\n\n# Integrate over each dimension\nintegral_x = simps(Z, x)\nintegral_y = simps(Z, y)\n\n# The total integral is the product of the integrals over each dimension\nintegral = integral_x * integral_y\n\nprint(integral)\n```\n\nThis will give you the 2D integral of the function over the grid. Note that `simps` performs Simpson's rule integration, which is more accurate than the rectangle rule for smooth functions.\n\nPlease note that the `simps` function requires an even number of points to work properly. If you have an odd number of points, you can either add an extra point or interpolate the function to make it even.", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "import numpy as np\nfrom scipy import stats\n\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\n\n# sort the data\ngrades = np.sort(grades)\n\n# calculate the ECDF\nx = stats.ecdf(grades)\n\n# get the values\nresult = x(grades)\n", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "from scipy import stats\n\n# Sort the data\ngrades = np.sort(grades)\n\n# Calculate the ECDF\nx, y = stats.ecdf(grades)\n\n# Create a function to evaluate the ECDF\neval_ecdf = stats.EmpiricalDistribution(grades)\n\n# Evaluate the function\nresult = eval_ecdf(eval)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "import numpy as np\nfrom scipy import stats\n\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\n\n# Sort the data\ngrades = np.sort(grades)\n\n# Compute the ECDF\nx, y = stats.ecdf(grades)\n\n# Find the longest interval that satisfies ECDF < threshold\nlow, high = None, None\nmax_length = 0\nfor i in range(len(x)):\n    for j in range(i+1, len(x)):\n        if y[i] < threshold and y[j] >= threshold:\n            length = x[j] - x[i]\n            if length > max_length:\n                max_length = length\n                low, high = x[i], x[j]\n\nprint(\"Longest interval:\", (low, high))\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.hstack([np.ones(int(one_ratio * size)), np.zeros(size - int(one_ratio * size))])\nnp.random.shuffle(nums)\n", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()\n", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "import torch\nimport numpy as np\na = np.ones(5)\na_pt = torch.from_numpy(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = a.numpy()\n", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "import tensorflow as tf\nimport numpy as np\n\na = np.ones([2,3,4])\na_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a)\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = a.argsort()[-N:][::-1]\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)\n", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(0, a.shape[0], 2) for j in range(0, a.shape[1], 2)])\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "result = []\nfor i in range(a.shape[0]-1):\n    for j in range(a.shape[1]-1):\n        patch = a[i:i+2, j:j+2]\n        result.append(patch)\nresult = np.array(result)\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "result = a.reshape(4, 2, 2, 2).transpose(0, 2, 1, 3).reshape(-1, 2, 2)\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "result = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        patch = a[i:i+patch_size, j:j+patch_size]\n        if patch.shape == (patch_size, patch_size):\n            result.append(patch)\nresult = np.array(result)\nresult.shape = result.shape[:-2] + (patch_size, patch_size)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.reshape(a, (h, w))\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "result = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        patch = a[i:i+patch_size, j:j+patch_size]\n        if patch.shape == (patch_size, patch_size):\n            result.append(patch)\nresult = np.array(result)\nresult.shape = (result.shape[0], patch_size, patch_size)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]\n", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high+1, :]\n", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low-1:min(high, a.shape[1])]\n", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "a = np.array([[float(j) for j in i.split()] for i in string.strip('[]').split('], [')])\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "import numpy as np\n\ndef loguni(n, min, max, base=np.e):\n    return np.power(base, np.random.uniform(np.log(min), np.log(max), n))\n\nmin = 1\nmax = np.e\nn = 10000\n\nresult = loguni(n, min, max)\n", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "min_value = np.exp(min)\nmax_value = np.exp(max)\nresult = np.random.uniform(low=min_value, high=max_value, size=n)\n", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": "You can use the inverse transform sampling method to generate log-uniform random numbers. The idea is to generate uniform random numbers on the interval [0, 1], and then apply the inverse function to these numbers to get log-uniform numbers.\n\nHere is the Python code:\n\n```python\nimport numpy as np\n\ndef log_uniform(min_val, max_val, size):\n    return min_val**(1-np.random.random(size)) * max_val**(np.random.random(size))\n\n# Example usage:\nresult = log_uniform(1, np.e, 10000)\n```\n\nIn this code, `np.random.random(size)` generates `size` numbers from a uniform distribution on the interval [0, 1]. The expression `min_val**(1-np.random.random(size))` generates numbers on the interval [min_val, 1], and `max_val**(np.random.random(size))` generates numbers on the interval [1, max_val]. Multiplying these two sets of numbers together gives numbers from a log-uniform distribution on the interval [min_val, max_val].", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] *= a\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = [None]*len(A)\nB[0] = a*A[0]\nif len(A) > 1:\n    B[1] = a*A[1] + b*B[0]\nfor t in range(2, len(A)):\n    B[t] = a*A[t] + b*B[t-1] + c*B[t-2]\nB = pd.Series(B)\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.array([])\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))\n", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.ravel_multi_index(index, dims, mode='raise') - 1\n", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "result = np.ravel_multi_index(index, dims, mode='raise')\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "data = np.zeros((2, 3), dtype=[('a', int), ('b', float)])\ndf = pd.DataFrame(data, index=index, columns=columns)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.bincount(accmap, weights=a)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.bincount(index, a).max()\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.bincount(accmap, weights=a)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = np.empty(len(set(index)))\nfor i, v in enumerate(sorted(set(index))):\n    if v < 0:\n        v = len(a) + v\n    result[i] = np.min(a[index == v])\nresult\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "x = np.array(x)\ny = np.array(y)\nz = x + y\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(np.arange(len(lista_elegir)), samples, p=probabilit)\nresult = [lista_elegir[i] for i in result]\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "result = np.pad(a, ((1, 1), (1, 1)), 'constant')\nresult = result[1+low_index:3+high_index, 1+low_index:3+high_index]\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]\n", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.iscomplex(x)]\n", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = data[:-(data.size % bin_size)]\nbin_data = bin_data.reshape(-1, bin_size)\nbin_data_mean = bin_data.mean(axis=1)\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.array_split(data, len(data) // bin_size)\nbin_data_max = [np.max(bin) for bin in bin_data]\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = data.reshape(-1, bin_size)\nbin_data = bin_data[:-1] if bin_data.shape[0]*bin_size != data.shape[1] else bin_data\nbin_data_mean = bin_data.mean(axis=1)\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = data[-(len(data)//bin_size)*bin_size:]\nbin_data = bin_data.reshape(-1, bin_size)\nbin_data_mean = bin_data.mean(axis=1)\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = data[:, -bin_size:]\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = data[:, -bin_size:]\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "def smoothclamp(x, x_min, x_max):\n    t = (x - x_min) / (x_max - x_min) # Normalize x to range [0, 1]\n    t = max(0, min(1, t)) # Clamp t to [0, 1]\n    t = t*t*(3 - 2*t) # Smooth t\n    return x_min + t * (x_max - x_min) # Scale and shift to original range\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "def smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min)/(x_max - x_min), 0.0, 1.0)\n    return x_min + (x_max - x_min) * (6*t**5 - 15*t**4 + 10*t**3) if N == 5 else x_min + (x_max - x_min) * t\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "result = np.correlate(a, b, mode='full')\nresult = np.roll(result, -result.size//2 + 1)\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = df.groupby('major').apply(lambda x: x.values.reshape(1,4,5)).values.reshape(4,15,5)\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = df.groupby('major').apply(lambda x: x.values).values\nresult.shape\n", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.array([list(np.binary_repr(num, width=m)) for num in a]).astype(int)\n", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.array([list(np.binary_repr(i, width=m)) for i in a]).astype(int)\n", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "result = np.array([[int(bit) for bit in format(num, '0'+str(m)+'b')] for num in a])\nresult = np.bitwise_xor.reduce(result, axis=0)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nresult = (mean - 3*std_dev, mean + 3*std_dev)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\n\nlower_bound = mean - 2*std_dev\nupper_bound = mean + 2*std_dev\n\nresult = (lower_bound, upper_bound)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nthird_std_dev = 3 * std_dev\nlower_bound = mean - third_std_dev\nupper_bound = mean + third_std_dev\nresult = (lower_bound, upper_bound)\nreturn result\n\n# Call the function with the array a\nresult = f(a = example_a)\nprint(result)\n### END SOLUTION", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\n\n# calculate the 2nd standard deviation\nsecond_std_dev = 2 * std_dev\n\n# calculate the 2nd standard deviation interval\nlower_bound = mean - second_std_dev\nupper_bound = mean + second_std_dev\n\n# create a boolean array where True indicates an outlier and False indicates a value within the 2nd standard deviation interval\nresult = (a < lower_bound) | (a > upper_bound)\nresult\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data.compressed(), percentile)\n", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows,:] = 0\na[:,zero_cols] = 0\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows-1, :] = 0\na[:, zero_cols-1] = 0\n", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1:, 0] = 0\na[1:, 1:] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.equal.outer(a, np.amax(a, axis=1)).astype(bool)\n", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.equal.reduce(a, axis=1)\n", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "import scipy.stats\n\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\n\nresult, _ = scipy.stats.pearsonr(post, distance)\nresult\n", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.einsum('ij,ik->ijk', X, X)[..., None]\n", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.sqrt(np.mean(Y, axis=(1, 2)))\n", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = number in a\n", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "C = A[~np.in1d(A, B)]\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "You can use numpy's `in1d` function to check if each element of `A` is in `B`. This function returns a boolean array of the same length as `A` where each value is `True` if the corresponding value in `A` is in `B` and `False` otherwise. Then use this boolean array to index `A` to get the desired output.\n\nHere is the code:\n\n```python\nC = A[np.in1d(A, B)]\n```\n\nThis code will return a new array `C` that contains only the elements of `A` that are in `B`. The order of the elements in `C` will be the same as they are in `A`.\n\nThis solution is efficient and should work faster than a loop for large arrays.", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "C = A[np.logical_or(A==B[0], A==B[1]) | np.logical_and(A>B[0], A<B[1])]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = len(a) + 1 - rankdata(a)\n", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "r = rankdata(a)\nresult = len(a) - r\nresult = result.astype(int)\n", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": "You can use the `rankdata` function from `scipy.stats` to rank your data. Then, subtract the rank from the length of the array plus one to get the reverse ranking.\n\nHere is the solution:\n\n```python\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef f(a):\n    # Get the rank of the array\n    ranks = rankdata(a)\n    # Subtract the rank from the length of the array plus one to get the reverse ranking\n    reverse_rank = len(a) + 1 - ranks\n    return reverse_rank.astype(int)\n\nexample_a = [1,2,3,4,3,2,3,4]\nprint(f(example_a))\n```\n\nThis will output:\n\n```python\n[7 6 3 1 3 6 3 1]\n```\n\nThis code works by first using the `rankdata` function to rank the elements in the array. The `rankdata` function assigns a rank to each element based on its order in the array. Then, it subtracts the rank from the length of the array plus one to get the reverse ranking. The `astype(int)` function is used to convert the resulting array to integers.", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))\n", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))\n", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:, second, third]\n", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20,10,10,2))\nEND SOLUTION", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "result = X / np.abs(X).sum(axis=1, keepdims=1)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "result = X / np.linalg.norm(X, ord=2, axis=1, keepdims=True)\n", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "result = np.divide(X, np.abs(X).max(axis=1)[:, None])\n", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = [df['a'].astype(str).str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "from scipy.spatial.distance import cdist\n\nresult = cdist(a, a)\n", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "from scipy.spatial.distance import cdist\n\nresult = cdist(a, a)\n", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "from scipy.spatial.distance import cdist\n\nresult = cdist(a, a)\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(np.array(A, dtype=float))\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "import math\n\n# Convert 'inf' to math.inf and the strings to floats\nNA = np.array([math.inf if a == 'inf' else float(a) for a in A])\n\n# Now calculate the mean\nAVG = np.mean(NA)\nprint(AVG)\nEND SOLUTION", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "import numpy as np\nA = ['np.inf', '33.33', '33.33', '33.37']\n\n# Convert 'np.inf' to np.inf and the rest of the elements to float\nNA = [np.inf if x=='np.inf' else float(x) for x in A]\n\nAVG = np.mean(NA)\nprint(AVG)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "result = a[np.append(True, (a != np.roll(a,1)) & (a != 0))]\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "result = np.where(np.diff(a) != 0)\nresult = a[result]\n", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": "df = pd.DataFrame(columns=['lat', 'lon', 'val'])\n\nfor i in range(len(example_lat)):\n    for j in range(len(example_lat[i])):\n        df = df.append({'lat': example_lat[i][j], 'lon': example_lon[i][j], 'val': example_val[i][j]}, ignore_index=True)\n\nreturn df\n\n### END SOLUTION\n\n# Then you can call the function like this:\ndf = f()\nprint(df)", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df.max(axis=1)\ndf\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "result = []\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        y_start = max(0, i - size[0]//2)\n        y_end = min(a.shape[0], i + size[0]//2 + 1)\n        x_start = max(0, j - size[1]//2)\n        x_end = min(a.shape[1], j + size[1]//2 + 1)\n        result.append(a[y_start:y_end, x_start:x_end])\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "result = []\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        y_start = max(0, i - size[0]//2)\n        y_end = min(a.shape[0], i + size[0]//2 + 1)\n        x_start = max(0, j - size[1]//2)\n        x_end = min(a.shape[1], j + size[1]//2 + 1)\n        result.append(a[y_start:y_end, x_start:x_end])\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.nanmean(a.real) + 1j * np.nanmean(a.imag)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": "You can use the `np.nanmean` function which computes the mean of an array ignoring any `nan` or `inf` values.\n\n```python\ndef f(a = np.array([1 + 0j, 2 + 3j, np.inf + 0j])):\n    a = np.nan_to_num(a, nan=0)\n    return np.nanmean(a.real), np.nanmean(a.imag)\n```\n\nThis will return the mean of the real and imaginary parts separately.\n\nNote: `np.nan_to_num` function is used to replace `nan` and `inf` values with zeros in the array.\n\nIf you want to get a single value as a result, you can take the average of the means:\n\n```python\ndef f(a = np.array([1 + 0j, 2 + 3j, np.inf + 0j])):\n    a = np.nan_to_num(a, nan=0)\n    return (np.nanmean(a.real) + np.nanmean(a.imag) * 1j)\n```\n\nThis will return a single complex number as a result.", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[...,-1:]\n", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:]\n", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = any(np.array_equal(c, x) for x in CNTS)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "result = any(np.array_equal(c, x) for x in CNTS)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "x = np.array([0, 1])\ny = np.array([0, 1])\nf = intp.interp2d(x, y, a, kind='linear')\nresult = f(x_new, y_new)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df = pd.DataFrame(data)\ndf[name] = df.groupby('D')['Q'].cumsum()\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.diag(i)\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a = np.fill_diagonal(a, 0)\n", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_u = start.value//10**9\nend_u = end.value//10**9\nresult = pd.DatetimeIndex((pd.np.linspace(start_u, end_u, n)).astype(int), unit='s')\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0]\nif result.size > 0:\n    result = result[0]\nelse:\n    result = -1\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))\nresult = indices[0]\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "coeffs = np.polyfit(x, y, 2)\nresult = coeffs[::-1]\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "coeffs = np.polyfit(x, y, degree)\nresult = coeffs[::-1]\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "df = df.sub(a, axis=0)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,kl->ijl', A, B)\n", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_reshaped = np.ravel(a).reshape(-1, 1)\nscaler.fit_transform(a_reshaped)\nresult = scaler.inverse_transform(scaler.transform(a_reshaped)).reshape(a.shape)\n", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a.reshape(-1, a.shape[-1])).reshape(a.shape)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "arr_temp = arr.copy()\nmask = arr_temp < -10\nmask2 = arr_temp >= 15\narr[mask] = 0\narr[~mask & ~mask2] = arr_temp[~mask & ~mask2] + 5\narr[mask2] = 30\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "for i in range(len(n1)):\n    mask = arr[i] < n1[i]\n    mask2 = arr[i] >= n2[i]\n    arr[i][mask] = 0\n    arr[i][~mask & ~mask2] += 5\n    arr[i][mask2] = 30\n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "result = np.nonzero(np.abs(s1 - s2) > 1e-10)[0].shape[0]\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "result = np.sum(np.isnan(s1) != np.isnan(s2)) + np.sum((np.isnan(s1) == False) & (np.isnan(s2) == False) & (np.abs(s1 - s2) > 1e-10))\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = all(i.all() == a[0].all() for i in a)\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = np.all(np.isnan(a))\n", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])))\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])))\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": "You can use the `numpy.pad()` function to pad an array. Here's how you can do it:\n\n```python\ndef f(arr, shape=(93,13)):\n    pad_width = ((0, shape[0] - arr.shape[0]), (0, shape[1] - arr.shape[1]))\n    result = np.pad(arr, pad_width, mode='constant')\n    return result\n```\n\nIn the above code, `pad_width` is a tuple of tuples where each tuple specifies the number of values that should be padded to the start and end of each axis of the array. The `mode` parameter is set to `'constant'` which means that the padding values are constant and not dependent on the data.\n\nYou can call this function with your array and the desired shape as arguments:\n\n```python\nresult = f(example_arr, shape=(93,13))\n```\n\nThis will return a new array with the same data but with the shape adjusted to match the desired shape. If the array is smaller than the desired shape, the data will be centered in the new array and the rest of the new array will be padded with zeros. If the array is larger than the desired shape, the extra values will be truncated.", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = a.reshape(int(a.shape[0]/3), 3)\n", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = np.take_along_axis(a, np.expand_dims(b, axis=-1), axis=-1).squeeze(-1)\n", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = np.take_along_axis(a, np.expand_dims(b, axis=-1), axis=-1).squeeze(-1)\n", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = np.take_along_axis(a, b[..., np.newaxis], axis=-1).squeeze(-1)\n", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b])\n", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "mask = np.arange(a.shape[2]) != b[:,:,None]\nresult = a[mask].sum()\n", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "df.loc[df['a'].between(2,4), 'b']\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "result = im[1:-1, 1:-1]\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "def trim_zeros(A):\n    rows = np.any(A, axis=1)\n    cols = np.any(A, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n    return A[rmin:rmax+1, cmin:cmax+1]\n\nresult = trim_zeros(A)\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "mask = im.any(axis=1)\nresult = im[mask][:,1:]\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "result = im[1:-1, 1:-1]\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure()\nplt.scatter(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('x vs y')\nplt.legend(['x-y'])\nplt.show()", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.minorticks_y()\nplt.show()", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minorticks_x()\n\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "line_styles = ['-', '--', '-.', ':']\n\nfor i, style in enumerate(line_styles):\n    y = np.random.rand(10)\n    plt.plot(x, y, style, label=f'Line {i+1}')\n\nplt.legend()\nplt.show()", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "line_styles = ['-', '--', '-.', ':']\n\nfor i, style in enumerate(line_styles):\n    y = np.random.rand(10)\n    plt.plot(x, y, style, label=f'Line {i+1}')\n\nplt.legend()\nplt.show()", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, marker='d', linestyle='-', markeredgecolor='blue', markerfacecolor='red')\nplt.show()", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.plot(x, y, marker='d', markersize=10, linestyle='-')\nplt.show()", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set(ylim=(0, 40))\nplt.show()", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "plt.plot(x)\nplt.axvspan(2, 4, color='red', alpha=0.3)\nplt.show()", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "plt.plot([0, 1], [0, 2])\nplt.show()", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "plt.figure()\nplt.plot([0, 1], [0, 2])\nplt.show()", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", style=\"Gender\", data=df, kind=\"scatter\")\nplt.show()", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "plt.figure(figsize=(8, 6))\nsns.set(style=\"whitegrid\")\nplt.plot(x, y, marker='o')\nplt.title('Regular Matplotlib Style Plot Using Seaborn')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "df = pd.DataFrame({'x': x, 'y': y})\n\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=df, x='x', y='y')\nplt.title('Line plot of y = sin(x)')\nplt.show()", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, marker='+', linestyle=' ', markersize=20, markeredgewidth=7)\nplt.show()", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\nplt.show()", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.plot(x, y, label='cosine')\nplt.legend(title='Legend', title_fontsize='20')\nplt.show()", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "l.set_markerfacecolor((0, 0, 0, 0.2))\n\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30)\nplt.setp(l, ms=30, mfc='black', mec='black')\nplt.show()", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "l.set_color('red')\nl.set_markerfacecolor('red')", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\nplt.legend()\nplt.show()", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=-45)\nplt.legend()\nplt.show()", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "plt.xticks(np.arange(0, max(x), 2))\nplt.legend()\nplt.show()", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend()\nplt.show()", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.imshow(H, cmap='hot')\nplt.colorbar(label='Intensity')\nplt.show()", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.plot(x, y)\nplt.xlabel(\"X\")\nplt.xticks([0, np.pi, 2 * np.pi], ['0', '\u03c0', '2\u03c0'])\nplt.show()", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g = sns.boxplot(x=\"method\", y=\"orbital_period\", data=df)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.show()", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "plt.title(\"\\n\".join(textwrap.wrap(myTitle, width=20)), fontsize=10)\n\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "plt.figure(figsize=(8, 6))\nplt.plot(x, y)\nplt.gca().invert_yaxis()\nplt.show()", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "plt.xticks([0, 1.5])\nplt.show()", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "plt.yticks([-1, 1])\nplt.show()", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure()\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.legend(loc='upper center')\nplt.show()", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, edgecolors='black', facecolors='blue')\nplt.show()", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "plt.plot(x, y, 'o-')\nplt.xticks(np.arange(min(x), max(x)+1, 1.0))\nplt.yticks(np.arange(min(y), max(y)+1, 1.0))\nplt.show()", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.ticklabel_format(style='plain', axis='y')\nplt.show()", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "To plot a dashed line on a seaborn lineplot, you can use the `linestyle` parameter in the `plot` function. Here's how you can do it:\n\n```python\nsns.lineplot(x=x, y=y)\nplt.plot(x, y, linestyle='--', color='red')\nplt.show()\n```\n\nIn this example, `linestyle='--'` will create a dashed line, and `color='red'` will make the line red. You can adjust the `linestyle` and `color` parameters to fit your needs.", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, sharex=True)\naxs[0].plot(x, y1)\naxs[0].set_title('Sine Function')\naxs[1].plot(x, y2)\naxs[1].set_title('Cosine Function')\nplt.show()", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, axs = plt.subplots(2)\n\naxs[0].plot(x, y1)\naxs[0].set_title('Sine Function')\naxs[0].spines['top'].set_color('none')\naxs[0].spines['bottom'].set_color('none')\naxs[0].spines['left'].set_color('none')\naxs[0].spines['right'].set_color('none')\n\naxs[1].plot(x, y2)\naxs[1].set_title('Cosine Function')\naxs[1].spines['top'].set_color('none')\naxs[1].spines['bottom'].set_color('none')\naxs[1].spines['left'].set_color('none')\naxs[1].spines['right'].set_color('none')\n\nplt.show()", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\nplt.show()", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.tick_params(\n    axis='x',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected\n    labelbottom=False) # labels along the bottom edge are off\n\nplt.show()", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks([3, 4], ['Third', 'Fourth'])\nplt.grid(axis='x')\nplt.show()", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks([3, 4])\nplt.grid(axis='y', which='both')\nplt.show()", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks([3, 4])\nplt.xticks([1, 2])\nplt.grid(axis='y', which='both', linestyle='-', linewidth=0.5)\nplt.grid(axis='x', which='both', linestyle='-', linewidth=0.5)\nplt.show()", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n\n# show the plot\nplt.show()", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.tight_layout()\nplt.show()\nplt.clf()", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\n\nplt.legend()\n\nplt.show()", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\n\n# This turns off the frame\nax.set_frame_on(False)\n\n# Set the ticks on the axes\nax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)\nax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n\n# Set the tick labels\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(column_labels, minor=False)\n\n# Set the heatmap labels\nax.set_xlabel('Row Labels')\nax.set_ylabel('Column Labels')\n\n# Rotate the x-axis labels for better visibility\nplt.xticks(rotation=45)\n\n# Display the heatmap\nplt.show()", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.plot(x, y)\nplt.xlabel('X', labelpad=20)\nplt.show()", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.xticks([])\nplt.show()", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.yticks(ticks=y, labels=y)\nplt.ylabel('Y-axis')\nplt.xlabel('X-axis')\nplt.title('Y over X')\nplt.show()", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y)\nplt.ylabel('Y')\nplt.yticks(y)\nplt.yticks(y, [])\nplt.ylabel('Y', rotation=0, ha='right')\nplt.show()", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', scatter_kws={\"color\": \"green\"}, line_kws={\"color\": \"green\"})\nsns.distplot(tips['tip'], color='blue')\n\nplt.show()", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', line_kws={'color':'green'}, hist_kws={'color':'blue'})\n\nplt.show()", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "joint_plot = sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg')\n\nplt.show()", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 6))\n\nbar_width = 0.35\n\nbar1 = plt.bar(df['celltype'], df['s1'], bar_width, label='s1')\nbar2 = plt.bar(df['celltype'], df['s2'], bar_width, bottom=df['s1'], label='s2')\n\nplt.xlabel('celltype')\nplt.ylabel('Values')\nplt.title('Bar plot of s1 and s2 using celltype')\nplt.xticks(rotation=0)\nplt.legend()\n\nplt.tight_layout()\nplt.show()", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 6))\n\n# Create a bar plot for s1 and s2\nplt.bar(df[\"celltype\"], df[\"s1\"], label='s1')\nplt.bar(df[\"celltype\"], df[\"s2\"], bottom=df[\"s1\"], label='s2')\n\n# Set the x-axis tick labels to be the celltype\nplt.xticks(df[\"celltype\"])\n\n# Rotate x-axis tick labels\nplt.xticks(rotation=45)\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y, color='blue')\nplt.xticks(color='red')\nplt.xlabel('X', color='red')\nplt.show()", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, color='red')\nplt.xlabel('X')\nplt.show()", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.xticks(rotation='vertical', fontsize=10)\nplt.show()", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "plt.axvline(0.22058956, color='r')\nplt.axvline(0.33088437, color='g')\nplt.axvline(2.20589566, color='b')\nplt.show()", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.imshow(rand_mat, cmap='hot', interpolation='nearest')\nplt.xticks(numpy.arange(len(xlabels)), xlabels)\nplt.yticks(numpy.arange(len(ylabels))[::-1], ylabels[::-1])\nplt.xlabel('X Labels')\nplt.ylabel('Y Labels')\nplt.title('Heatmap of Random Matrix')\nplt.colorbar()\nplt.show()", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\nrc(\"mathtext\", default=\"regular\")\n\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\n\nfig, axs = plt.subplots(2)\n\naxs[0].plot(time, Swdown, \"-\", label=\"Swdown\")\naxs[0].plot(time, Rn, \"-\", label=\"Rn\")\naxs[0].legend(loc=0)\naxs[0].grid()\naxs[0].set_xlabel(\"Time (h)\")\naxs[0].set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\n\naxs[1].plot(time, temp, \"-r\", label=\"temp\")\naxs[1].legend(loc=0)\naxs[1].grid()\naxs[1].set_xlabel(\"Time (h)\")\naxs[1].set_ylabel(r\"Temperature ($^\\circ$C)\")\n\nplt.tight_layout()\nplt.show()\nplt.clf()", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\naxs[0].plot(x, y)\naxs[0].set_title('Y')\n\naxs[1].plot(x, y)\naxs[1].set_title('Y')\n\nplt.tight_layout()\nplt.show()", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(\n    data=df,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    s=30,\n)\nplt.show()", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "plt.scatter(a, b)\n\nfor i, txt in enumerate(c):\n    plt.annotate(txt, (a[i], b[i]))\n\nplt.xlabel('a')\nplt.ylabel('b')\nplt.title('Scatter Plot of a over b')\nplt.show()", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\")\nplt.show()", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\", title_fontsize='large', title_fontweight='bold')\nplt.show()", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, edgecolor='black', linewidth=1.2)\nplt.show()", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, axs = plt.subplots(1, 2, gridspec_kw={'width_ratios':[3, 1]})\n\naxs[0].plot(x, y)\naxs[1].plot(y, x)\n\nplt.tight_layout()\nplt.show()", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins=bins, alpha=0.5, label='x')\nplt.hist(y, bins=bins, alpha=0.5, label='y')\nplt.legend(loc='upper right')\nplt.show()", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.hist([x, y], stacked=True, bins=8, color=['blue', 'green'])\nplt.show()", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "plt.figure()\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n\n# calculate the slope and y-intercept of the line\nm = (d - b) / (c - a)\nc = b - m * a\n\n# create an array of x values from 0 to 5\nx = [0, 5]\n\n# calculate the corresponding y values\ny = [m * xi + c for xi in x]\n\n# plot the line\nplt.plot(x, y, '-r')\n\n# show the plot\nplt.show()", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, axs = plt.subplots(2)\n\n# create a colormap for x\ncax1 = axs[0].imshow(x, interpolation='nearest')\naxs[0].set_title('Colormap for x')\n\n# create a colormap for y\ncax2 = axs[1].imshow(y, interpolation='nearest')\naxs[1].set_title('Colormap for y')\n\n# add a colorbar for both subplots\nfig.colorbar(cax1, ax=axs.ravel().tolist())\n\nplt.show()", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure()\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2)\nfig.suptitle('Y and Z')\n\naxs[0].plot(x, y)\naxs[0].set_title('Y over X')\n\naxs[1].plot(a, z)\naxs[1].set_title('Z over A')\n\nplt.tight_layout()\nplt.show()", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.figure()\n\n# Extracting x and y coordinates\nx = [p[0] for p in points]\ny = [p[1] for p in points]\n\n# Plotting the points\nplt.plot(x, y, marker='o')\n\n# Setting the y-axis to log scale\nplt.yscale('log')\n\n# Displaying the plot\nplt.show()", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.plot(x, y)\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.plot(x, y)\nax.set_xticks(np.arange(1, 11))\nax.set_yticks(np.arange(1, 11))\nplt.show()", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "for i in range(len(lines)):\n    xs = [point[0] for point in lines[i]]\n    ys = [point[1] for point in lines[i]]\n    plt.plot(xs, ys, color=c[i])\n\nplt.show()", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.figure(figsize=(10, 6))\nplt.loglog(x, y, basex=10, basey=10)\n\n# mark the axes with numbers like 1, 10, 100\nxticks = [1, 10, 100, 1000]\nyticks = [1, 10, 100, 1000]\nplt.xticks(xticks, xticks)\nplt.yticks(yticks, yticks)\n\n# do not use scientific notation\nplt.ticklabel_format(style='plain', axis='both')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-Log Plot of y over x')\nplt.grid(True)\nplt.show()", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure()\n\n# plot A\nplt.plot(df.index, df['A'], label='A')\nplt.scatter(df.index, df['A'], color='blue')\n\n# plot B\nplt.plot(df.index, df['B'], label='B')\nplt.scatter(df.index, df['B'], color='red')\n\n# plot C\nplt.plot(df.index, df['C'], label='C')\nplt.scatter(df.index, df['C'], color='green')\n\n# plot D\nplt.plot(df.index, df['D'], label='D')\nplt.scatter(df.index, df['D'], color='yellow')\n\nplt.legend(loc='best')\nplt.show()", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "plt.hist(data, bins=[0, 1000, 2000, 3000, 4000, 5000, 6000, 16000], edgecolor='black')\n\n# Renormalize the data to sum up to 1\ndata_normalized = [i / sum(data) for i in data]\n\n# Format the y tick labels into percentage and set y tick labels as 10%, 20%, etc.\nplt.yticks(data_normalized, [f'{int(x*100)}%' for x in data_normalized])\n\nplt.show()", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=5, markerfacecolor='red', markeredgecolor='red', alpha=0.5, linestyle='-')\nplt.show()", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\naxs[0].plot(x, y, label='y')\naxs[1].plot(z, a, label='a')\n\n# Create a legend for the entire figure\nfig.legend(loc='upper center', bbox_to_anchor=(0.5, 1.02), ncol=2, fancybox=True)\n\nplt.show()", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, figsize=(10, 10))\n\nsns.regplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", ax=axs[0])\naxs[0].set_title(\"Bill Depth vs Bill Length\")\n\nsns.regplot(data=df, x=\"bill_length_mm\", y=\"flipper_length_mm\", ax=axs[1])\naxs[1].set_title(\"Flipper Length vs Bill Length\")\n\nplt.tight_layout()\nplt.show()", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax2 = ax.twiny()\nax2.set_xlim(ax.get_xlim())\nax2.set_xticks(ax.get_xticks())\nlabels = [item.get_text() for item in ax.get_xticklabels()]\nlabels = ['second' if int(label) == 1 else label for label in labels]\nax2.set_xticklabels(labels)\n\nplt.show()", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label='$\\lambda$')\nplt.legend()\nplt.show()", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "plt.xticks(range(0, 10, 2), [str(i) for i in range(0, 10, 2)] + ['2.1', '3', '7.6'])\nplt.show()", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60, ha='left')\n\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.yticks(rotation=-60)\nplt.xticks(va='top')\n\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.xticks(alpha=0.5)\n\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=-0.5, y=0.1)\nplt.show()", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=-0.5, y=0.1)\nplt.show()", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(1, 2)\nfig.suptitle('Figure')\n\naxs[0].plot(x, y)\naxs[1].plot(x, y)\n\nplt.show()", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, marker='|', hatch='//')\nplt.show()", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolors='none', marker='|')\nplt.show()", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*', hatch='\\\\')\nplt.show()", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, s=100, marker='*', edgecolors='black', linewidths=1)\nplt.show()", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "plt.figure()\nplt.imshow(data, cmap='hot', extent=[1, 5, 1, 4])\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.show()", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(y, x, orientation='horizontal')\nplt.show()", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=[c[key] for key in d])\nplt.show()", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "import matplotlib.pyplot as plt\n\n# Create a list of x-coordinates for the line\nx = [3, 3]\n\n# Create a list of y-coordinates for the line\ny = [0, 1]\n\n# Plot the line\nplt.plot(x, y, label='cutoff')\n\n# Add a legend\nplt.legend()\n\n# Display the plot\nplt.show()", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(6, 6))\nax = plt.subplot(111, projection='polar')\nbars = ax.bar(range(2), height, width=0.5)\nax.set_xticks(range(2))\nax.set_xticklabels(labels)\n\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels = l, wedgeprops = {'width' : 0.4})\nplt.show()", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b--')\nplt.grid(True)\nplt.show()", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\nplt.grid(which='major', linestyle=' ', linewidth='0')\nplt.show()", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, wedgeprops={\"edgecolor\":\"black\"})\nplt.title(\"Pie Chart of Daily Activities\")\nplt.show()", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, wedgeprops={\"edgecolor\":\"black\"})\nplt.title(\"Pie Chart of Daily Activities\")\nplt.show()", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, marker='o', markersize=5, markerfacecolor='none', markeredgecolor='blue')\nplt.show()", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(x=55, color='green', linestyle='--')\n\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "barWidth = 0.3\nfig = plt.subplots(figsize =(12, 8))\n\n# Set position of bar on X axis\nbar1 = np.arange(len(blue_bar))\nbar2 = [x + barWidth for x in bar1]\n\n# Create bar plot on the basis of the given data\nplt.bar(bar1, blue_bar, color ='blue', width = barWidth, edgecolor ='grey', label ='Blue Bar')\nplt.bar(bar2, orange_bar, color ='orange', width = barWidth, edgecolor ='grey', label ='Orange Bar')\n\n# Adding labels\nplt.xlabel('X-axis', fontsize = 15)\nplt.ylabel('Y-axis', fontsize = 15)\nplt.title('Side-by-side bar plot')\n\n# Adding legend\nplt.legend()\n\n# Show plot\nplt.show()", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, ax1 = plt.subplots()\n\n# Plot y over x in the first subplot\nline1 = ax1.plot(x, y, color='tab:red', label='y')\nax1.set_xlabel('x')\nax1.set_ylabel('y', color='tab:red')\nax1.tick_params('y', colors='tab:red')\n\n# Create a second y-axis that shares the same x-axis\nax2 = ax1.twinx()\n# Plot z over a in the second subplot\nline2 = ax2.plot(x, z, color='tab:blue', label='z')\nax2.set_ylabel('z', color='tab:blue')\nax2.tick_params('y', colors='tab:blue')\n\n# Add a legend to the first subplot\nlines = line1 + line2\nlabels = [l.get_label() for l in lines]\nax1.legend(lines, labels, loc='upper left')\n\nplt.show()", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\nplt.colorbar(label='y-value')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter plot of y over x with color based on y-value')\nplt.show()", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.plot(x, y)\nplt.xticks(np.arange(0, max(x)+1, 1))\nplt.show()", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "g = sns.catplot(data=df, x=\"sex\", y=\"bill_length_mm\", col=\"species\", kind=\"bar\", sharey=False, ci=None)\ng.despine(left=True)\nplt.show()", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure()\ncircle1 = plt.Circle((0.5, 0.5), 0.2, fill=False)\nplt.gca().add_patch(circle1)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.show()", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\boldsymbol{\\phi}$', fontsize=20)\nplt.show()", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\nplt.show()", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.plot(x, y, label=\"Line\")\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0., handlelength=0.3)\nplt.show()", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\nplt.show()", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend()\nplt.show()", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.imshow(data, cmap='hot', interpolation='nearest')\nplt.colorbar()\nplt.show()", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.plot(x, y)\nplt.title(r\"$\\bf{\" + 'Figure' + \"}$\" + ' 1')\nplt.show()", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\", legend=False)\nplt.show()", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.gca().invert_xaxis()\nplt.show()", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.scatter(x, y)\nplt.gca().set_clip_on(False)\nplt.show()", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, facecolor='red', edgecolor='black')\nplt.show()", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n\nfor ax in axs.flatten():\n    ax.plot(x, y)\n\nplt.show()", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.linspace(0, 10, 6), edgecolor='black')\nplt.show()", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(8, 6))\nplt.plot(x, y, 'k-', label='y = x + 1')\nplt.fill_between(x, y - error, y + error, alpha=0.2, label='Error')\nplt.legend()\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot of y with error shading')\nplt.grid(True)\nplt.show()", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.axhline(0, color='white')\nplt.axvline(0, color='white')\n\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, axs = plt.subplots(2, sharex=True)\nfig.suptitle('Y vs Z')\n\naxs[0].plot(x, y)\naxs[0].set_title('Y')\n\naxs[1].plot(a, z)\naxs[1].set_title('Z')\n\nplt.tight_layout()\nplt.show()", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].tick_params(axis='both', which='major', labelsize=5)\nplt.tight_layout()\nplt.show()", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.figure(figsize=(8, 8))\nplt.matshow(d, fignum=1)\nplt.show()", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "import matplotlib.pyplot as plt\nimport matplotlib.table as table\n\nfig, ax = plt.subplots()\nax.set_title('Penguins Data')\n\ncolLabels = df.columns.values\nrowLabels = df.index.values\ncellText = df.values\n\ntab = table.table(ax, cellText=cellText, colLabels=colLabels, rowLabels=rowLabels, loc='center')\n\ntab.auto_set_font_size(False)\ntab.set_fontsize(10)\n\nax.set_xticks([])\nax.set_yticks([])\n\nplt.tight_layout()\nplt.show()", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.figure(figsize=(8,6))\n\n# Top x-axis\nax1 = plt.subplot(211)\nplt.plot(x, y, 'r')\nplt.xticks(visible=False)\nplt.ylabel('Y-axis')\n\n# Bottom x-axis\nax2 = plt.subplot(212, sharex=ax1)\nplt.plot(x, y, 'b')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\n\n# Show both sets of x-axis tick labels\nax1.xaxis.tick_top()\nax1.xaxis.set_label_position('top')\nax2.xaxis.tick_bottom()\nax2.xaxis.set_label_position('bottom')\n\nplt.tight_layout()\nplt.show()", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.figure(figsize=(8,6))\n\n# plot y over x\nplt.plot(x, y, color='blue')\n\n# show x axis ticks on both top and bottom of the figure\nax = plt.gca()\nax.xaxis.tick_top()\nax.xaxis.tick_bottom()\n\nplt.show()", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xticks(x)\nplt.yticks(y)\nplt.show()", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "g = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\ng.fig.subplots_adjust(top=0.85)\ng.fig.suptitle('Group: Fat', fontsize=16)\ng.col_wrap = 2\nfor ax in g.axes.flat:\n    ax.set_title('Group: No Fat')\nplt.show()", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", kind=\"scatter\", data=df)\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"Pulse\")\nplt.show()", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", kind=\"scatter\", data=df)\nplt.ylabel('')\nplt.show()", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\nplt.show()", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label=\"y\")\nplt.legend(frameon=False)\nplt.show()", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "plt.figure(figsize=(8, 6))\n\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='sin(t) + cos(t)')\n\nplt.legend()\nplt.show()", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "sns.stripplot(data=df, x=\"sex\", y=\"bill_length_mm\", hue=\"species\", dodge=True, jitter=True)\nplt.legend([],[], frameon=False)\nplt.show()", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "g = sns.FacetGrid(df, row=\"b\", hue=\"b\")\ng.map(sns.pointplot, \"a\", \"c\", order=df[\"a\"].sort_values().unique())\n\nfor ax, (_, rows) in zip(g.axes.flat, g.axes.flat):\n    xticks = rows.get_xticks()\n    xticklabels = [int(xtick) for xtick in rows.get_xticks()]\n    xticks = xticks[::2]\n    xticklabels = xticklabels[::2]\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xticklabels)\n\nplt.show()", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(x, y, z)\n\nax.view_init(azim=100, elev=50)\n\nplt.show()", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.show()", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x.assign(1)\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x.assign(114514)\n", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "num_classes = 10\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "num_classes = 10\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "num_classes = 10\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": "You can use the tf.one_hot function to convert your integer labels into one-hot vectors. Here's how you can do it:\n\n```python\nimport tensorflow as tf\n\nexample_labels = [0, 6, 5, 4, 2]\n\ndef f(labels=example_labels):\n    # Convert labels to a tensor\n    labels_tensor = tf.constant(labels, dtype=tf.int32)\n    \n    # Convert integer labels to one-hot vectors\n    result = tf.one_hot(labels_tensor, depth=10, dtype=tf.int32)\n    \n    return result\n\nprint(f(example_labels).numpy())\n```\n\nThis will output:\n\n```\n[[1 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 1 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0]]\n```\n\nThe tf.one_hot function converts integer labels into one-hot vectors. The depth parameter specifies the size of the dimension of the one-hot vectors. In your case, it's 10 because you have 10 classes. The dtype parameter specifies the data type of the output tensor. In your case, it's tf.int32.", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "num_classes = 10\nresult = tf.one_hot(labels, depth=num_classes)[:, ::-1]\nresult = tf.cast(result, tf.int32)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return tf.py_function(lambda i: [i, i+1, i+2], [i], [tf.int64])\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(my_map_func)\nds = ds.unbatch()\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": "def my_map_func(i):\n  return tf.data.Dataset.from_tensor_slices([i, i+1, i+2])\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.flat_map(my_map_func)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n\nThe solution is to use the flat_map function instead of map. The map function applies the transformation to each element in the dataset, but flat_map first applies the transformation to each element and then flatten the result into a single dataset.\nThis will give the desired output:\n[10, 11, 12, 20, 21, 22, 30, 31, 32]\n\nNote: The tf.compat.v1.py_func function is deprecated and will be removed in a future version. Instead, you can use tf.numpy_function or tf.map_fn.", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "max_length = 8\n\nrange_tensor = tf.range(max_length)\nrange_tensor = tf.expand_dims(range_tensor, 0)\nlengths_tensor = tf.expand_dims(lengths, 1)\nmask = tf.less(range_tensor, lengths_tensor)\nmask = tf.cast(mask, tf.int32)\n\nresult = mask\n", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "max_length = 8\n\nrange_tensor = tf.range(max_length)\nrange_tensor = tf.expand_dims(range_tensor, 0)\nlengths_tensor = tf.expand_dims(lengths, 1)\nmask = tf.less(range_tensor, lengths_tensor)\nmask = tf.cast(mask, tf.int32)\n\nresult = mask\n", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "max_length = 8\n\nranges = tf.range(max_length)[None, :]\nlengths = tf.constant(lengths)[:, None]\n\nresult = tf.cast(ranges < lengths, tf.float32)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": "max_length = 8\n\ndef f(lengths):\n    batch_size = tf.shape(lengths)[0]\n    max_length_tensor = tf.constant(max_length, dtype=tf.int32)\n    ones_tensor = tf.ones(shape=(batch_size, 1), dtype=tf.int32)\n    range_tensor = tf.range(max_length_tensor, dtype=tf.int32)\n    range_tensor = tf.reshape(range_tensor, shape=(1, max_length))\n    lengths_tensor = tf.reshape(lengths, shape=(batch_size, 1))\n    mask = tf.less(range_tensor, lengths_tensor)\n    return tf.cast(mask, tf.int32)\n\nresult = f(example_lengths)\n\nprint(result)\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "max_length = 8\n\nrange_tensor = tf.range(max_length)[tf.newaxis, :]\nlengths_tensor = tf.constant(lengths)[:, tf.newaxis]\n\nresult = tf.cast(range_tensor >= lengths_tensor, tf.float32)\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "indices = tf.meshgrid(\n    tf.range(tf.shape(a)[0]),\n    tf.range(tf.shape(b)[0]),\n    indexing='ij',\n)\nflattened_indices = tf.stack(indices, axis=-1)\n\nflattened_a = tf.gather(tf.reshape(a, (-1,)), flattened_indices[:, 0])\nflattened_b = tf.gather(tf.reshape(b, (-1,)), flattened_indices[:, 1])\n\nresult = tf.stack([flattened_a, flattened_b], axis=-1)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": "You can use tf.data.Dataset to create a cartesian product of two tensors. Here is an example:\n\n```python\nimport tensorflow as tf\n\nexample_a = tf.constant([1,2,3])\nexample_b = tf.constant([4,5,6,7])\n\ndef cartesian_product(a, b):\n    a_flat = tf.reshape(a, [-1])\n    b_flat = tf.reshape(b, [-1])\n    a_tiled = tf.tile(tf.expand_dims(a_flat, 1), [1, tf.shape(b_flat)[0]])\n    b_tiled = tf.tile(tf.expand_dims(b_flat, 0), [tf.shape(a_flat)[0], 1])\n    return tf.stack([a_tiled, b_tiled], axis=-1)\n\nresult = cartesian_product(example_a, example_b)\n```\n\nThis function first flattens the tensors a and b, then it creates two new tensors a_tiled and b_tiled, where each element of a is repeated for each element of b, and vice versa. Finally, it stacks the two tensors along the last axis to form a new tensor where each row contains a pair of elements from a and b.\n\nPlease note that this function assumes that the tensors a and b have the same shape or can be broadcasted to the same shape. If they don't, you'll need to add additional code to handle that case.", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "result = tf.squeeze(a, [2])\n", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "result = tf.expand_dims(a, axis=2)\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "result = tf.expand_dims(a, axis=[0, 3])\n", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = 1/A\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "squared_diff = tf.square(a - b)\nresult = tf.reduce_sum(squared_diff, axis=1)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "squared_diff = tf.square(a - b)\nresult = tf.reduce_sum(squared_diff, axis=0)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": "You can use the tf.math.squared_difference function to get the squared difference between the two tensors, and then use tf.reduce_sum with the axis parameter set to 1 to sum the squared differences along the rows:\n\n```python\ndef f(A=example_a,B=example_b):\n    squared_diff = tf.math.squared_difference(A, B)\n    result = tf.reduce_sum(squared_diff, axis=1)\n    return result\n```\n\nThis will give you the element-wise L2 distance between the rows of A and B.", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "indices = tf.stack([y, z], axis=1)\nm = tf.gather_nd(x, indices)\nresult = m\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "indices = tf.transpose([row, col])\nm = tf.gather_nd(x, indices)\nresult = m\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": "You can use tf.gather_nd to get the elements from the x tensor based on the y and z indices.\n\nHere is how you can do it:\n\n```python\nindices = tf.stack([example_y, example_z], axis=1)\nm = tf.gather_nd(example_x, indices)\n```\n\nIn this code, tf.stack is used to combine example_y and example_z into a 2D tensor with shape [2, 2], where the first dimension represents the index of the row and the second dimension represents the index of the column.\n\nThen, tf.gather_nd is used to gather elements from example_x based on the indices. The result is a 1D tensor with shape [2], which contains the elements at the positions specified by the indices.\n\nSo, m will be a Tensor with values [2, 6].", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "result = tf.einsum('bij,bjk->bik', A, B)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "result = tf.einsum('bij,bjk->bik', A, B)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "result = [i.decode('utf-8') for i in x]\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": "You can use the `tf.strings.unicode_transcode` function to convert bytes to strings. Here is how you can do it:\n\n```python\ndef f(x):\n    x = tf.constant(x)\n    x = tf.strings.unicode_transcode(x, \"UTF-8\", \"UTF-8\")\n    return x.numpy().tolist()\n```\n\nThis function takes three arguments: the input bytes tensor, the input encoding, and the output encoding. In your case, both the input and output encodings are \"UTF-8\".\n\nPlease note that you need to convert the tensorflow tensor to a normal python list using `numpy().tolist()`.\n\nAlso, please note that you need to have TensorFlow 2.3 or later to use the `tf.strings.unicode_transcode` function. If you are using an older version of TensorFlow, you can convert bytes to strings using the `decode` method:\n\n```python\ndef f(x):\n    return [i.decode(\"utf-8\") for i in x]\n```", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "mask = x != 0\nsums = tf.reduce_sum(x, axis=-2, keepdims=True)\ncounts = tf.reduce_sum(mask, axis=-2, keepdims=True)\nresult = sums / counts\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "mask = x != 0\naverage = tf.math.reduce_mean(x, axis=-2, keepdims=True)\nsquared = tf.math.square(x - average)\nsum_squared = tf.math.reduce_sum(squared, axis=-2, keepdims=True)\ncount = tf.math.reduce_sum(mask, axis=-2, keepdims=True)\nvariance = sum_squared / (count - 1)\nresult = tf.where(mask, variance, tf.zeros_like(variance))\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "You can solve this problem by first creating a mask tensor that indicates where in `x` the zeros are located. Then, you can use this mask to exclude the zeros from the average calculation.\n\nHere is the solution:\n\n```python\ndef f(x):\n    mask = x != 0\n    sum_x = tf.reduce_sum(tf.boolean_mask(x, mask), axis=-2, keepdims=True)\n    count_non_zero = tf.reduce_sum(tf.cast(mask, tf.float32), axis=-2, keepdims=True)\n    result = sum_x / count_non_zero\n    return result\n\nprint(f(example_x))\n```\n\nThis code first creates a mask tensor where `True` indicates non-zero values and `False` indicates zeros. Then it uses `tf.boolean_mask` to select only the non-zero values from `x` and sums them along the second-to-last dimension (axis=-2). It also counts the number of non-zero values in `x` by casting the mask tensor to `float32` and summing it along the same axis. Finally, it divides the sum of non-zero values by the count of non-zero values to get the average. The result is broadcasted to the same shape as `x` by the division operation.", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "import tensorflow as tf\n\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B))\n\nEND SOLUTION\n<code>\n\nIn TensorFlow 2.x, eager execution is enabled by default. This means that you can run operations directly without having to use a tf.Session. So, you don't need to use a session to run the operation. The above code will give the same result as the original code but without the need for a session.\n\nThe speed difference you are seeing is due to the difference in the way TensorFlow 1.x and TensorFlow 2.x handle eager execution. In TensorFlow 1.x, eager execution is an opt-in feature that you have to enable by creating a tf.Session. In TensorFlow 2.x, eager execution is enabled by default and you don't need to use a tf.Session to run operations. This can result in significant speed improvements for simple operations.\n\nHowever, please note that TensorFlow 2.x is not fully backward compatible with TensorFlow 1.x. This means that some TensorFlow 1.x code may not run directly in TensorFlow 2.x due to changes in the way TensorFlow handles eager execution and other features.", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "result = tf.argmax(a, axis=1)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "result = tf.argmax(a, axis=1)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": "You can use the tf.argmax function to get the indices of the maximum values along an axis. Here is how you can do it:\n\n```python\ndef f(a=example_a):\n    result = tf.argmax(a, axis=1)\n    return result\n```\n\nThe `tf.argmax` function returns the indices of the maximum values along an axis. In your case, you want to find the index of the maximum value in each row (axis=1), so you pass `axis=1` to the function.\n\nSo if you call `f(example_a)`, it will return:\n\n```python\n<tf.Tensor: shape=(3,), dtype=int64, numpy=array([5, 4, 0], dtype=int64)>\n```\n\nThis indicates that the maximum values are at the 5th, 4th, and 0th index in each row of the input tensor.", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=0)\n", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "model.save(\"my_model\")\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "tf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[10], minval=1, maxval=5, dtype=tf.int32)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "tf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[114], minval=2, maxval=6, dtype=tf.int32)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": "tf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[10], minval=1, maxval=5, dtype=tf.int32)\nreturn result\n    ### END SOLUTION", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "result = tf.__version__\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "log_x = np.log(x)\ncoef = np.polyfit(log_x, y, 1)\nresult = np.array([coef[0], coef[1]])\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "coeffs = np.polyfit(x, np.log(y), 1)\nresult = np.array([np.exp(coeffs[1]), coeffs[0]])\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\n", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "from scipy import stats\n\n# Perform the two-sample KS test\nstat, p_value = stats.ks_2samp(x, y)\n\n# Check if p-value is less than alpha to reject the null hypothesis\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c) - 2)**2 + ((3*a - b - c))**2 + sin(b) + cos(b) + 4\n\nresult = optimize.minimize(f, initial_guess)\nresult.x\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = 1 - scipy.stats.norm.cdf(z_scores)\n", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = [scipy.stats.norm.sf(abs(z), loc=mu, scale=sigma) for z in z_scores]\np_values = np.array(p_values)\n", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = scipy.stats.norm.ppf(1-np.array(p_values))\n", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([stddev],loc=0)\nresult =  dist.cdf(x)\n", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm([stddev],loc=mu)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa.multiply(sb)\n", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": "You can use the `diagonal` method of the sparse matrix to get the diagonal elements as a dense array, perform the multiplication, and then convert it back to a sparse matrix. Here is how you can do it:\n\n```python\ndef f(sA = example_sA, sB = example_sB):\n    # Extract the diagonal of sA\n    diag_sA = np.diag(sA)\n    # Multiply it with sB\n    result_array = diag_sA * sB.A1\n    # Convert it back to a sparse matrix\n    result = sparse.csr_matrix(np.diag(result_array))\n    return result\n```\n\nThis function will return a sparse matrix where each row and column is multiplied by the corresponding element in `sB`. If `sB` is a 1D array, you can use `sB.A1` to get a 1D array from `sB`.\n\nPlease note that this function assumes that `sB` is a 1D array with the same length as the number of rows in `sA`. If `sB` is a matrix, you need to adjust the function accordingly.", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "interpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "interpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle)\n\n# Center of rotation\ncx, cy = data_orig.shape[0]/2, data_orig.shape[1]/2\n\n# Rotation matrix\ntheta = np.deg2rad(angle)\nR = np.array([[np.cos(theta), -np.sin(theta)],\n              [np.sin(theta), np.cos(theta)]])\n\n# Translation matrix\nT = np.array([[1, 0, cx],\n              [0, 1, cy]])\n\n# Inverse translation matrix\nT_inv = np.array([[1, 0, -cx],\n                  [0, 1, -cy]])\n\n# Point in original frame\np = np.array([x0, y0])\n\n# Point in rotated frame\np_rot = np.dot(T_inv, np.dot(R, np.dot(T, p)))\n\nxrot, yrot = p_rot\n\n# Ensure integer values\nxrot, yrot = int(xrot), int(yrot)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = M.diagonal()\n", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "times_uniform = np.interp(times, (times[0], times[-1]), (0, 1))\nresult = stats.kstest(times_uniform, \"uniform\")\nprint(result)\n", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": "    # First, we need to define the cumulative distribution function (cdf) of the uniform distribution.\n    # The cdf of a uniform distribution is given by F(x) = 1 - (b - x) / (b - a) for x in [a, b]\n    # Here, a = 0 and b = T\n    def uniform_cdf(x):\n        return 1 - (example_T - x) / example_T\n\n    # Next, we need to convert the times to a numpy array and sort it.\n    times_np = np.array(times)\n    times_np.sort()\n\n    # Then, we calculate the empirical distribution function (empirical cdf) of the times.\n    # The empirical cdf of a random variable X is given by F_n(x) = (number of values in X less than or equal to x) / (number of values in X)\n    empirical_cdf = lambda x: np.sum(times_np <= x) / len(times_np)\n\n    # Finally, we can perform the Kolmogorov-Smirnov test.\n    # The Kolmogorov-Smirnov test statistic is the maximum absolute difference between the empirical cdf and the uniform cdf.\n    ks_test_statistic = max(abs(uniform_cdf(x) - empirical_cdf(x)) for x in times_np)\n\n    # The null hypothesis of the K-S test is that the data are drawn from the uniform distribution.\n    # We reject the null hypothesis if the test statistic is greater than a critical value.\n    # The critical value is often chosen as the square root of the true rate parameter.\n    critical_value = np.sqrt(example_rate)\n\n    # If the test statistic is greater than the critical value, we reject the null hypothesis.\n    reject_null_hypothesis = ks_test_statistic > critical_value\n\n    return reject_null_hypothesis\n\nprint(f())\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "times_uniform = np.interp(times, (times[0], times[-1]), (0, 1))\nks_statistic, p_value = stats.kstest(times_uniform, \"uniform\")\nresult = p_value > 0.05\n", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = sparse.hstack([c1, c2])\n", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.hstack([c1, c2])\n", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])\n", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "dist_matrix = scipy.spatial.distance.cdist(points1, points2)\nresult = scipy.optimize.linear_sum_assignment(dist_matrix)\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "dist_matrix = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\nresult = scipy.optimize.linear_sum_assignment(dist_matrix)\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b = sparse.csr_matrix(np.diag(np.diag(b.toarray())))\nEND SOLUTION\nb", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "labels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "labels, num_labels = ndimage.label(img < threshold)\nresult = num_labels\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": "labels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\nreturn result\n### END SOLUTION\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "labels, num_labels = ndimage.label(img > threshold)\nsizes = np.bincount(labels.ravel())\ncentroids = ndimage.measurements.center_of_mass(img, labels, range(1, num_labels + 1))\nresult = [np.sqrt(c[0]**2 + c[1]**2) for c in centroids]\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = (M + M.T)/2\nM = M + M.T - np.diag(M.diagonal())\nM = sparse.csr_matrix(M)\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": "You can use the `maximum` function from `scipy.sparse` to combine the existing values in the sparse matrix with the transpose of the matrix. Here's how you can do it:\n\n```python\nfrom scipy.sparse import lil_matrix, csr_matrix, maximum\n\ndef make_symmetric(sA):\n    sA = maximum(sA, sA.transpose())\n    return sA\n\nexample_sA = lil_matrix((10, 10), dtype=np.int8)\nfor i in range(10):\n    for j in range(10):\n        if np.random.rand() < 0.1:\n            example_sA[i, j] = 1\n\nexample_sA = make_symmetric(example_sA)\n```\n\nIn this code, `maximum` combines the elements of the original matrix and its transpose, choosing the maximum value for each pair of elements. This ensures that the resulting matrix is symmetric.\n\nNote: The `maximum` function is not available for all sparse matrix formats. For example, it is not available for `dok_matrix` or `coo_matrix`. If you are using one of these formats, you will need to convert your matrix to a format that supports `maximum`, such as `csr_matrix` or `csc_matrix`.", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "def remove_isolated_ones(arr):\n    structure = np.ones((3, 3))\n    structure[1, 1] = 0\n    isolated_ones = scipy.ndimage.filters.minimum_filter(arr, footprint=structure)\n    isolated_ones = isolated_ones == 1\n    arr[isolated_ones] = 0\n    return arr\n\nsquare = remove_isolated_ones(square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "def remove_isolated_cells(image):\n    labeled_image, num_labels = scipy.ndimage.label(image)\n    label_sizes = np.bincount(labeled_image.ravel())\n    too_small = label_sizes > 1\n    too_small_mask = np.isin(labeled_image, np.where(too_small)[0])\n    isolated_cells_removed = np.where(too_small_mask, 0, image)\n    return isolated_cells_removed\n\nsquare = remove_isolated_cells(square)\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = col.mean()\nstandard_deviation = np.std(col.toarray())\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "Max = col.data.max()\nMin = col.data.min()\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "from scipy import stats\n\n# Get the data from the column\ndata = col.data\n\n# Calculate the median\nMedian = np.median(data)\n\n# Calculate the mode\nMode = stats.mode(data)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier_series(x, *a):\n    return sum(ai * np.cos(n * np.pi / tau * x) for n, ai in enumerate(a, 1))\n\npopt, pcov = curve_fit(fourier_series, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "from scipy.spatial.distance import cdist\n\n# Get unique IDs\nunique_ids = np.unique(example_array)\nunique_ids = unique_ids[unique_ids != 0]  # remove zero if exists\n\n# Initialize result list\nresult = []\n\n# Loop through each unique ID\nfor i in unique_ids:\n    # Get coordinates of the ID\n    y, x = np.where(example_array == i)\n    # Calculate pairwise distances\n    distances = cdist(np.column_stack((y, x)), np.column_stack((y, x)), 'euclidean')\n    # Remove self-distances\n    np.fill_diagonal(distances, np.inf)\n    # Get the minimum distance\n    min_distance = distances.min()\n    # Append to result\n    result.append((i, i, min_distance))\n\n# Convert result to numpy array\nresult = np.array(result)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "from scipy.spatial.distance import cdist\nfrom scipy.ndimage import label, measurements\n\n# Label unique IDs\nlabels, num_labels = label(example_array)\n\n# Get coordinates of unique IDs\ncoords = np.array(measurements.center_of_mass(example_array, labels, range(1, num_labels + 1)))\n\n# Calculate pairwise Manhattan distances\ndistances = cdist(coords, coords, metric='cityblock')\n\n# Convert to integer (from float due to possible floating point precision issues)\ndistances = distances.astype(int)\n\n# Set diagonal to a large value (to exclude self-comparisons)\nnp.fill_diagonal(distances, np.inf)\n\n# Print result\nprint(distances)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": "import itertools\n\ndef f(example_array):\n    # Get unique IDs\n    ids = np.unique(example_array)\n    ids = ids[ids != 0]  # remove background id (assumed to be 0)\n\n    # Initialize result list\n    result = []\n\n    # Loop over all unique ID pairs\n    for id1, id2 in itertools.combinations(ids, 2):\n        # Get binary arrays for each ID\n        mask1 = example_array == id1\n        mask2 = example_array == id2\n\n        # Get coordinates of cells with each ID\n        coords1 = np.argwhere(mask1)\n        coords2 = np.argwhere(mask2)\n\n        # Calculate pairwise distances\n        distances = scipy.spatial.distance.cdist(coords1, coords2)\n\n        # Get minimum distance\n        min_distance = distances.min()\n\n        # Append to result\n        result.append((id1, id2, min_distance))\n\n    return result\n\n# Test function\nresult = f(example_array)\nfor r in result:\n    print(f\"IDs: {r[0]}, {r[1]}. Minimum distance: {r[2]}\")", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "result = np.zeros((5, 100))\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\n    result[i] = interpolate.splev(x_val, tck, der = 0)\n", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "statistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "data = [x1, x2]\nresult = ss.anderson_ksamp(data)\nprint('Statistic: %.3f' % result.statistic)\nprint('Critical values:')\nfor i, critical_value in enumerate(result.critical_values):\n    print('\\t%i: %.3f' % (i+1, critical_value))\nif result.statistic < max(result.critical_values):\n    print('The distributions are the same (fail to reject H0)')\nelse:\n    print('The distributions are different (reject H0)')\n", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def kendall_tau(df, window):\n    cols = df.columns\n    tau_dict = {}\n    for col1 in cols:\n        for col2 in cols:\n            if col1 != col2:\n                tau_dict[f'{col1}{col2}'] = df[col1].rolling(window=window).apply(lambda x: stats.kendalltau(x, df[col2])[0])\n    return pd.concat([df, tau_dict], axis=1)\n\ndf = kendall_tau(df, 3)\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "result = sa.nnz == 0\n", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "result = (sa.nnz == 0)\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = np.block([[a[i]] for i in range(a.shape[0])])\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "result = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": "You can extract the pvalue from the result by indexing the result. Here is how you can do it:\n\n```python\ndef f(pre_course_scores = example_pre_course_scores, during_course_scores = example_during_course_scores):\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = result.pvalue\n    return p_value\n```\n\nYou can then call the function with your data to get the p-value:\n\n```python\np_value = f(pre_course_scores, during_course_scores)\nprint(p_value)\n```\n\nThis will print the p-value from the Wilcoxon rank-sum test.", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "import scipy.stats as stats\n\nkurtosis_result = stats.kurtosis(a, fisher=False, bias=False)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=True, bias=False)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "func = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = func(x, y)\n", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": "You can use scipy.interpolate.griddata to achieve this. Here is an example:\n\n```python\nimport numpy as np\nfrom scipy.interpolate import griddata\n\n# Create a grid of points\nx = np.linspace(-1, 1, 50)\ny = np.linspace(-2, 0, 50)\nxg, yg = np.meshgrid(x, y)\n\n# Create a grid of values\nz = (xg + yg)*np.exp(-6.0 * (xg * xg + yg * yg))\n\n# Define the points you want to interpolate at\ns = np.array([-0.5, 0.5])\nt = np.array([-1.5, -0.5])\n\n# Interpolate\nresult = griddata((x.ravel(), y.ravel()), z.ravel(), (s, t), method='cubic')\n```\n\nIn this example, `s` and `t` are the points at which you want to interpolate the values. The `griddata` function will return the interpolated values at these points. The `method='cubic'` argument specifies that the interpolation should be done using cubic interpolation.\n\nPlease note that the `griddata` function requires the points at which you want to interpolate to be in the form of a 2D array, where each row corresponds to a point. So `s` and `t` need to be reshaped to `(2, 1)` before being passed to the function.", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "import numpy as np\n\n# Create a mapping from point index to region index\npoint_region = vor.point_region\n\n# Initialize result array\nresult = np.zeros(len(extraPoints), dtype=int)\n\n# Iterate over extra points\nfor i, point in enumerate(extraPoints):\n    # Find the nearest point in the Voronoi diagram\n    nearest_point_index = np.argmin([np.linalg.norm(np.array(point)-np.array(points[j])) for j in range(len(points))])\n    # Get the region of the nearest point\n    region = point_region[nearest_point_index]\n    # Check if the extra point is inside the region\n    if (vor.points[region][0], vor.points[region][1]) in vor.vertices:\n        result[i] = region\n    else:\n        result[i] = -1  # or any other value indicating that the point is not inside any region\n\nprint(result)\n", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "import numpy as np\n\n# Function to check if a point is inside a polygon\ndef point_in_poly(x, y, poly):\n    n = len(poly)\n    inside = False\n\n    p1x, p1y = poly[0]\n    for i in range(n + 1):\n        p2x, p2y = poly[i % n]\n        if y > min(p1y, p2y):\n            if y <= max(p1y, p2y):\n                if x <= max(p1x, p2x):\n                    if p1y != p2y:\n                        xints = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n                    if p1x == p2x or x <= xints:\n                        inside = not inside\n        p1x, p1y = p2x, p2y\n\n    return inside\n\n# Initialize result array\nresult = np.zeros(len(points), dtype=int)\n\n# Iterate over extra points\nfor point in extraPoints:\n    # Iterate over voronoi regions\n    for i, region in enumerate(vor.regions):\n        if region:\n            polygon = [vor.vertices[j] for j in region]\n            # Check if point is inside polygon\n            if point_in_poly(point[0], point[1], polygon):\n                result[i] += 1\n\nprint(result)\n", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "data = []\nindices = []\nindptr = [0]\n\nfor i, vec in enumerate(vectors):\n    data.extend(vec)\n    indices.extend(range(len(vec)))\n    indptr.append(indptr[-1] + len(vec))\n\nresult = sparse.csr_matrix((data, indices, indptr), shape=(len(vectors), max_vector_size))\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant', origin=[0, 1])\n", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M.data[M.indptr[row]:M.indptr[row+1]][np.argwhere(M.indices[M.indptr[row]:M.indptr[row+1]] == column).flatten()[0]]\n", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = [M[i, j] for i, j in zip(row, column)]\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[:, i, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner, _ = scipy.integrate.quad(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n\nprob = NormalDistro(u,o2,x)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": "def NormalDistro(u, o2, x):\n    dev = abs((x-u)/o2)\n    P_inner, _ = scipy.integrate.quad(NDfx, -dev, dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n\n# Example usage:\nprob = NormalDistro(u=1, o2=3, x=2.5)\nprint(prob)\n\nThe error you're encountering is because you're trying to call a function as if it were a module, but it's actually a method of the scipy.integrate module. You should use scipy.integrate.quad instead of scipy.integrate(NDfx,-dev,dev). quad is a function that performs numerical integration of a function.\nAlso, you need to return the result of the integration in your function.\nThe function quad returns two values: the result of the integration and an estimate of the absolute error. In your case, you only care about the result of the integration, so you should assign it to P_inner like this: P_inner, _ = scipy.integrate.quad(NDfx, -dev, dev). The underscore (_) is a convention in Python to indicate that we're ignoring the second returned value from quad.", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "import numpy as np\n\ndef dctmtx(N):\n    \"\"\"\n    Returns the NxN DCT matrix.\n    \"\"\"\n    dct_1d = lambda x: np.sqrt(2/x) * np.cos(np.outer(np.arange(x), np.arange(x)).T)\n    dct_1d[0, :] *= np.sqrt(1/2)\n    dct_1d[:, 0] *= np.sqrt(1/2)\n    dct_1d[0, 0] /= np.sqrt(2)\n    \n    dct_2d = np.kron(dct_1d, dct_1d)\n    return dct_2d\n\nresult = dctmtx(N)\nprint(result)\nEND SOLUTION", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "diags = [matrix[i][:-1] for i in range(3)]\noffsets = [-1, 0, 1]\nresult = sparse.diags(diags, offsets, shape=(5, 5)).toarray()\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "import numpy as np\nimport scipy.special\n\nN = 100000\np = 0.5\n\n# Calculate q (1-p)\nq = 1 - p\n\n# Create an array of size N+1\nn = np.arange(N+1)\n\n# Use scipy.special.comb to calculate the binomial coefficients\n# and then use numpy's outer function to create the 2D matrix\nM = scipy.special.comb(n, np.outer(n, np.ones(N+1))) * (p**np.outer(n, np.ones(N+1))) * ((q)**(np.outer(n, np.ones(N+1))-n.reshape(-1,1)))\n\n# Set all non-integer values to 0\nM = np.around(M).astype(int)\n\nresult = M\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=0)\n", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x), axis=0)\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "zscore_df = pd.DataFrame()\nfor col in df.columns:\n    z_scores = [stats.zscore(df[col])]\n    zscore_df[col] = z_scores\n\nresult = pd.concat([df.rename(columns=lambda x: 'data ' + x), zscore_df.rename(columns=lambda x: 'zscore ' + x)], axis=1)\nresult\n", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "zscore_df = pd.DataFrame()\n\nfor col in df.columns:\n    z_scores = [stats.zscore(df[col])]\n    zscore_df[col] = np.round(z_scores, 3)\n\nzscore_df.index = ['zscore']*len(df)\nzscore_df.index.name = 'probegenes'\n\nresult = pd.concat([df.rename_axis('probegenes').add_suffix(' data'), zscore_df]).sort_index(level=0)\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, np.array(starting_point), np.array(direction))\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y, x = np.ogrid[:shape[0], :shape[1]]\ncenter = ((shape[0]-1)/2, (shape[1]-1)/2)\ndist_from_center = distance.cdist(np.dstack((y, x)), np.array([center]))\nresult = dist_from_center[:,:,0]\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.ogrid[:shape[0], :shape[1]]\ncenter = (shape[0] - 1) / 2, (shape[1] - 1) / 2\ndist_from_center = np.abs(x - center[1]) + np.abs(y - center[0])\nresult = dist_from_center\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": "Here is a solution:\n\n```python\nimport numpy as np\n\ndef f(shape = (6, 6)):\n    rows, cols = shape\n    y, x = np.ogrid[:rows, :cols]\n    cy, cx = np.array(shape) // 2  # center\n    return np.sqrt((x - cx)**2 + (y - cy)**2)\n\nprint(f((6, 6)))\n```\n\nThis function uses numpy's ogrid to create a 2D array of coordinates, then computes the Euclidean distance from the center of the array to each point. The result is a 2D array of the same shape as the input with each value representing the Euclidean distance from the center.", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "zoom_factors = np.array(shape) / np.array(x.shape)\nresult = scipy.ndimage.zoom(x, zoom_factors, order=1)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return a.dot(x ** 2)\n\ndef residual(x, a, y):\n    return np.sum((y - func(x, a)) ** 2)\n\nout = scipy.optimize.minimize(residual, x0, args=(a, y))['x']\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x, a):\n    return a.dot(x**2)\n\ndef residual(x, a, y):\n    return np.sum((y - func(x, a))**2)\n\nout = scipy.optimize.minimize(residual, x0, args=(a, y), method='L-BFGS-B', bounds=scipy.optimize.Bounds(x_lower_bounds, np.inf))\nprint(out.x)\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "def dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "def dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t) if 0 < t < 2*np.pi else 2*np.pi)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "def dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "for t in range(4):\n    cons.append({'type':'ineq', 'fun': lambda x, t=t: x[t]})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])\n", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.vstack([sa, sb])\n", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\n# Define the function to integrate\ndef integrand(x, c):\n    return 2*c*x\n\n# Define the range of c values\nc_values = np.loadtxt('cooltemp.dat', usecols=1)\n\n# Initialize an array to store the results\nresults = []\n\n# Loop over each value of c\nfor c in c_values:\n    # Define the integration function for the current c\n    integration_function = lambda x: integrand(x, c)\n    # Perform the integration\n    result, error = scipy.integrate.quad(integration_function, 0, 1)\n    # Append the result to the results list\n    results.append(result)\n\n# Convert the results list to a numpy array\nresults = np.array(results)\n\n# Now results contains the definite integral for each value of c\nprint(results)\nEND SOLUTION\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": "Here is the correct way to do it:\n\n```python\nimport numpy as np\nfrom scipy import integrate\n\n# Define the function to integrate\ndef func(x, c):\n    return 2*c*x\n\n# Define the range of c values\nc_values = np.linspace(0, 1, 100)  # 100 values between 0 and 1\n\n# Initialize an array to store the results\nI = []\n\n# Loop over each value of c\nfor c in c_values:\n    # Integrate the function from 0 to 1 with the current value of c\n    result, error = integrate.quad(lambda x: func(x, c), 0, 1)\n    I.append(result)\n\n# Convert the list to a numpy array\nI = np.array(I)\n```\n\nIn this code, `func` is the function to integrate, which is a simple linear function of `x` and `c`. The `integrate.quad` function is used to numerically integrate `func` from 0 to 1 with the current value of `c`. The results are stored in the list `I`. Finally, `I` is converted to a numpy array for convenience.", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "You can use the `.multiply` method to multiply sparse matrices by scalars. However, it doesn't directly support addition with a scalar. But you can combine it with `.add` method to achieve your goal.\n\nHere is how you can do it:\n\n```python\nV = V.multiply(x)\n```\n\nThis will multiply each element in the matrix `V` by `x`.\n\nIf you want to add a scalar to only non-zero elements of the matrix, you can use the `.A` property to convert the sparse matrix to a dense one, perform the addition, and then convert it back to a sparse matrix.\n\n```python\nV = V.A + x\nV = sparse.csr_matrix(V)\n```\n\nThis will add `x` to each non-zero element in the matrix `V`.\n\nPlease note that converting a sparse matrix to a dense one and back to a sparse matrix will lose some of the benefits of the sparse matrix representation, such as efficient storage of zero values. So, this method should be used with caution.", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "You can use the `COO` sparse matrix format to add a scalar to only non-zero elements of a sparse matrix. Here's how you can do it:\n\n```python\nfrom scipy import sparse\nimport numpy as np\n\n# Create a sparse matrix in COOrdinates format\nV = sparse.random(60000, 60000, density=0.05, format='coo', random_state=42)\n\n# Define the scalar\nx = 100.0\n\n# Add the scalar to only non-zero elements\nV.data += x\n```\n\nThis will add `x` to all non-zero elements in `V` without creating a new matrix. The `COO` sparse matrix format is efficient for constructing and modifying sparse matrices, but it can be memory-intensive if the matrix is very sparse.\n\nIf you want to convert the `COO` matrix back to a different format (e.g., `CSR`) for further computations, you can do so using the `tocsr()` or `tobsr()` methods:\n\n```python\nV = V.tocsr()\n```\n\nThis will consume slightly more memory than the `COO` format, but will allow you to perform most operations efficiently.", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "V.data += x\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "for i in range(sa.shape[1]):\n    column = sa[:,i].data\n    column_length = math.sqrt(sum(x**2 for x in column))\n    sa[:,i] = sparse.csc_matrix(column/column_length)\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "for i in range(sa.shape[1]):\n    col = sa[:,i].data\n    norm = math.sqrt(sum(x**2 for x in col))\n    sa[:,i] = sparse.csr_matrix(col/norm)\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = (a>0).astype(int)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = (a>0).astype(int)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "distances = scipy.spatial.distance.cdist(centroids, data)\nresult = np.argmin(distances, axis=1)\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nresult = data[np.argmin(distances, axis=0)]\n", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "distances = scipy.spatial.distance.cdist(centroids, data)\nclosest_indices = np.argpartition(distances, k, axis=1)[:, :k]\nresult = np.array([row[np.argpartition(distances[i], k)[:k]] for i, row in enumerate(distances)])\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "result = np.zeros((len(xdata), len(bdata)))\nfor i, (x, b) in enumerate(zip(xdata, bdata)):\n    result[i] = fsolve(lambda a: eqn(x, a, b), x0=0.5)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "result = []\nfor x, a in zip(xdata, adata):\n    sol = fsolve(eqn, x0=0.5, args=(a, x))\n    result.append(sol)\nresult = np.array(result)\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "def bekkers_cdf(x, a, m, d):\n    return integrate.quad(lambda t: bekkers(t, a, m, d), 0, x)[0]\n\nF_sample = np.array([bekkers_cdf(x, estimated_a, estimated_m, estimated_d) for x in sample_data])\nF_sample /= max(F_sample)\n\ncdf = lambda x: stats.kstest(sample_data, 'norm')[0]\nD, p_value = sp.stats.kstest(F_sample, cdf)\nresult = (D, p_value)\n", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "def bekkers_cdf(x, a, m, d):\n    return integrate.quad(lambda t: bekkers(t, a, m, d), 0, x)[0]\n\nsample_cdf = np.vectorize(bekkers_cdf)\n\nks_statistic, p_value = sp.stats.kstest(sample_cdf(sample_data, estimated_a, estimated_m, estimated_d), 'norm')\n\nresult = p_value < 0.05\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "df['Time'] = pd.to_datetime(df['Time'], format='%Y-%m-%d-%H:%M:%S')\ndf.set_index('Time', inplace=True)\ndf['A'] = df['A'].astype(float)\ndf['A'].resample('25S').apply(integrate.trapz)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "import numpy as np\n\n# Convert the list of tuples to numpy arrays\nx = np.array(x)\ny = np.array(y)\neval = np.array(eval)\n\n# Perform linear interpolation\nresult = scipy.interpolate.griddata(x, y, eval, method='linear')\n\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "def multinomial_loglike(weights, freqs):\n    \"\"\"\n    Log-likelihood function for multinomial distribution\n    \"\"\"\n    total = sum(freqs)\n    if sum(weights) != 1:\n        return np.inf  # Weights should sum up to 1\n    if min(weights) < 0:\n        return np.inf  # Weights should be non-negative\n    return -np.log(np.prod(weights ** freqs)) / total\n\ndef estimate_weights(freqs):\n    \"\"\"\n    Estimate weights for multinomial distribution using MLE\n    \"\"\"\n    return sciopt.fmin(lambda w: -multinomial_loglike(w, freqs), [1.0 / len(freqs)] * len(freqs), disp=False)\n\nweights = estimate_weights(a['A1'].value_counts().sort_index().values)\n", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "result = sciopt.fminbound(e, *(pmin, pmax), args=(x,y))\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "import numpy as np\n\ndef find_relative_extrema(arr, n):\n    # Pad the array with n//2 elements at the beginning and end\n    padded_arr = np.pad(arr, (n//2, n//2), mode='edge')\n    # Create a boxcar kernel of size n+1\n    kernel = np.ones(n+1)\n    # Convolve the padded array with the kernel to get the min and max\n    min_arr = signal.convolve(padded_arr, kernel, mode='valid')\n    max_arr = signal.convolve(padded_arr, -kernel, mode='valid')\n    # Find the indices where the original array elements are equal to the min and max\n    min_indices = np.nonzero(np.isclose(arr, min_arr[:len(arr)]))[0] + n//2\n    max_indices = np.nonzero(np.isclose(arr, max_arr[:len(arr)]))[0] + n//2\n    # Combine the indices and return\n    return np.sort(np.concatenate((min_indices, max_indices)))\n\nresult = find_relative_extrema(arr, n)\nprint(result)\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "def find_relative_extrema(arr, n):\n    extrema = []\n    for i in range(arr.shape[0]):\n        for j in range(n, arr.shape[1]-n):\n            window = arr[i, j-n:j+n+1]\n            if np.all(window <= arr[i, j]) or np.all(window >= arr[i, j]):\n                extrema.append([i, j])\n    return extrema\n\nresult = find_relative_extrema(arr, n)\nprint(result)\n", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "numeric_cols = df.select_dtypes(include=[np.number]).columns\ndf = df[np.abs(stats.zscore(df[numeric_cols])) < 3].dropna()\n", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": "from sklearn.datasets import load_iris\nimport pandas as pd\n\n# Load the iris dataset\ndata = load_iris()\n\n# Create DataFrame\ndf = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                  columns= data['feature_names'] + ['target'])\n\nprint(df.head())\n\n### END SOLUTION", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "df = df.copy()\n\n# First we get all the unique items in the list\nall_items = set(item for sublist in df['Col3'].tolist() for item in sublist)\n\n# Then we create new columns for each item\nfor item in all_items:\n    df[item] = df['Col3'].apply(lambda x: 1 if item in x else 0)\n\n# Finally we remove the original column\ndf = df.drop('Col3', axis=1)\n\ndf_out = df\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "df = df.copy()\n\n# First we create a list of unique fruits\nfruits = set(fruit for fruits_list in df['Col3'] for fruit in fruits_list)\n\n# Then we create binary columns for each fruit\nfor fruit in fruits:\n    df[fruit] = df['Col3'].apply(lambda fruits_list: int(fruit in fruits_list))\n\n# Finally we drop the 'Col3' column as it is no longer needed\ndf = df.drop('Col3', axis=1)\n\ndf_out = df\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\n\ndf = df.join(pd.DataFrame(mlb.fit_transform(df.pop('Col4')),\n                          columns=mlb.classes_,\n                          index=df.index))\n\ndf_out = df\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "df = pd.get_dummies(df, columns=df.columns[-1])\ndf_out = df\ndf_out\nEND SOLUTION", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "df = pd.get_dummies(df, columns=df.columns[-1])\ndf_out = df\ndf_out\nEND SOLUTION\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "from sklearn.calibration import CalibratedClassifierCV\n\n# Fit the LinearSVC model\nsvmmodel = suppmach.LinearSVC(penalty='l1', C=1, dual=False)\nsvmmodel.fit(X, y)\n\n# Calibrate the model\ncalibrated = CalibratedClassifierCV(svmmodel, cv=5)\ncalibrated.fit(X, y)\n\n# Predict the probabilities\nproba = calibrated.predict_proba(x_test)\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "from sklearn.calibration import CalibratedClassifierCV\n\n# Train the LinearSVC model\nmodel_svc = model.fit(X, y)\n\n# Calibrate the model\nmodel_calibrated = CalibratedClassifierCV(model_svc, cv=5)\nmodel_calibrated.fit(X, y)\n\n# Predict the probabilities\nproba = model_calibrated.predict_proba(x_predict)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "from scipy.sparse import csr_matrix, hstack\n\n# assuming that transform_output is a csr_matrix\n# and df_origin is a DataFrame\n\n# convert csr_matrix to dense matrix (numpy array)\ntransform_output_array = transform_output.toarray()\n\n# convert numpy array to DataFrame\ntransform_output_df = pd.DataFrame(transform_output_array)\n\n# concatenate transformed DataFrame with original DataFrame\ndf = pd.concat([df_origin, transform_output_df], axis=1)\n\nEND SOLUTION\n<code>\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "sparse_matrix = csr_matrix(transform_output)\ndf_sparse = pd.DataFrame.sparse.from_spmatrix(sparse_matrix)\ndf = pd.concat([df_origin, df_sparse], axis=1)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": "You can convert the scipy.sparse.csr.csr_matrix to a pandas DataFrame and then concatenate it with the original DataFrame. Here is an example:\n\n```python\n# Assuming that `transform_output` is your scipy.sparse.csr.csr_matrix\n# and it has been transformed to a DataFrame `df_transform_output`\n\n# Convert the csr_matrix to a DataFrame\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Now you can concatenate it with your original DataFrame\n# df_origin is your original DataFrame and df_transform_output is the DataFrame you want to concatenate\nresult = pd.concat([df_origin.reset_index(drop=True), df_transform_output.reset_index(drop=True)], axis=1)\n```\n\nPlease note that the `reset_index(drop=True)` is used to ensure that the index of the DataFrame is reset and does not contain any missing values. This is necessary because concatenating DataFrames may result in a new index that contains missing values.\n\nAlso, make sure that the dimensions of the concatenated DataFrame are correct. The number of rows should be the same in both DataFrames and the number of columns should be the sum of the original columns.", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "# Delete 'poly' step\nclf.steps.pop(1)\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "clf.steps.pop(1)\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "clf.steps.pop(1)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "from sklearn.pipeline import make_pipeline\n\n# Insert a step at the beginning\nclf = make_pipeline(PolynomialFeatures(), *clf.steps)\n\n# Insert a step at the end\nclf = Pipeline(clf.steps + [('new_step', PolynomialFeatures())])\n\n# Insert a step in the middle\nclf = Pipeline(clf.steps[:1] + [('new_step', PolynomialFeatures())] + clf.steps[1:])\n\n# Delete a step\nclf = Pipeline(clf.steps[:1] + clf.steps[2:])\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# initial pipeline\nclf = Pipeline([('reduce_poly', PolynomialFeatures()), ('dim_svm', PCA()), ('sVm_233', SVC())])\n\n# insert a step at the beginning\nclf.steps.insert(0, ('new_step', PCA()))\n\n# delete the second step\ndel clf.steps[1]\n\n# print the updated pipeline\nprint(clf.named_steps)\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Insert ('t1919810', PCA()) right before 'svdm'\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n\n# Insert PCA() right before SVC()\nclf.steps.insert(2, ('t1919810', PCA()))\n\n# Check the result\nprint(clf.named_steps)\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "model = xgb.XGBRegressor()\n\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [(testX, testY)]}\n\ngridsearch = GridSearchCV(model, param_grid, verbose=verbose, \n                          cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), \n                          n_jobs=n_jobs, iid=iid, fit_params=fit_params)\n\ngridsearch.fit(trainX,trainY)\n\nb = gridsearch.best_score_\nc = gridsearch.predict(testX)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "model = xgb.XGBRegressor()\n\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [(testX, testY)],\n            \"verbose\" : False} # Set to True if you want to see the output log\n\ngridsearch = GridSearchCV(model, param_grid, verbose=1, \n                          cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), \n                          n_jobs=n_jobs, iid=iid, fit_params=fit_params)\n\ngridsearch.fit(trainX, trainY)\n\nb = gridsearch.best_score_\nc = gridsearch.predict(testX)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train, test in cv:\n    logreg.fit(X[train], y[train])\n    proba.append(logreg.predict_proba(X[test]))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": "The StandardScaler transforms the data such that its distribution will have a mean value 0 and standard deviation of 1. This is a linear transformation and hence, the inverse of the transformation should yield the original data back.\n\nHowever, in your case, you are applying the transformation on a single column 't' of your dataframe. So, when you inverse the transformation, you will not get back the original 't' column. Instead, you will get a new column which is scaled version of 't'.\n\nIf you want to scale 't' and then predict it, you can do so. But when you check the score or inverse the transformation, you will not get back the original 't' values.\n\nHere is an example:\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# Assume train_df['t'] is your target variable\nscaler.fit(train_df[['t']])\ntrain_df['t_scaled'] = scaler.transform(train_df[['t']])\n\n# Now you can use train_df['t_scaled'] for your regression model\n\n# After getting your predictions, you can inverse the transformation\npredictions_scaled = model.predict(test_df)\npredictions = scaler.inverse_transform(predictions_scaled)\n\n# Now predictions will be your original target variable values\n```\n\nIn this example, 't_scaled' is the scaled version of 't'. You can use 't_scaled' for your regression model and then inverse the transformation to get back the original 't' values.\n\nRemember, the inverse transformation will not be the exact original 't' values because of the transformation process. It will be a scaled version of 't'.", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\n", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\n", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "model_name = model.__class__.__name__\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data)\n", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit_transform(data, target)\n", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "grid_search = GridSearchCV(bc, param_grid, cv=3)\ngrid_search.fit(X_train, y_train)\n\n# Print the best parameters it found\nprint(grid_search.best_params_)\n\n# Get the best model\nbest_model = grid_search.best_estimator_\n\n# Use the best model to predict the test data\nproba = best_model.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "regressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\n\n# Reshape your input data because it should be 2D array (n_samples, n_features)\nX = np.array(X).reshape(-1, 1)\ny = np.array(y).reshape(-1)\n\n# Fit the model\nrgr = regressor.fit(X, y)\n\n# Make a prediction\nX_test = np.array(X_test).reshape(-1, 1)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "regressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\n\n# Reshape your data appropriately, -1 means that the value is inferred from the length of the array\nX = np.array(X).reshape(-1, 1)\ny = np.array(y).reshape(-1)\n\nrgr = regressor.fit(X, y)\n\n# To make a prediction use the `predict` method\nX_test = np.array(X_test).reshape(-1, 1)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "def preprocess(s):\n    return s.upper()\n\n# example variable `tfidf`\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "def prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "data_scaled = preprocessing.scale(data)\ndf_out = pd.DataFrame(data_scaled, index=data.index, columns=data.columns)\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "data_scaled = pd.DataFrame(preprocessing.scale(data), index=data.index, columns=data.columns)\ndf_out = data_scaled\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\nbest_model = grid.best_estimator_['model']\ncoef = best_model.coef_\nprint(coef)\n", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\nprint(coef)\n", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# Get the indices of the selected features\nfeature_indices = model.get_support(indices=True)\n\n# Get the names of the selected features\ncolumn_names = X.columns[feature_indices]\n\n# Convert the column names to a list\ncolumn_names = list(column_names)\n\nprint(column_names)\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\nfeature_idx = model.get_support() # list of booleans for selected features\n\n# get feature names\ncolumn_names = X.columns[feature_idx]\n\nprint(column_names)\nEND SOLUTION\n", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# Get the indices of the selected features\nfeature_indices = model.get_support(indices=True)\n\n# Get the names of the selected features\ncolumn_names = X.columns[feature_indices]\n\n# If you want to get the selected features as a list\nselected_features = X.columns[feature_indices].tolist()\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# Get the indices of the selected features\nfeature_indices = model.get_support(indices=True)\n\n# Get the names of the selected features\ncolumn_names = X.columns[feature_indices].tolist()\n", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\ncluster_center = km.cluster_centers_[p]\ndistances = np.linalg.norm(X - cluster_center, axis=1)\nclosest_indices = np.argsort(distances)[:50]\nclosest_50_samples = X[closest_indices]\n", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\ncentroids = km.cluster_centers_\nclosest_50_samples = X[np.argsort(np.linalg.norm(X - centroids[p], axis=1))[:50]]\n", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\ncluster_center = km.cluster_centers_[p]\ndistances = np.linalg.norm(X - cluster_center, axis=1)\nclosest_indices = np.argsort(distances)[:100]\nclosest_100_samples = X[closest_indices]\n", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": "km.fit(X)\ncluster_center = km.cluster_centers_[p]\ndistances = np.linalg.norm(X - cluster_center, axis=1)\nsamples = X[np.argsort(distances)[:50]]\nreturn samples\n\n### END SOLUTION\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "from sklearn.preprocessing import OneHotEncoder\n\n# Instantiate the OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\n\n# Reshape your data either using array.reshape(-1, 1) if your data has a single feature\n# or using array.reshape(1, -1) if it contains a single sample\nX_train = enc.fit_transform(X_train[0].reshape(-1, 1)).toarray()\n\nEND SOLUTION\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "# Convert the categorical column to one-hot encoding\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# define support vector regression model\nmodel = SVR(kernel='rbf')\n\n# fit model\nmodel.fit(X, y)\n\n# predict\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn.svm import SVR\n\n# Create a support vector regression model with a gaussian kernel\nmodel = SVR(kernel='rbf')\n\n# Fit the model to the data\nmodel.fit(X, y)\n\n# Predict the output\npredict = model.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# Create a support vector regression model with a polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n\n# Fit the model to the data\nmodel.fit(X, y)\n\n# Predict the output\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# Create a support vector regression model with a polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n\n# Fit the model to the data\nmodel.fit(X, y)\n\n# Predict the output\npredict = model.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "from sklearn.metrics.pairwise import cosine_similarity\n\ndef get_cosine_similarity(documents, queries):\n    tfidf_matrix = tfidf.transform(documents + queries)\n    doc_term_matrix = tfidf_matrix[:len(documents)]\n    query_term_matrix = tfidf_matrix[len(documents):]\n    cosine_similarities = cosine_similarity(query_term_matrix, doc_term_matrix).flatten()\n    return cosine_similarities\n\ncosine_similarities_of_queries = get_cosine_similarity(documents, queries)\ncosine_similarities_of_queries = cosine_similarities_of_queries.reshape(len(queries), len(documents))\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "from sklearn.metrics.pairwise import cosine_similarity\n\ndef get_cosine_similarity(documents, queries):\n    tfidf_matrix = tfidf.transform(documents + queries)\n    doc_term_matrix = tfidf_matrix[:len(documents)]\n    query_term_matrix = tfidf_matrix[len(documents):]\n    cosine_similarities = cosine_similarity(query_term_matrix, doc_term_matrix).flatten()\n    return cosine_similarities\n\ncosine_similarities_of_queries = get_cosine_similarity(documents, queries)\ncosine_similarities_of_queries = cosine_similarities_of_queries.reshape(len(queries), len(documents)).T\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": "from sklearn.metrics.pairwise import cosine_similarity\n\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer().fit_transform(documents)\n    cosine_similarities_of_queries = []\n    for query in queries:\n        query_tfidf = TfidfVectorizer().fit(documents).transform([query])\n        cosine_similarities = cosine_similarity(query_tfidf, tfidf).flatten()\n        cosine_similarities_of_queries.append(cosine_similarities)\n    return cosine_similarities_of_queries\n\n# usage\ncosine_similarities_of_queries = solve(queries, documents)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\nnew_features\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nnew_f = mlb.fit_transform(f)\ndf = pd.DataFrame(new_f, columns=mlb.classes_)\ndf\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\nnew_features\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": "from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\n\ndf = pd.DataFrame(new_features, columns=mlb.classes_)\n\nreturn df\n\n### END SOLUTION\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\n\nnew_features\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create an instance of Agglomerative Clustering with 2 clusters\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit predict the data and obtain cluster labels\ncluster_labels = cluster.fit_predict(data_matrix)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create an instance of AgglomerativeClustering with 2 clusters\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit and predict the clusters \nlabels = cluster.fit_predict(data_matrix)\n\n# Get the cluster labels\ncluster_labels = labels.tolist()\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create an instance of Agglomerative Clustering with 2 clusters\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit predict the clusters\ncluster_labels = cluster.fit_predict(simM)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Perform the hierarchical/agglomerative clustering\nZ = linkage(data_matrix, 'ward')\n\n# Extract the cluster labels\ncluster_labels = fcluster(Z, 2, criterion='maxclust')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Perform hierarchical/agglomerative clustering\nZ = linkage(data_matrix, 'ward')\n\n# Extract the clustering labels\ncluster_labels = fcluster(Z, 2, criterion='maxclust')\n\n# Print the clustering labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Perform the hierarchical/agglomerative clustering\nZ = linkage(simM, 'ward')\n\n# Extract the clusters\ncluster_labels = fcluster(Z, 2, criterion='maxclust')\n\n# Print the clusters\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler().fit(data)\ncentered_scaled_data = scaler.transform(data)\n", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# reshape(-1, 1) is used because standard scaler expects a 2D array\ndata = data.reshape(-1, 1)\n\ncentered_scaled_data = scaler.fit_transform(data)\n\n# to get back the original data back\noriginal_data = scaler.inverse_transform(centered_scaled_data)\n\n# reshape your data to the original shape\noriginal_data = original_data.reshape(-1)\n", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from scipy import stats\n\nbox_cox_data, fitted_lambda = stats.boxcox(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import PowerTransformer\n\n# Initiate the transformer\npt = PowerTransformer(method='box-cox')\n\n# Reshape your data as the transformer expects a 2D array\ndata = data.reshape(-1, 1)\n\n# Fit and transform the data\nbox_cox_data = pt.fit_transform(data)\n\n# If you want to inverse the transformation (optional)\ninverse_data = pt.inverse_transform(box_cox_data)\n\n# If you want to get the lambda used during transformation (optional)\nlambda_used = pt.lambdas_\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import PowerTransformer\n\n# Initialize the PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\n\n# Fit and transform the data\nyeo_johnson_data = pt.fit_transform(data)\n\n# Convert numpy array back to pandas DataFrame\nyeo_johnson_data = pd.DataFrame(yeo_johnson_data, columns = data.columns)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import PowerTransformer\n\n# Initialize the PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\n\n# Reshape your data as the PowerTransformer expects a 2D array\ndata = data.reshape(-1, 1)\n\n# Fit and transform the data\nyeo_johnson_data = pt.fit_transform(data)\n\n# If you want to inverse transform the data back to its original form\ninverse_transformed_data = pt.inverse_transform(yeo_johnson_data)\n\n# If you want to check the skewness after transformation\nprint(\"Skewness after transformation: \", skewness(yeo_johnson_data))\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer()\nX = vectorizer.fit_transform([text])\ntransformed_text = vectorizer.inverse_transform(X)\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Assuming that dataset is your DataFrame\nX = dataset.iloc[:, :-1]  # All columns except the last one\ny = dataset.iloc[:, -1]  # The last column\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Assuming df is your DataFrame and target is the name of the column\n# that contains the target values\ntarget = df.columns[-1]\nX = df.drop(target, axis=1)\ny = df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Assuming that dataset is your DataFrame\nX = dataset.iloc[:, :-1]  # All columns except the last one\ny = dataset.iloc[:, -1]  # The last column\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.66, random_state=42)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into train and test sets\ntrain_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# Split the train set into features (x_train) and labels (y_train)\nx_train = train_set.iloc[:, :-1]\ny_train = train_set.iloc[:, -1]\n\n# Split the test set into features (x_test) and labels (y_test)\nx_test = test_set.iloc[:, :-1]\ny_test = test_set.iloc[:, -1]\n\n# Return the solution\nreturn x_train, y_train, x_test, y_test\n\n### END SOLUTION", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "mse_values = df['mse'].values.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(mse_values)\nlabels = kmeans.predict(mse_values)\nEND SOLUTION\n<code>\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "import numpy as np\n\n# Reshape your data because KMeans expects a 2D array\nf1 = np.array(df['mse']).reshape(-1, 1)\n\n# Create a range of values for f2, make sure it has the same size as f1\nf2 = np.array(list(range(0, len(f1)))).reshape(-1, 1)\n\n# Combine both features\nX = np.concatenate((f1, f2), axis=1)\n\n# Fit KMeans model\nkmeans = KMeans(n_clusters=2, n_init=10).fit(X)\n\n# Predict the clusters\nlabels = kmeans.predict(X)\n\n# Get the centroids of the clusters\ncentroids = kmeans.cluster_centers_\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "svc = LinearSVC(penalty='l1', dual=False)\nsvc.fit(X, y)\nmask = svc.coef_ != 0\nselected_feature_names = np.array(vectorizer.get_feature_names())[mask]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "svc = LinearSVC(penalty='l1', dual=False)\nsvc.fit(X, y)\ncoef = svc.coef_\nindices = np.argsort(coef, axis=1)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[indices[:, -10:]]\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": "clf = LinearSVC(penalty='l1', dual=False)\nclf.fit(X, y)\ncoef = clf.coef_\nselected_features = np.where(coef != 0)[1]\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_features]\nreturn selected_feature_names\n### END SOLUTION", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nEND SOLUTION\nreturn feature_names, X\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nEND SOLUTION\nreturn feature_names, X\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "slopes = []\nfor col in df1.columns[1:]: # skip the 'Time' column\n    df2 = df1[[col, 'Time']].dropna() # select the column and 'Time' and remove NaNs\n    X = df2['Time'].values.reshape(-1,1)\n    Y = df2[col].values.reshape(-1,1)\n    linear_regressor = LinearRegression()  # create object for the class\n    linear_regressor.fit(X, Y)  # perform linear regression\n    slopes.append(linear_regressor.coef_[0][0])  # append the slope\nslopes = np.array(slopes)  # convert list to numpy array\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "slopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    if len(df2) > 1:  # make sure there are enough data points to fit the model\n        df3 = df2[['Time', col]]\n        npMatrix = np.matrix(df3)\n        X, Y = npMatrix[:,0], npMatrix[:,1]\n        slope = LinearRegression().fit(X,Y)\n        slopes.append(slope.coef_[0])\n    else:\n        slopes.append(np.nan)  # or some other indicator of insufficient data\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "le = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\ntransformed_df = df\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "le = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": "le = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\nreturn df\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "from sklearn.linear_model import ElasticNet\n\n# Create a ElasticNet instance\nElasticNet = linear_model.ElasticNet()\n\n# Fit the model\nElasticNet.fit(X_train, y_train)\n\n# Print out the coefficients\nprint('Coefficients: \\n', ElasticNet.coef_)\n\n# Print out the intercept\nprint('Intercept: \\n', ElasticNet.intercept_)\n\n# Print out the R^2 for training set\ntraining_set_score = ElasticNet.score(X_train, y_train)\nprint('R^2 for training set: ', training_set_score)\n\n# Print out the R^2 for test set\ntest_set_score = ElasticNet.score(X_test, y_test)\nprint('R^2 for test set: ', test_set_score)\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\nreshaped_array = np_array.reshape(-1, 1)\nscaled_array = scaler.fit_transform(reshaped_array)\ntransformed = scaled_array.reshape(np_array.shape)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\nreshaped_array = np_array.reshape(-1,1)\nscaler.fit_transform(reshaped_array)\ntransformed = scaler.transform(reshaped_array).reshape(np_array.shape)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": "You can reshape your array into a 2D format suitable for the MinMaxScaler, then reshape it back to the original shape:\n\n```python\ndef Transform(a):\n    # Reshape the array to a 2D format\n    a_reshaped = a.reshape(-1, 1)\n    \n    # Create a MinMaxScaler object\n    scaler = MinMaxScaler()\n    \n    # Fit and transform the reshaped array\n    a_scaled_reshaped = scaler.fit_transform(a_reshaped)\n    \n    # Reshape the array back to the original shape\n    a_scaled = a_scaled_reshaped.reshape(a.shape)\n    \n    return a_scaled\n```\n\nThis will normalize the entire numpy array all together.", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "close_buy1 = close[-1:].values.reshape(1, -1)\nm5 = ma50[-1:].values.reshape(1, -1)\nm10 = ma100[-1:].values.reshape(1, -1)\nma20 = ma200[-1:].values.reshape(1, -1)\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\n\npredict = clf.predict(b)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "from sklearn.preprocessing import LabelEncoder\n\n# Convert string labels into numbers.\nle = LabelEncoder()\nnew_X = np.array(X)\nnew_X[:, 0] = le.fit_transform(new_X[:, 0])\nnew_X = new_X.astype(float)\n\n# Use the new_X instead of X\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "from sklearn.preprocessing import LabelEncoder\n\n# Convert string labels into numbers.\nle = LabelEncoder()\nnew_X = np.array(X)\nnew_X[:, 0] = le.fit_transform(new_X[:, 0])\nnew_X = new_X.astype(float)\n\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "from sklearn.preprocessing import LabelEncoder\n\n# Convert string data to numerical data\nle = LabelEncoder()\nnew_X = np.array(X)\nnew_X[:, 0] = le.fit_transform(new_X[:, 0])\nnew_X = new_X.astype(float)\n\n# Fit the model\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "# Seperating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n# predict\npredict = logReg.predict(X)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "# X = dataframe.iloc[:,:-1].astype(float)\n# y = dataframe.iloc[:,-1]\n# logReg = LogisticRegression()\n# logReg.fit(X, y)\n# predict = logReg.predict(X)\n# print(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "features_dataframe = features_dataframe.sort_values(\"date\")\nsplit_point = int(len(features_dataframe) * 0.2)\ntrain_dataframe = features_dataframe.iloc[:-split_point]\ntest_dataframe = features_dataframe.iloc[-split_point:]\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "train_size = 0.8\nn = len(features_dataframe)\ntrain_size = int(n * train_size)\n\nfeatures_dataframe = features_dataframe.sort_values(\"date\")\n\ntrain_dataframe = features_dataframe.iloc[:train_size]\ntest_dataframe = features_dataframe.iloc[train_size:]\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": "You can use the function `train_test_split` from `sklearn.model_selection` to split your data. However, you need to make sure that your dates are sorted in ascending order before splitting.\n\nHere is how you can do it:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfeatures_dataframe = load_data()  # load your data\nfeatures_dataframe = features_dataframe.sort_values('date')  # sort your data by date\n\n# split the data\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=0.2, shuffle=False)\n```\n\nIn this code, `shuffle=False` is used to maintain the order of the data. If you don't specify this, the data will be shuffled before splitting, which means the order of your data will be random.\n\nPlease note that the `train_test_split` function from `sklearn.model_selection` does not sort the dataframe by default. So, you need to sort your dataframe by date before splitting it.\n\nAlso, make sure that your dates are in a column named 'date'. If they are in a different column, replace 'date' with the name of your date column.", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2', 'X3']] = df.groupby('Month')[['X2', 'X3']].transform(lambda x: scaler.fit_transform(x))\ndf = df.rename(columns={'X2': 'X2_scale', 'X3': 'X3_scale'})\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "cols = ['A2', 'A3']\nfor col in cols:\n    myData['new_'+col] = myData.groupby('Month')[col].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)))\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase = False, token_pattern=r\"(?u)\\b\\w\\w+\\b\")\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase = False, token_pattern=r\"(?u)\\b\\w\\w+\\b\")\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values(by='mean_fit_time')\nfull_results\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "from sklearn.externals import joblib\njoblib.dump(fitted_model, 'sklearn_model.pkl')\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "tfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix, tfidf_matrix)\ndf_similarity = pd.DataFrame(cosine_similarity_matrix, index=df['items'], columns=df['items'])\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "for param_group in optim.param_groups:\n    param_group['lr'] = 0.001\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "for param_group in optim.param_groups:\n    param_group['lr'] = 0.001\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "for param_group in optim.param_groups:\n    param_group['lr'] = 0.0005\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "You can change the learning rate of your optimizer by accessing its param_groups attribute and modifying the 'lr' key of the first and only element in the list. Here's how you can do it:\n\n```python\noptim = torch.optim.SGD(model.parameters(), lr=0.05)\n\n# To change the learning rate to 0.005\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.005\n```\n\nAs for when to use 0.05 or 0.0005, it depends on your specific task and data. Generally, smaller learning rates like 0.0005 are used when dealing with very high dimensional data or very complex models, while larger learning rates like 0.05 are used with simpler models or lower dimensional data.\n\nHowever, it's important to note that changing the learning rate during training can be risky and can potentially lead to instabilities in your training process. It's often better to use learning rate warmup, where you gradually increase the learning rate over the first few epochs of training, and then gradually decrease it as training progresses. This can help your model to converge more quickly and accurately.", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "word_vectors = word2vec.wv\nvocab_size = len(word_vectors.key_to_index)\nembedding_dim = word2vec.wv.vector_size\n\n# Create a PyTorch tensor from the word vectors\nweight = torch.FloatTensor(word_vectors.vectors)\n\n# Create an Embedding layer with pre-trained weights\nembedding = torch.nn.Embedding.from_pretrained(weight)\n\n# Use the embedding layer to embed your input data\nembedded_input = embedding(input_Tensor)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": "embedding_matrix = zeros((vocab_size, 100))\nfor word, i in t.vocab.stoi.items():\n    if word in word2vec.wv.key_to_index:\n        embedding_matrix[i] = word2vec.wv[word]\n\n# Now, create an Embedding layer with the weights you just created\nembedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32))\n\n# Now, you can use this embedding layer in your model\n# For example, if you have a tensor of size (batch_size, seq_len)\n# You can use it like this:\n# embedded_input = embedding(input_Tensor)\n\nreturn embedded_input\n    ### END SOLUTION", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(x.detach().numpy())\n", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "x = torch.rand(4,4)\npx = pd.DataFrame(x.detach().numpy())\nEND SOLUTION", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(x.detach().numpy())\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "C = B[:, A_log.bool()]\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "C = B.index_select(1, torch.nonzero(A_logical).squeeze())\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "import torch\n\n# Create a tensor B\nB = torch.tensor([[999, 777, 114514], [9999, 7777, 1919810]])\n\n# Create a tensor A_log with boolean values\nA_log = torch.tensor([True, True, False])\n\n# Use torch.masked_select to perform the logical indexing\nC = torch.masked_select(B, A_log)\n\n# Reshape the tensor C to the original shape of B\nC = C.reshape(B.shape)\n\nprint(C)\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "C = torch.index_select(B, 1, torch.nonzero(A_log == 1).squeeze())\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": "You can use advanced indexing in PyTorch to achieve this. Here is how you can do it:\n\n```python\nimport torch\n\n# Define the logical index\nA_log = torch.tensor([True, False, True])\n\n# Define the tensor\nB = torch.tensor([[1, 2, 3], [4, 5, 6]])\n\n# Use advanced indexing to get the columns where A_log is True\nC = B[:, A_log]\n\nprint(C)\n```\n\nIn this code, `A_log` is a boolean tensor where `True` indicates the column you want to keep and `False` indicates the column you want to discard. We use this tensor as the second argument in advanced indexing to get the columns we want from `B`.\n\nPlease note that the `A_log` tensor should be a `torch.bool` tensor, not a `torch.ByteTensor`. The error message you're seeing is because PyTorch does not support indexing with a `torch.ByteTensor`.", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "C = torch.index_select(B, 1, torch.nonzero(A_log).squeeze())\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "C = torch.index_select(B, 1, idx)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "import pandas as pd\nimport torch\nimport numpy as np\n\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=object)\n\nx_tensor = torch.tensor(x_array[0].astype(np.float32))\nfor i in range(1, len(x_array)):\n    x_tensor = torch.cat((x_tensor, torch.tensor(x_array[i].astype(np.float32))))\n\nx_tensor = x_tensor.view(len(x_array), -1)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array.tolist())\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": "import pandas as pd\nimport torch\nimport numpy as np\n\ndef Convert(a):\n    # Convert the object type array to list of lists\n    a_list = a.tolist()\n    # Convert the list of lists to a list of tensors\n    a_tensor = [torch.tensor(i, dtype=torch.float16) for i in a_list]\n    # Stack the list of tensors into a single tensor\n    t = torch.stack(a_tensor)\n    return t\n\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=object)\n\nt = Convert(x_array)\nprint(t)\n\n### END SOLUTION", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "import torch\n\nlens = [3, 5, 4]\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\nmask = mask.long()\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "import torch\n\nlens = [1, 9, 3, 5]\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\nmask = mask.long()\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "import torch\n\nlens = [3, 5, 4]\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\nmask = mask.long()\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": "import torch\n\ndef get_mask(lens):\n    max_len = max(lens)\n    batch_size = len(lens)\n    mask = torch.zeros(batch_size, max_len)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask.long()\n\nlens = [3, 5, 4]\nmask = get_mask(lens)\nprint(mask)\n<jupyter_output>\ntensor([[1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 0]])\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "Tensor_3D = torch.diag_embed(Tensor_2D)\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": "You can use torch.diag_embed to convert a 2D tensor into a 3D tensor with the second and third dimensions being the same as the input tensor, and the first dimension being the diagonal of the input tensor.\n\nHere is the solution:\n\n```python\ndef Convert(t):\n    # Expand dims to create a 3D tensor with the same diagonal values\n    result = torch.diag_embed(t)\n    return result\n```\n\nYou can use this function like this:\n\n```python\nTensor_2D = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nTensor_3D = Convert(Tensor_2D)\nprint(Tensor_3D.shape)  # Output: torch.Size([3, 3, 3])\n```\n\nThis will create a 3D tensor where the first dimension is the batch dimension, the second and third dimensions are the same size as the input tensor and the first dimension is the diagonal of the input tensor.", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = a.unsqueeze(0) if a.dim() == 1 else a\nb = b.unsqueeze(0) if b.dim() == 1 else b\nab = torch.cat((a,b),0)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "ab = torch.cat((a.unsqueeze(0), b.unsqueeze(0)), dim=0)\nab = ab.view(-1, a.shape[1])\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": "You can use torch.cat() function to concatenate the tensors along a new dimension. Here is how you can do it:\n\n```python\nab = torch.cat((a, b.unsqueeze(0)), 0)\n```\n\nIn this case, b.unsqueeze(0) is used to increase the dimension of b from (1, 11) to (1, 11). After that, torch.cat() is used to concatenate a and b along the first dimension, which results in a tensor of shape (3, 11).\n\nPlease note that the shapes of a and b must be consistent along the dimension you are concatenating. In this case, they must both have the same number of columns (11). The number of rows can be different.", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "mask = torch.arange(a.size(1))[None,:,None] >= lengths[:,None,None]\na[mask] = 0\n", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "for i, length in enumerate(lengths):\n    a[i, length:, :] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "mask = torch.arange(a.size(1))[None,:,None] < lengths[:,None,None]\na[mask] = 0\n", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "mask = torch.arange(a.size(1))[None, :] < lengths[:, None]\na[mask] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.stack(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "list_of_tensors = [torch.randn(3) for _ in range(5)]\ntensor_of_tensors = torch.stack(list_of_tensors)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": "You can use torch.stack to convert a list of tensors into a single tensor.\n\n```python\nimport torch\n\nlist_of_tensors = [ torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\n```\n\nThis will create a tensor that is a 1-D tensor of the same length as the number of elements in list_of_tensors, with the i-th element of the tensor being the i-th element of list_of_tensors.\n\nIf you want to create a tensor that is a 2-D tensor, you can do:\n\n```python\nimport torch\n\nlist_of_tensors = [ torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack([tensor for tensor in list_of_tensors])\n```\n\nThis will create a tensor that is a 2-D tensor with the same number of rows as the number of elements in list_of_tensors, and each row is the corresponding tensor in list_of_tensors.", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.stack([torch.tensor(t) for t in list_of_tensors])\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "result = torch.index_select(t, 0, torch.tensor(idx)).squeeze()\n", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "result = torch.index_select(t, 0, torch.tensor(idx)).squeeze()\n", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "result = torch.index_select(t, 0, torch.tensor(idx, dtype=torch.long))\n", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "result = torch.gather(x, 1, ids.unsqueeze(-1).expand(-1,-1,2)).squeeze(1)\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "result = torch.gather(x, 1, ids.unsqueeze(-1).expand(-1,-1,x.size(-1))).squeeze(1)\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "ids_one_hot = torch.nn.functional.one_hot(ids, num_classes=2).bool()\nresult = torch.masked_select(x.view(-1, 2), ids_one_hot).view(-1, 2)\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "y = torch.argmax(softmax_output, dim=1).unsqueeze(1)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "y = torch.argmax(softmax_output, dim=1).unsqueeze(1)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "y = torch.argmin(softmax_output, dim=1, keepdim=True)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": "You can use torch.max function to find the maximum value and its index in each row of the tensor. Here is how you can do it:\n\n```python\n_, y = torch.max(softmax_output, dim=1)\n```\n\nIn the above line of code, the function torch.max returns two tensors. The first tensor contains the maximum value in each row of the input tensor and the second tensor contains the indices of these maximum values. Since you want the indices (which class each input belongs to), you should select the second tensor.\n\nSo, your complete code will look like this:\n\n```python\nimport torch\n\ndef solve(softmax_output):\n    _, y = torch.max(softmax_output, dim=1)\n    return y\n```\n\nThis function will return a tensor with the same number of dimensions as the input tensor (in this case, a 1D tensor), but with each element being the index of the maximum value in the corresponding row of the input tensor.", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "y = torch.argmin(softmax_output, dim=1)\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "def cross_entropy2d(input, target, weight=None, size_average=True):\n    # input: (n, c, w, z), target: (n, w, z)\n    n, c, w, z = input.size()\n    # log_p: (n, c, w, z)\n    log_p = F.log_softmax(input, dim=1)\n    # log_p: (n*w*z, c)\n    log_p = log_p.permute(0, 2, 3, 1).contiguous().view(-1, c)  # make class dimension last dimension\n    # target: (n, w, z) -> (n*w*z)\n    target = target.view(-1)\n    # Create a one-hot vector from target\n    target_one_hot = torch.zeros_like(log_p).scatter_(1, target.view(-1, 1), 1)\n    # Filter out the invalid entries\n    mask = target >= 0\n    log_p = log_p[mask]\n    target_one_hot = target_one_hot[mask]\n    loss = F.binary_cross_entropy_with_logits(log_p, target_one_hot, weight=weight, reduction='none')\n    if size_average:\n        loss = loss.mean()\n    return loss\n\nimages = Variable(torch.randn(5, 3, 4, 4))\nlabels = Variable(torch.LongTensor(5, 4, 4).random_(3))\nloss = cross_entropy2d(images, labels)\nprint(loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = torch.sum(A == B)\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = sum(A == B)\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = torch.sum(A != B)\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": "Here is a solution using PyTorch:\n\n```python\ndef Count(A, B):\n    return torch.sum(A == B).item()\n```\n\nThis function uses the `==` operator to compare the two tensors element-wise. The `torch.sum` function then adds up all the True values (which are equal) in the resulting tensor. The `.item()` method is used to get a Python number from a tensor containing a single value.\n\nHere is a solution using Numpy:\n\n```python\ndef Count(A, B):\n    return np.sum(A.numpy() == B.numpy())\n```\n\nThis function works similarly, but it first converts the PyTorch tensors to Numpy arrays using the `.numpy()` method before comparing them.\n\nPlease note that these solutions assume that the tensors are 1D. If they are 2D or higher, you would need to adjust the solution accordingly.", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "cnt_equal = sum(A[-x:] == B[-x:])\n", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "cnt_not_equal = torch.ne(A[-x:], B[-x:]).sum().item()\n", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "tensors_31 = [a[..., i:i+chunk_dim, :] for i in range(a.shape[3] - chunk_dim + 1)]\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "tensors_31 = [a[:, :, i:i+chunk_dim, :, :] for i in range(a.size(2) - chunk_dim + 1)]\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output[mask.bool()] = clean_input_spectrogram[mask.bool()]\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output[mask.bool()] = clean_input_spectrogram[mask.bool()]\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_val = torch.min(torch.abs(x), torch.abs(y))\n\nsigned_min = torch.where(torch.abs(x) == min_val, sign_x * min_val, sign_y * min_val)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "max_xy, max_indices = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = torch.where(torch.abs(x) == max_xy, x, y)\nsigned_max = torch.where(max_indices == 0, signed_max, -signed_max)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_val, min_indices = torch.min(torch.abs(x), torch.abs(y), dim=0)\n\nsigned_min = torch.where(min_indices == 0, x, y)\nsigned_min = signed_min * sign_x * sign_y\n\nreturn signed_min\n### END SOLUTION", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "output = MyNet(input)\n_, conf = torch.max(output, 1)\nconf = F.softmax(output, dim=1)\nconfidence_score = conf[0][conf_id].item()\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "a = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\n# get the last column of tensor a and the first column of tensor b\nlast_column_a = a[:, -1]\nfirst_column_b = b[:, 0]\n\n# calculate the average of the last column of tensor a and the first column of tensor b\nmiddle_column = (last_column_a + first_column_b) / 2\n\n# concatenate the first two columns of tensor a, the middle column, and the last two columns of tensor b\nresult = torch.cat((a[:, :1], middle_column.unsqueeze(1), b[:, 1:]), dim=1)\n\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": "Here is a solution using PyTorch:\n\n```python\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\n# Get the shape of the tensors\na_shape = a.shape\nb_shape = b.shape\n\n# Select the overlapping columns\na_last_col = a[:, -1].unsqueeze(1)\nb_first_col = b[:, 0].unsqueeze(1)\n\n# Calculate the average of the overlapping columns\navg_col = (a_last_col + b_first_col) / 2\n\n# Create a new tensor with the overlapping columns\noverlap = torch.cat([a[:, :-1], avg_col, b[:, 1:]], dim=1)\n\nprint(overlap)\n```\n\nThis code first extracts the last column of tensor 'a' and the first column of tensor 'b', calculates their average, and then concatenates the non-overlapping parts of 'a' and 'b' with the overlapping column. The result is a new tensor that overlaps 'a' and 'b' by only one column.", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "import torch\n\nt = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n\n# Create a zero tensor with the required shape\nnew = torch.zeros(6, 4)\n\n# Insert the original tensor into the center of the new tensor\nnew[1:7-1, 1:4] = t\n\nresult = new\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "import torch\n\nt = torch.tensor([[1, 2], [3, 4]])\n\n# Create a tensor of zeros with the same size as the input tensor, but with an extra dimension for the new axis\nnew = torch.zeros(1, *t.shape)\n\n# Stack the original tensor and the new tensor along the first dimension\nr = torch.cat([new, t.unsqueeze(0), new])\n\n# Stack the original tensor and the new tensor along the second dimension\nr = torch.cat([new.transpose(0, 1), t.unsqueeze(1), new.transpose(0, 1)], dim=1)\n\nprint(r)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "import torch\n\nt = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n\n# Create a -1 tensor of the desired shape\nnew = torch.full((6, 4), -1)\n\n# Insert the original tensor into the center of the new tensor\nnew[1:7:2, 1:5] = t\n\nresult = new\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "W = torch.unsqueeze(W, 0)\nW = W.repeat(10, 2, 1)\nresult = torch.bmm(data, W)\nresult = result.view(10, 2, 3)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
