{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dd1e6143eb764db09aa669789c13a83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c698f9c44c0f46eca93a655541a9f434",
              "IPY_MODEL_fec0c2d59f484b99a0a0672ccd9b2918",
              "IPY_MODEL_253b452c48a04d8097f63e667a273d1c"
            ],
            "layout": "IPY_MODEL_ed62949fae0b4de5aa622344ef2de8af"
          }
        },
        "c698f9c44c0f46eca93a655541a9f434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec94d7c35c04f25be65bcf32a54d271",
            "placeholder": "​",
            "style": "IPY_MODEL_c830a10430924080a9bc7c8f5b3e34ea",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fec0c2d59f484b99a0a0672ccd9b2918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccf9aed5a8d94c158c3454a2659339ef",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70ddf083273c4b3f97cdea59c845cdee",
            "value": 4
          }
        },
        "253b452c48a04d8097f63e667a273d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39d8487a2bb44099811df38b08088b78",
            "placeholder": "​",
            "style": "IPY_MODEL_6e2652d256394e6f9af0181cc3b98b09",
            "value": " 4/4 [00:05&lt;00:00,  1.25s/it]"
          }
        },
        "ed62949fae0b4de5aa622344ef2de8af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec94d7c35c04f25be65bcf32a54d271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c830a10430924080a9bc7c8f5b3e34ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccf9aed5a8d94c158c3454a2659339ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70ddf083273c4b3f97cdea59c845cdee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39d8487a2bb44099811df38b08088b78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e2652d256394e6f9af0181cc3b98b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34f5cd1f6909497d972c5572c19768ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0794205300ed44bb966ecf9c1499010b",
              "IPY_MODEL_641b8abc876a44fd9c8651c5b787f37e",
              "IPY_MODEL_d327f6fb98c740d58b6d23d5a2e3c7df"
            ],
            "layout": "IPY_MODEL_6573e37b070647f8bd3524debd310593"
          }
        },
        "0794205300ed44bb966ecf9c1499010b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c388b49ca3440e89fbd7360c6782837",
            "placeholder": "​",
            "style": "IPY_MODEL_2a544b04466e4ff185fd573fa6094e8c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "641b8abc876a44fd9c8651c5b787f37e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7843614c14c4b96924aedb7d975a112",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f508566d8d2478791bfe4c88c963812",
            "value": 4
          }
        },
        "d327f6fb98c740d58b6d23d5a2e3c7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db471aaaa5c94e12baffbc5ad2e72c7f",
            "placeholder": "​",
            "style": "IPY_MODEL_18d61fd4b9d9494ebbcee65826a12ba9",
            "value": " 4/4 [00:13&lt;00:00,  2.95s/it]"
          }
        },
        "6573e37b070647f8bd3524debd310593": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c388b49ca3440e89fbd7360c6782837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a544b04466e4ff185fd573fa6094e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7843614c14c4b96924aedb7d975a112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f508566d8d2478791bfe4c88c963812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db471aaaa5c94e12baffbc5ad2e72c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d61fd4b9d9494ebbcee65826a12ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s3HSuXMODKv5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfSQy7oxCkKb",
        "outputId": "934408ee-b6eb-4262-bb49-c642c4b7d94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (75.1.0)\n",
            "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (4.25.5)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.0.1)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.2.2)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.4.11)\n",
            "Requirement already satisfied: milvus-model>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus[model]) (0.2.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from milvus-lite>=2.4.0->pymilvus) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (4.47.1)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.20.1)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.5.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (24.12.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (4.12.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymilvus pymilvus[model] pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import MilvusClient\n",
        "import numpy as np\n",
        "from pymilvus import model\n",
        "\n",
        "client = MilvusClient(\"./milvus_demo18.db\")\n",
        "client.create_collection(\n",
        "    collection_name=\"demo_collection\",\n",
        "    dimension=768  # The vectors we will use in this demo has 384 dimensions\n",
        ")\n",
        "\n",
        "\n",
        "embedding_fn =  model.dense.SentenceTransformerEmbeddingFunction(\n",
        "    model_name='cornstack/CodeRankEmbed', # Specify the model name\n",
        "    device='cuda:0',\n",
        "    trust_remote_code=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ3-CVP1CsYk",
        "outputId": "60fa749d-d534-4011-eb23-5494cf17249d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.cornstack.CodeRankEmbed.351d33aed87e396209599ed2e379bbaac973a632.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import MilvusClient\n",
        "from pymilvus import model\n",
        "\n",
        "\n",
        "DISTANCE_THRESHOLD = 0.25\n",
        "\n",
        "#embedding_fn = model.DefaultEmbeddingFunction()\n",
        "\n",
        "\n",
        "def ip_search(\n",
        "    queries,\n",
        "    output_field,\n",
        "    collection_name,\n",
        "    distance_threshold=DISTANCE_THRESHOLD,\n",
        "    embedding_fn=embedding_fn,\n",
        "):\n",
        "    \"\"\"Function does an ip search, based on the parameters provided\"\"\"\n",
        "    query_vectors = embedding_fn.encode_documents(queries)\n",
        "    res = client.search(\n",
        "        collection_name=collection_name,  # target collection\n",
        "        data=query_vectors,  # query vectors\n",
        "        limit=5,  # number of returned entities\n",
        "        output_fields=output_field,  # specifies fields to be returned\n",
        "\n",
        "    )\n",
        "\n",
        "    res = [table['entity']['text'] for table in res[0] if table[\"distance\"] > DISTANCE_THRESHOLD]\n",
        "\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "C9cJn4jgCz_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz\n",
        "from transformers import GPT2TokenizerFast\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "tokenizer_gpt = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer_gpt.encode(text))\n",
        "\n",
        "# Step 1: Read PDF using PyMuPDF\n",
        "def read_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extract text from the PDF file and return it as a single string.\"\"\"\n",
        "    text = \"\"\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            page = pdf.load_page(page_num)\n",
        "            text += page.get_text(\"text\")  # Extract text from the page\n",
        "    return text\n",
        "\n",
        "def process_pdfs_in_folder(folder_path: str):\n",
        "    # Initialize text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=256,  # Size of chunks (in tokens)\n",
        "        chunk_overlap=24,  # Tokens overlap between chunks\n",
        "        length_function=count_tokens,  # Function to count tokens in each chunk\n",
        "    )\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "            text = read_pdf(file_path)\n",
        "\n",
        "            # Split the text into chunks\n",
        "            chunks = text_splitter.create_documents([text])\n",
        "            chunks = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "            print(f\"Processed {file_name}: {len(chunks)} chunks\")\n",
        "            # Process chunks (for example, printing them)\n",
        "\n",
        "            vectors = embedding_fn.encode_documents(chunks)\n",
        "\n",
        "            data = [\n",
        "                {\"id\": i, \"vector\": vectors[i], \"text\": chunks[i]} for i in range(len(vectors))\n",
        "            ]\n",
        "\n",
        "            print(\"Data has\", len(data), \"entities, each with fields: \", data[0].keys())\n",
        "            print(\"Vector dim:\", len(data[0][\"vector\"]))\n",
        "            res = client.insert(collection_name=\"demo_collection\", data=data)\n",
        "            print(res)\n",
        "\n",
        "\n",
        "\n",
        "folder_path = \"./docs\"\n",
        "process_pdfs_in_folder(folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhpZiUO_C2Z-",
        "outputId": "016a1ad7-ea68-445b-9fa0-ccc6023e0d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed python-cheatsheets-final.pdf: 91 chunks\n",
            "Data has 91 entities, each with fields:  dict_keys(['id', 'vector', 'text'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 91, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ip_search([\"How to fetch specific columns from a dataframe?\"], [\"text\"], \"demo_collection\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86jr4rDeeqzI",
        "outputId": "c46298cd-ab12-4182-96ba-18e6f6520381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\">>> import pandas as pd\\nUse the following import convention:\\nPandas Data Structures\\n>>> s = pd.Series([3, -5, 7, 4], index=['a', 'b', 'c', 'd'])\\n>>> data = {'Country': ['Belgium', 'India', 'Brazil'], \\n           'Capital': ['Brussels', 'New Delhi', 'Brasília'],\\n           'Population': [11190846, 1303171035, 207847528]}\\n>>> df = pd.DataFrame(data, \\n                     columns=['Country', 'Capital', 'Population'])\\nSelection\\n>>> s['b']                       Get one element\\n  -5\\n>>> df[1:]                       Get subset of a DataFrame\", \">>> s[(s < -1) | (s > 2)]           s where value is <-1 or >2 \\n>>> df[df['Population']>1200000000] Use filter to adjust DataFrame\\n   Setting\\n>>> s['a'] = 6                         Set index a of Series s to 6\\nApplying Functions\\n>>> f = lambda x: x*2\\n>>> df.apply(f)         Apply function\\n>>> df.applymap(f)          Apply function element-wise\\nRetrieving Series/DataFrame Information\\n>>> df.shape            (rows,columns)        \\n>>> df.index\\t\\n         Describe index\\t\\n \\n>>> df.columns          Describe DataFrame columns\\n>>> df.info()           Info on DataFrame\", '>>> df.info()           Info on DataFrame\\n>>> df.count()          Number of non-NA values\\nGetting \\nAlso see NumPy Arrays\\nSelecting, Boolean Indexing & Setting\\nBasic Information\\nSummary\\n>>> df.sum()               Sum of values      \\n>>> df.cumsum()                                     Cummulative sum of values \\n>>> df.min()/df.max()      Minimum/maximum values\\n>>> df.idxmin()/df.idxmax() Minimum/Maximum index value \\n>>> df.describe()          Summary statistics\\n>>> df.mean()                    Mean of values\\n>>> df.median()            Median of values\\nDropping', \"'Belgium'                                                                     column labels\\n>>> df.at([0], ['Country'])   \\n     'Belgium'\\n   By Label/Position\\n>>> df.ix[2]                         Select single row of \\n  Country      Brazil                    subset of rows\\n  Capital    Brasília\\n  Population  207847528\\n>>> df.ix[:,'Capital']               Select a single column of\", '>>> df.to_csv(\\'myDataFrame.csv\\')\\nRead and Write to Excel\\n>>> pd.read_excel(\\'file.xlsx\\')\\n>>> pd.to_excel(\\'dir/myDataFrame.xlsx\\', sheet_name=\\'Sheet1\\')\\n   Read multiple sheets from the same file\\n>>> xlsx = pd.ExcelFile(\\'file.xls\\')\\n>>> df = pd.read_excel(xlsx, \\'Sheet1\\')\\n>>> help(pd.Series.loc)\\nAsking For Help\\nRead and Write to SQL Query or Database Table\\n>>> from sqlalchemy import create_engine\\n>>> engine = create_engine(\\'sqlite:///:memory:\\')\\n>>> pd.read_sql(\"SELECT * FROM my_table;\", engine)\\n>>> pd.read_sql_table(\\'my_table\\', engine)\\n>>> pd.read_sql_query(\"SELECT * FROM my_table;\", engine)\\n>>> pd.to_sql(\\'myDf\\', engine)\\nread_sql()is a convenience wrapper around read_sql_table() and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets tqdm 'accelerate>=0.26.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PwoT0Y2DPmz",
        "outputId": "90185e1a-d33e-4fae-942f-db5c7852e150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-7B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-Coder-7B-Instruct\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dd1e6143eb764db09aa669789c13a83f",
            "c698f9c44c0f46eca93a655541a9f434",
            "fec0c2d59f484b99a0a0672ccd9b2918",
            "253b452c48a04d8097f63e667a273d1c",
            "ed62949fae0b4de5aa622344ef2de8af",
            "fec94d7c35c04f25be65bcf32a54d271",
            "c830a10430924080a9bc7c8f5b3e34ea",
            "ccf9aed5a8d94c158c3454a2659339ef",
            "70ddf083273c4b3f97cdea59c845cdee",
            "39d8487a2bb44099811df38b08088b78",
            "6e2652d256394e6f9af0181cc3b98b09"
          ]
        },
        "id": "pdIAskEhDKPg",
        "outputId": "24870e01-5a14-4610-afed-d27a71011d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd1e6143eb764db09aa669789c13a83f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--model\",\n",
        "    type=str,\n",
        "    default=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "    help=\"which results to run\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--resume\",\n",
        "    action=\"store_true\",\n",
        "    default=False,\n",
        "    help=\"where to resume inference\",\n",
        ")\n",
        "args = parser.parse_args()\n",
        "model_name = args.model\n",
        "\n",
        "cached_cnt = 0\n",
        "if os.path.exists(f\"data/{args.model.replace('/', '-')}-answers.jsonl\"):\n",
        "    if args.resume:\n",
        "        cached_cnt = len(open(f\"data/{args.model.replace('/', '-')}-answers.jsonl\", \"r\").readlines())\n",
        "    else:\n",
        "        exit(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=\"auto\", resume_download=True, trust_remote_code=True\n",
        ")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name} is on {param.device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# Need to set the padding token to the eos token for generation\n",
        "if tokenizer.eos_token:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "else:\n",
        "    tokenizer.add_special_tokens({\n",
        "        \"pad_token\": \"<pad>\"\n",
        "    })\n",
        "\n",
        "ds1000 = list(load_dataset(\"xlangai/DS-1000\")[\"test\"])\n",
        "prompts = [p[\"prompt\"] for p in ds1000[cached_cnt:]]\n",
        "\n",
        "# You can change the batch size depending on your GPU RAM\n",
        "\n",
        "batch_size = 2\n",
        "print(\"batch size\",batch_size)\n",
        "print(\"model\", model_name)\n",
        "# We set it to 8 since it is better for some hardware. More information here\n",
        "# https://github.com/huggingface/tokenizers/issues/991\n",
        "pad_to_multiple_of = 8\n",
        "\n",
        "# Split into batches\n",
        "# We will get the following results:\n",
        "# [ [\"I would like to\", \"hello how are you\"], [ \"what is going on\", \"roses are red and\"], [ \"welcome to the hotel\"] ]\n",
        "formatted_prompts = [prompts[i: i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "\n",
        "print(formatted_prompts)"
      ],
      "metadata": {
        "id": "IsoIweTLC6vc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "8b565a97-39ba-4a01-88f0-6bc6c43cdff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--model MODEL] [--resume]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-2e7c1388-d83d-4730-a8d7-7ac6dc4ff6da.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply padding on the left since we are doing generation\n",
        "padding_side_default = tokenizer.padding_side\n",
        "tokenizer.padding_side = \"left\"\n",
        "# Tokenize each batch\n",
        "tokenized_prompts = []\n",
        "for formatted_prompt in formatted_prompts:\n",
        "    messages = [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\":  \"\"\"\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant and an expert Python data scientist with over 20 years of experience. Before answering, take a deep breath and answer based on the following guidelines:\n",
        "              1. Use the provided context if relevant\n",
        "              2. If the context doesn't help, rely on your general knowledge\n",
        "              3. Always explain your reasoning\n",
        "              4. Provide complete, working code solutions, apporiate to the context of the question\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": prompt + \"\\nRelvevant Context:\\n\" + \"\\n\\n\".join(ip_search([formatted_prompt[0]], [\"text\"], \"demo_collection\")) + \"\"\"Please provide a solution based on the above context if relevant. If the context isn't helpful, provide a solution based on your general knowledge. Include:\n",
        "- Complete working code\n",
        "- Brief explanation\n",
        "- Any important considerations\"\"\"}\n",
        "        ]\n",
        "        for prompt in formatted_prompt\n",
        "    ]\n",
        "    # Generate template text\n",
        "    text_inputs = [\n",
        "        tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages\n",
        "    ]\n",
        "    # Tokenize the formatted prompts\n",
        "    tokenized_batch = tokenizer(\n",
        "        text_inputs, return_token_type_ids=False, padding=True, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized_prompts.append(tokenized_batch)\n",
        "# Put back the original padding behavior\n",
        "tokenizer.padding_side = padding_side_default\n",
        "\n",
        "model_name = model_name.replace('/', '-')\n",
        "generation_config = {\n",
        "    \"do_sample\": False,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"num_beams\": 1\n",
        "}\n",
        "for batch in tqdm(tokenized_prompts):\n",
        "    # Move the batch to the device\n",
        "    batch = batch.to(\"cuda\")\n",
        "    prompt_len = len(batch[\"input_ids\"][0])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **batch,\n",
        "            **generation_config\n",
        "        )\n",
        "    generated_text = tokenizer.batch_decode(outputs[:, prompt_len:], skip_special_tokens=True)\n",
        "    with open(f'data/{model_name}-answers.jsonl', 'a+') as f:\n",
        "        for code in generated_text:\n",
        "            stop_words = [\"</code>\", \"# SOLUTION END\"]\n",
        "            for stop_word in stop_words:\n",
        "                code = code.split(stop_word)[0]\n",
        "            r = {\n",
        "                'id': cached_cnt,\n",
        "                'code': code,\n",
        "                'metadata': ds1000[cached_cnt]['metadata']\n",
        "            }\n",
        "            f.write(json.dumps(r) + '\\n')\n",
        "            cached_cnt += 1\n",
        "output_path = f\"data/{model_name}-answers.jsonl\"\n",
        "print(f\"Results saved in {output_path}\")"
      ],
      "metadata": {
        "id": "VZPaVYUafjCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set model name and resume behavior directly in the code\n",
        "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "resume = True  # Set to True to resume from the last cached count\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=\"auto\", resume_download=True, trust_remote_code=True\n",
        ")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name} is on {param.device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "if tokenizer.eos_token:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "else:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
        "\n",
        "# Load dataset\n",
        "ds1000 = list(load_dataset(\"xlangai/DS-1000\")[\"test\"])\n",
        "prompts = [p[\"prompt\"] for p in ds1000]\n",
        "\n",
        "# Check for cached answers if resume is enabled\n",
        "cached_cnt = 0\n",
        "#output_path = f'{model_name.replace(\"/\", \"-\")}-answers.jsonl'\n",
        "output_path=f\"./data/{model_name.replace('/', '-')}-answers.jsonl\"\n",
        "if resume and os.path.exists(output_path):\n",
        "    cached_cnt = len(open(output_path, \"r\").readlines())\n",
        "\n",
        "prompts = prompts[cached_cnt:]  # Resume from the last processed prompt if applicable\n",
        "\n",
        "# Set parameters\n",
        "batch_size = 2\n",
        "pad_to_multiple_of = 8\n",
        "\n",
        "# Split prompts into batches\n",
        "formatted_prompts = [prompts[i: i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "print(\"cached_cnt\", cached_cnt)\n"
      ],
      "metadata": {
        "id": "2aAneQqAEEM3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "34f5cd1f6909497d972c5572c19768ac",
            "0794205300ed44bb966ecf9c1499010b",
            "641b8abc876a44fd9c8651c5b787f37e",
            "d327f6fb98c740d58b6d23d5a2e3c7df",
            "6573e37b070647f8bd3524debd310593",
            "3c388b49ca3440e89fbd7360c6782837",
            "2a544b04466e4ff185fd573fa6094e8c",
            "a7843614c14c4b96924aedb7d975a112",
            "1f508566d8d2478791bfe4c88c963812",
            "db471aaaa5c94e12baffbc5ad2e72c7f",
            "18d61fd4b9d9494ebbcee65826a12ba9"
          ]
        },
        "outputId": "3780ba0f-3e9b-4f65-c1bc-35ac49b370c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34f5cd1f6909497d972c5572c19768ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.embed_tokens.weight is on cuda:0\n",
            "model.layers.0.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.0.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.0.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.0.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.0.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.0.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.0.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.0.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.0.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.0.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.0.input_layernorm.weight is on cuda:0\n",
            "model.layers.0.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.1.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.1.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.1.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.1.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.1.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.1.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.1.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.1.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.1.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.1.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.1.input_layernorm.weight is on cuda:0\n",
            "model.layers.1.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.2.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.2.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.2.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.2.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.2.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.2.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.2.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.2.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.2.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.2.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.2.input_layernorm.weight is on cuda:0\n",
            "model.layers.2.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.3.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.3.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.3.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.3.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.3.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.3.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.3.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.3.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.3.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.3.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.3.input_layernorm.weight is on cuda:0\n",
            "model.layers.3.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.4.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.4.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.4.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.4.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.4.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.4.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.4.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.4.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.4.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.4.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.4.input_layernorm.weight is on cuda:0\n",
            "model.layers.4.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.5.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.5.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.5.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.5.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.5.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.5.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.5.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.5.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.5.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.5.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.5.input_layernorm.weight is on cuda:0\n",
            "model.layers.5.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.6.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.6.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.6.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.6.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.6.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.6.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.6.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.6.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.6.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.6.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.6.input_layernorm.weight is on cuda:0\n",
            "model.layers.6.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.7.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.7.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.7.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.7.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.7.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.7.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.7.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.7.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.7.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.7.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.7.input_layernorm.weight is on cuda:0\n",
            "model.layers.7.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.8.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.8.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.8.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.8.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.8.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.8.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.8.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.8.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.8.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.8.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.8.input_layernorm.weight is on cuda:0\n",
            "model.layers.8.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.9.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.9.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.9.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.9.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.9.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.9.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.9.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.9.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.9.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.9.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.9.input_layernorm.weight is on cuda:0\n",
            "model.layers.9.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.10.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.10.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.10.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.10.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.10.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.10.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.10.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.10.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.10.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.10.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.10.input_layernorm.weight is on cuda:0\n",
            "model.layers.10.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.11.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.11.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.11.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.11.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.11.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.11.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.11.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.11.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.11.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.11.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.11.input_layernorm.weight is on cuda:0\n",
            "model.layers.11.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.12.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.12.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.12.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.12.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.12.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.12.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.12.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.12.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.12.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.12.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.12.input_layernorm.weight is on cuda:0\n",
            "model.layers.12.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.13.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.13.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.13.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.13.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.13.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.13.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.13.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.13.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.13.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.13.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.13.input_layernorm.weight is on cuda:0\n",
            "model.layers.13.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.14.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.14.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.14.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.14.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.14.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.14.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.14.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.14.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.14.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.14.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.14.input_layernorm.weight is on cuda:0\n",
            "model.layers.14.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.15.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.15.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.15.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.15.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.15.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.15.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.15.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.15.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.15.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.15.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.15.input_layernorm.weight is on cuda:0\n",
            "model.layers.15.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.16.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.16.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.16.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.16.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.16.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.16.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.16.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.16.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.16.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.16.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.16.input_layernorm.weight is on cuda:0\n",
            "model.layers.16.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.17.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.17.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.17.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.17.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.17.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.17.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.17.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.17.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.17.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.17.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.17.input_layernorm.weight is on cuda:0\n",
            "model.layers.17.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.18.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.18.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.18.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.18.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.18.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.18.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.18.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.18.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.18.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.18.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.18.input_layernorm.weight is on cuda:0\n",
            "model.layers.18.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.19.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.19.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.19.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.19.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.19.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.19.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.19.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.19.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.19.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.19.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.19.input_layernorm.weight is on cuda:0\n",
            "model.layers.19.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.20.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.20.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.20.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.20.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.20.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.20.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.20.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.20.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.20.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.20.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.20.input_layernorm.weight is on cuda:0\n",
            "model.layers.20.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.21.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.21.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.21.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.21.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.21.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.21.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.21.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.21.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.21.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.21.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.21.input_layernorm.weight is on cuda:0\n",
            "model.layers.21.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.22.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.22.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.22.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.22.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.22.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.22.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.22.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.22.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.22.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.22.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.22.input_layernorm.weight is on cuda:0\n",
            "model.layers.22.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.23.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.23.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.23.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.23.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.23.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.23.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.23.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.23.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.23.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.23.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.23.input_layernorm.weight is on cuda:0\n",
            "model.layers.23.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.24.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.24.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.24.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.24.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.24.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.24.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.24.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.24.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.24.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.24.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.24.input_layernorm.weight is on cuda:0\n",
            "model.layers.24.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.25.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.25.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.25.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.25.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.25.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.25.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.25.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.25.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.25.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.25.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.25.input_layernorm.weight is on cuda:0\n",
            "model.layers.25.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.26.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.26.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.26.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.26.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.26.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.26.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.26.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.26.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.26.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.26.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.26.input_layernorm.weight is on cuda:0\n",
            "model.layers.26.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.27.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.27.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.27.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.27.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.27.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.27.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.27.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.27.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.27.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.27.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.27.input_layernorm.weight is on cuda:0\n",
            "model.layers.27.post_attention_layernorm.weight is on cuda:0\n",
            "model.norm.weight is on cuda:0\n",
            "lm_head.weight is on cuda:0\n",
            "cached_cnt 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and process each batch\n",
        "cached_cnt=0\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenized_prompts = []\n",
        "for formatted_prompt in formatted_prompts:\n",
        "    messages = [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\":  \"\"\"\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. Before answering, take a deep breath and answer based on the following guidelines:\n",
        "              1. Use the provided context ONLY if relevant\n",
        "              2. If the context doesn't help, rely on your general knowledge\n",
        "              3. Answer the question, EXACTLY ACCORDING TO HOW IT IS ASKED, that is, only provide the neccessary code needed to add to the user's code for it to work.\n",
        "\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": prompt + \"\\n<context>\\n\" + \"\\n\\n\".join(ip_search([formatted_prompt[0]], [\"text\"], \"demo_collection\"))+ \"\\n</context>\\n\"}\n",
        "        ]\n",
        "        for prompt in formatted_prompt\n",
        "    ]\n",
        "    text_inputs = [\n",
        "        tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages\n",
        "    ]\n",
        "    tokenized_batch = tokenizer(\n",
        "        text_inputs, return_token_type_ids=False, padding=True, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\", max_length=2048\n",
        "    )\n",
        "    tokenized_prompts.append(tokenized_batch)\n",
        "tokenizer.padding_side = \"right\"  # Reset padding side\n",
        "\n",
        "# Generation configuration\n",
        "generation_config = {\n",
        "    \"do_sample\": False,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"num_beams\": 1\n",
        "}\n",
        "\n",
        "# Run inference and save outputs\n",
        "\n",
        "with open(output_path, 'a+') as f:\n",
        "    for batch in tqdm(tokenized_prompts):\n",
        "        batch = batch.to(\"cuda\")\n",
        "        prompt_len = len(batch[\"input_ids\"][0])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**batch, **generation_config)\n",
        "        generated_text = tokenizer.batch_decode(outputs[:, prompt_len:], skip_special_tokens=True)\n",
        "\n",
        "        for code in generated_text:\n",
        "            stop_words = [\"</code>\", \"# SOLUTION END\"]\n",
        "            for stop_word in stop_words:\n",
        "                code = code.split(stop_word)[0]\n",
        "            result = {\n",
        "                'id': cached_cnt,\n",
        "                'code': code,\n",
        "                'metadata': ds1000[cached_cnt]['metadata']\n",
        "            }\n",
        "            f.write(json.dumps(result) + '\\n')\n",
        "            cached_cnt += 1\n",
        "\n",
        "print(f\"Results saved in {output_path}\")\n"
      ],
      "metadata": {
        "id": "SNtN1kxzG_68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae2cf20-2b3b-4a36-bedc-d7a73f5a969e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2691: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 500/500 [38:31<00:00,  4.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved in ./data/Qwen-Qwen2.5-Coder-7B-Instruct-answers.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Sx-1yjrTXxR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GyQ5yMfnBU7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}