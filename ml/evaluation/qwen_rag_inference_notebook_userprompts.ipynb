{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3HSuXMODKv5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfSQy7oxCkKb",
        "outputId": "6b8028c6-1b66-483e-b1a1-967c5e83d12b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.11/dist-packages (2.5.5)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (75.1.0)\n",
            "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (1.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (4.25.6)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (1.0.1)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (2.2.2)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (2.4.11)\n",
            "Requirement already satisfied: pymilvus.model>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus[model]) (0.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from milvus-lite>=2.4.0->pymilvus) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2025.1)\n",
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus.model>=0.3.0->pymilvus[model]) (4.48.3)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (from pymilvus.model>=0.3.0->pymilvus[model]) (1.21.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus.model>=0.3.0->pymilvus[model]) (1.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (0.5.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (1.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (4.12.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymilvus pymilvus[model] pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ3-CVP1CsYk",
        "outputId": "bdc9bb63-ba2d-45c6-d37a-11f84bc660e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.cornstack.CodeRankEmbed.b84c9f7b3c0e693aae198b938314b55be0972c94.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ],
      "source": [
        "from pymilvus import MilvusClient\n",
        "import numpy as np\n",
        "from pymilvus import model\n",
        "\n",
        "client = MilvusClient(\"./milvus_demo18.db\")\n",
        "client.create_collection(\n",
        "    collection_name=\"demo_collection\",\n",
        "    dimension=768  # The vectors we will use in this demo has 384 dimensions\n",
        ")\n",
        "\n",
        "\n",
        "embedding_fn =  model.dense.SentenceTransformerEmbeddingFunction(\n",
        "    model_name='cornstack/CodeRankEmbed', # Specify the model name\n",
        "    device='cuda:0',\n",
        "    trust_remote_code=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JZ8nH8cbNBPH"
      },
      "outputs": [],
      "source": [
        "# modified ip search to also filter by library metadata\n",
        "\n",
        "from pymilvus import MilvusClient\n",
        "from pymilvus import model\n",
        "import re\n",
        "\n",
        "DISTANCE_THRESHOLD = 0.25\n",
        "\n",
        "# def ip_search(\n",
        "#     queries,\n",
        "#     output_field,\n",
        "#     collection_name,\n",
        "#     distance_threshold=DISTANCE_THRESHOLD,\n",
        "#     embedding_fn=embedding_fn,\n",
        "# ):\n",
        "#     \"\"\"Searches for relevant context and filters by library metadata.\"\"\"\n",
        "\n",
        "#     query_vectors = embedding_fn.encode_documents(queries)\n",
        "#     res = client.search(\n",
        "#         collection_name=collection_name,\n",
        "#         data=query_vectors,\n",
        "#         limit=5,\n",
        "#         output_fields=[\"text\", \"metadata\"],  # Retrieve metadata along with text\n",
        "#     )\n",
        "\n",
        "#     # Extract the target library from the query\n",
        "#     query_text = \" \".join(queries).lower()\n",
        "#     relevant_libraries = [\"matplotlib\", \"numpy\", \"pandas\", \"pytorch\", \"scipy\", \"sklearn\", \"tensorflow\"]\n",
        "#     detected_library = next((lib for lib in relevant_libraries if lib in query_text), None)\n",
        "\n",
        "#     filtered_results = []\n",
        "#     for table in res[0]:\n",
        "#         if table[\"distance\"] < distance_threshold:\n",
        "#             retrieved_library = table.get(\"metadata\", {}).get(\"library\", \"\").lower()\n",
        "\n",
        "#             # Ensure retrieved context matches the detected library\n",
        "#             if detected_library and retrieved_library == detected_library:\n",
        "#                 filtered_results.append(table[\"entity\"][\"text\"])\n",
        "\n",
        "#     return filtered_results if filtered_results else []\n",
        "\n",
        "\n",
        "# ip search function with keyword matching\n",
        "def ip_search(\n",
        "    queries,\n",
        "    output_field,\n",
        "    collection_name,\n",
        "    distance_threshold=0.5,\n",
        "    embedding_fn=embedding_fn,\n",
        "):\n",
        "    \"\"\"Searches for relevant context and filters by library metadata.\"\"\"\n",
        "\n",
        "    # Library keyword mappings\n",
        "    library_keywords = {\n",
        "        \"pandas\": [\"pd\", \"dataframe\", \"df\", \"iloc\", \"loc\", \"series\", \"groupby\", \"merge\", \"concat\", \"pivot\", \"melt\"],\n",
        "        \"numpy\": [\"np\", \"array\", \"ndarray\", \"linspace\", \"arange\", \"zeros\", \"ones\", \"random\", \"linalg\", \"fft\"],\n",
        "        \"matplotlib\": [\"plt\", \"pyplot\", \"figure\", \"subplot\", \"plot\", \"scatter\", \"hist\", \"bar\", \"boxplot\", \"imshow\"],\n",
        "        \"sklearn\": [\"scikit\", \"scikit-learn\", \"classifier\", \"regressor\", \"cluster\", \"pipeline\", \"grid_search\", \"cross_val\", \"metrics\"],\n",
        "        \"scipy\": [\"stats\", \"interpolate\", \"optimize\", \"signal\", \"sparse\", \"spatial\", \"integrate\", \"ode\"],\n",
        "        \"pytorch\": [\"torch\", \"nn\", \"optim\", \"cuda\", \"tensor\", \"autograd\", \"module\", \"dataloader\", \"dataset\"],\n",
        "        \"tensorflow\": [\"tf\", \"keras\", \"estimator\", \"layers\", \"variable\", \"session\", \"placeholder\", \"eager\", \"tpu\", \"gpu\"]\n",
        "    }\n",
        "\n",
        "    query_vectors = embedding_fn.encode_documents(queries)\n",
        "    res = client.search(\n",
        "        collection_name=collection_name,\n",
        "        data=query_vectors,\n",
        "        limit=5,\n",
        "        output_fields=[\"text\", \"metadata\"],\n",
        "    )\n",
        "\n",
        "    # Extract the target library from the query\n",
        "    query_text = \" \".join(queries).lower()\n",
        "    detected_libraries = set()\n",
        "\n",
        "    # Check for direct library mentions\n",
        "    relevant_libraries = list(library_keywords.keys())\n",
        "    for lib in relevant_libraries:\n",
        "        if lib in query_text:\n",
        "            detected_libraries.add(lib)\n",
        "\n",
        "    # Check for keyword matches\n",
        "    for lib, keywords in library_keywords.items():\n",
        "        for keyword in keywords:\n",
        "            # Check for whole word matches to avoid partial matches\n",
        "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', query_text):\n",
        "                detected_libraries.add(lib)\n",
        "                break\n",
        "\n",
        "    filtered_results = []\n",
        "\n",
        "    for table in res[0]:\n",
        "        if table[\"distance\"] < distance_threshold:\n",
        "            retrieved_library = table.get(\"entity\", {}).get(\"metadata\", {}).get(\"library\", \"\").lower()\n",
        "\n",
        "            # Include results if they match any detected library\n",
        "            if detected_libraries and retrieved_library in detected_libraries:\n",
        "                filtered_results.append(table[\"entity\"][\"text\"])\n",
        "\n",
        "    return filtered_results if filtered_results else []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D78t95uNBPJ",
        "outputId": "b1b69a20-b728-4b6c-f7ef-8d0b4d8d4598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sklearn.pdf: 10 chunks\n",
            "Data has 10 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 10, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
            "Processed tensorflow.pdf: 14 chunks\n",
            "Data has 14 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 14, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}\n",
            "Processed pandas.pdf: 24 chunks\n",
            "Data has 24 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 24, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]}\n",
            "Processed matplotlib.pdf: 14 chunks\n",
            "Data has 14 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 14, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}\n",
            "Processed pytorch.pdf: 19 chunks\n",
            "Data has 19 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 19, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]}\n",
            "Processed numpy.pdf: 17 chunks\n",
            "Data has 17 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 17, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}\n",
            "Processed scipy.pdf: 16 chunks\n",
            "Data has 16 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 16, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1818 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed cheatsheet: 29 chunks\n",
            "Data has 29 entities, each with fields: dict_keys(['id', 'vector', 'text', 'metadata'])\n",
            "Vector dim: 768\n",
            "{'insert_count': 29, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]}\n"
          ]
        }
      ],
      "source": [
        "# Adding library metadata when reading the files and websites\n",
        "\n",
        "import os\n",
        "import fitz\n",
        "from transformers import GPT2TokenizerFast\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "tokenizer_gpt = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer_gpt.encode(text))\n",
        "\n",
        "# Step 1: Read PDF using PyMuPDF\n",
        "def read_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extract text from the PDF file and return it as a single string.\"\"\"\n",
        "    text = \"\"\n",
        "    with fitz.open(file_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            page = pdf.load_page(page_num)\n",
        "            text += page.get_text(\"text\")  # Extract text from the page\n",
        "    return text\n",
        "\n",
        "def process_pdfs_in_folder(folder_path: str):\n",
        "    \"\"\"Process PDFs, extract text, split into chunks, and store with library metadata.\"\"\"\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=256,\n",
        "        chunk_overlap=24,\n",
        "        length_function=count_tokens,\n",
        "    )\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "            # Extract library name from filename (e.g., \"numpy.pdf\" → \"numpy\")\n",
        "            library_name = file_name.replace(\".pdf\", \"\").lower()\n",
        "\n",
        "            text = read_pdf(file_path)\n",
        "\n",
        "            # Split the text into chunks\n",
        "            chunks = text_splitter.create_documents([text])\n",
        "            chunks = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "            print(f\"Processed {file_name}: {len(chunks)} chunks\")\n",
        "\n",
        "            # Store with metadata\n",
        "            vectors = embedding_fn.encode_documents(chunks)\n",
        "\n",
        "            data = [\n",
        "                {\"id\": i, \"vector\": vectors[i], \"text\": chunks[i], \"metadata\": {\"library\": library_name}}\n",
        "                for i in range(len(vectors))\n",
        "            ]\n",
        "\n",
        "            print(\"Data has\", len(data), \"entities, each with fields: \", data[0].keys())\n",
        "            print(\"Vector dim:\", len(data[0][\"vector\"]))\n",
        "\n",
        "            res = client.insert(collection_name=\"demo_collection\", data=data)\n",
        "            print(res)\n",
        "\n",
        "\n",
        "\n",
        "# Process data from websites\n",
        "def read_webpage(url: str, class_type: str, class_name: str) -> str:\n",
        "    \"\"\"Extract text from a web page and return it as a single string.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Failed to fetch page: {url}\")\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    main_content = soup.find(class_type, class_=class_name)\n",
        "\n",
        "    if main_content:\n",
        "        return main_content.get_text(separator=\"\\n\", strip=True)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def process_webpage(url: str, page_name: str, class_type: str, class_name: str, library_name: str):\n",
        "    \"\"\"Process webpage text, split into chunks, and store with metadata.\"\"\"\n",
        "\n",
        "    text = read_webpage(url, class_type, class_name)\n",
        "\n",
        "    if not text:\n",
        "        print(f\"No content extracted from {url}\")\n",
        "        return\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=256,\n",
        "        chunk_overlap=24,\n",
        "        length_function=count_tokens,\n",
        "    )\n",
        "\n",
        "    # Split text into chunks\n",
        "    chunks = text_splitter.create_documents([text])\n",
        "    chunks = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "    print(f\"Processed {page_name}: {len(chunks)} chunks\")\n",
        "\n",
        "    vectors = embedding_fn.encode_documents(chunks)\n",
        "\n",
        "    data = [\n",
        "        {\"id\": i, \"vector\": vectors[i], \"text\": chunks[i], \"metadata\": {\"library\": library_name}}\n",
        "        for i in range(len(vectors))\n",
        "    ]\n",
        "\n",
        "    print(\"Data has\", len(data), \"entities, each with fields:\", data[0].keys())\n",
        "    print(\"Vector dim:\", len(data[0][\"vector\"]))\n",
        "\n",
        "    res = client.insert(collection_name=\"demo_collection\", data=data)\n",
        "    print(res)\n",
        "\n",
        "\n",
        "def read_csv(file_path: str):\n",
        "    \"\"\"Read CSV file and extract relevant columns.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Ensure necessary columns exist\n",
        "    required_columns = {\"QuestionTitle\", \"QuestionBody\", \"AnswerBody\"}\n",
        "    if not required_columns.issubset(df.columns):\n",
        "        raise ValueError(f\"CSV file must contain columns: {required_columns}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_csv(file_path: str):\n",
        "    \"\"\"Process CSV data and split it into text chunks.\"\"\"\n",
        "    df = read_csv(file_path)\n",
        "\n",
        "    # Initialize text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=256,  # Size of chunks (in tokens)\n",
        "        chunk_overlap=24,  # Overlap between chunks\n",
        "        length_function=count_tokens,\n",
        "    )\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Combine Question and Answer\n",
        "        combined_text = f\"Title: {row['QuestionTitle']}\\n\\nQuestion: {row['QuestionBody']}\\n\\nAnswer: {row['AnswerBody']}\"\n",
        "\n",
        "        # Split the text into chunks\n",
        "        chunks = text_splitter.create_documents([combined_text])\n",
        "        chunks = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "    print(f\"Processed {len(df)} Q&A pairs into {len(all_chunks)} chunks\")\n",
        "\n",
        "    # Embed and insert into vector store\n",
        "    vectors = embedding_fn.encode_documents(all_chunks)\n",
        "\n",
        "    data = [\n",
        "        {\"id\": i, \"vector\": vectors[i], \"text\": all_chunks[i]} for i in range(len(vectors))\n",
        "    ]\n",
        "\n",
        "    print(\"Data has\", len(data), \"entities, each with fields:\", data[0].keys())\n",
        "    print(\"Vector dim:\", len(data[0][\"vector\"]))\n",
        "\n",
        "    res = client.insert(collection_name=\"demo_collection\", data=data)\n",
        "    print(res)\n",
        "\n",
        "\n",
        "# csv_file_path = \"./docs/sklearn_stackoverflow.csv\"\n",
        "# process_csv(csv_file_path)\n",
        "\n",
        "\n",
        "folder_path = \"./docs\"\n",
        "process_pdfs_in_folder(folder_path)\n",
        "\n",
        "url_dict = {\n",
        "    \"https://ipgp.github.io/scientific_python_cheat_sheet/?utm_content=buffer7d821&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer#numpy-import-numpy-as-np\": [\"cheatsheet\", \"section\", \"main-content\", \"numpy\"]\n",
        "}\n",
        "for url, (page_name, class_type, class_name, library_name) in url_dict.items():\n",
        "    process_webpage(url, page_name, class_type, class_name, library_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86jr4rDeeqzI",
        "outputId": "178a3d77-3e56-4801-d754-e4182333d748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Applying Functions\\n>>> f = lambda x: x*2\\n>>> df.apply(f)         Apply function\\n>>> df.applymap(f)          Apply function element-wise\\nRetrieving Series/DataFrame Information\\n>>> df.shape            (rows,columns)        \\n>>> df.index\\t\\n         Describe index\\t\\n \\n>>> df.columns          Describe DataFrame columns\\n>>> df.info()           Info on DataFrame\\n>>> df.count()          Number of non-NA values\\nGetting \\nAlso see NumPy Arrays\\nSelecting, Boolean Indexing & Setting\\nBasic Information\\nSummary\\n>>> df.sum()               Sum of values']\n"
          ]
        }
      ],
      "source": [
        "print(ip_search([\"How to fetch specific columns from a dataframe?\"], [\"text\"], \"demo_collection\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PwoT0Y2DPmz",
        "outputId": "8ca1af23-58b6-4e82-9b35-c97c33d100b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets tqdm 'accelerate>=0.26.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "72c0181e5b8a48d3b59c263c2e9b8f1b",
            "1e7ab4b2d3a04860bf395f7b32312436",
            "356ec5ff3eba4c218591d0a03c737973",
            "39ddefd341a446349db4662387a36ac8",
            "6117840eef03464aa5b9075bd7f27afd",
            "698a90e6cadd454ca2568ce9b5270b76",
            "4083184fdb4b41dfb680c3bbc4dd9858",
            "c429179e2f3f489b8fbae253e26304ea",
            "65161d1caa544787bc70236f68fc3215",
            "de77a196421845d7822ef020edbaf459",
            "b5b23ea1b957409cb29e7a50cafcf746"
          ]
        },
        "id": "pdIAskEhDKPg",
        "outputId": "18a84b21-7715-4fff-97bd-90435865dbff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72c0181e5b8a48d3b59c263c2e9b8f1b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-7B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-Coder-7B-Instruct\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--model\",\n",
        "    type=str,\n",
        "    default=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "    help=\"which results to run\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--resume\",\n",
        "    action=\"store_true\",\n",
        "    default=False,\n",
        "    help=\"where to resume inference\",\n",
        ")\n",
        "\n",
        "# Adding this line to prevent SystemExit:2\n",
        "parser.add_argument('-f')\n",
        "\n",
        "args = parser.parse_args()\n",
        "model_name = args.model\n",
        "\n",
        "cached_cnt = 0\n",
        "if os.path.exists(f\"data/{args.model.replace('/', '-')}-answers.jsonl\"):\n",
        "    if args.resume:\n",
        "        cached_cnt = len(open(f\"data/{args.model.replace('/', '-')}-answers.jsonl\", \"r\").readlines())\n",
        "    else:\n",
        "        exit(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=\"auto\", resume_download=True, trust_remote_code=True\n",
        ")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name} is on {param.device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# Need to set the padding token to the eos token for generation\n",
        "if tokenizer.eos_token:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "else:\n",
        "    tokenizer.add_special_tokens({\n",
        "        \"pad_token\": \"<pad>\"\n",
        "    })\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bda2e612b3ef4b03a87f6e5528fd88ab",
            "befc1de95cde4e29a161f7b8cd8e9a49",
            "49b4a785a32243f8b0faed23f491b33e",
            "872ced7b880441d790d5bae28618c14b",
            "eec9506f9b224ad3bae4942d42080b63",
            "984944cb6fc1419a9f93050d7f3bcc9c",
            "4d974079a91f4cb3a9d0c6d34bcbc90f",
            "6fd8142671d946bcaf39ea76e515971a",
            "fba3671c47ba489f9f90548ada39bb2b",
            "b50fe291259741f69face03eebe016f8",
            "ade4734d826a456791a00c7c1f1dd82f"
          ]
        },
        "id": "bWdjMzlbW0z3",
        "outputId": "93e26a75-7d5b-46a8-be6d-a8032db2482c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bda2e612b3ef4b03a87f6e5528fd88ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.embed_tokens.weight is on cuda:0\n",
            "model.layers.0.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.0.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.0.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.0.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.0.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.0.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.0.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.0.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.0.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.0.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.0.input_layernorm.weight is on cuda:0\n",
            "model.layers.0.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.1.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.1.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.1.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.1.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.1.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.1.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.1.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.1.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.1.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.1.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.1.input_layernorm.weight is on cuda:0\n",
            "model.layers.1.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.2.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.2.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.2.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.2.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.2.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.2.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.2.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.2.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.2.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.2.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.2.input_layernorm.weight is on cuda:0\n",
            "model.layers.2.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.3.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.3.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.3.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.3.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.3.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.3.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.3.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.3.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.3.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.3.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.3.input_layernorm.weight is on cuda:0\n",
            "model.layers.3.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.4.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.4.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.4.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.4.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.4.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.4.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.4.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.4.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.4.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.4.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.4.input_layernorm.weight is on cuda:0\n",
            "model.layers.4.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.5.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.5.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.5.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.5.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.5.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.5.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.5.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.5.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.5.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.5.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.5.input_layernorm.weight is on cuda:0\n",
            "model.layers.5.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.6.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.6.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.6.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.6.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.6.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.6.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.6.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.6.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.6.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.6.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.6.input_layernorm.weight is on cuda:0\n",
            "model.layers.6.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.7.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.7.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.7.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.7.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.7.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.7.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.7.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.7.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.7.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.7.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.7.input_layernorm.weight is on cuda:0\n",
            "model.layers.7.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.8.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.8.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.8.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.8.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.8.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.8.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.8.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.8.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.8.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.8.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.8.input_layernorm.weight is on cuda:0\n",
            "model.layers.8.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.9.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.9.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.9.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.9.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.9.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.9.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.9.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.9.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.9.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.9.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.9.input_layernorm.weight is on cuda:0\n",
            "model.layers.9.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.10.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.10.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.10.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.10.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.10.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.10.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.10.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.10.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.10.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.10.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.10.input_layernorm.weight is on cuda:0\n",
            "model.layers.10.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.11.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.11.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.11.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.11.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.11.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.11.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.11.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.11.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.11.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.11.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.11.input_layernorm.weight is on cuda:0\n",
            "model.layers.11.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.12.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.12.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.12.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.12.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.12.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.12.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.12.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.12.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.12.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.12.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.12.input_layernorm.weight is on cuda:0\n",
            "model.layers.12.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.13.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.13.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.13.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.13.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.13.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.13.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.13.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.13.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.13.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.13.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.13.input_layernorm.weight is on cuda:0\n",
            "model.layers.13.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.14.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.14.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.14.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.14.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.14.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.14.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.14.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.14.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.14.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.14.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.14.input_layernorm.weight is on cuda:0\n",
            "model.layers.14.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.15.self_attn.q_proj.weight is on cuda:0\n",
            "model.layers.15.self_attn.q_proj.bias is on cuda:0\n",
            "model.layers.15.self_attn.k_proj.weight is on cuda:0\n",
            "model.layers.15.self_attn.k_proj.bias is on cuda:0\n",
            "model.layers.15.self_attn.v_proj.weight is on cuda:0\n",
            "model.layers.15.self_attn.v_proj.bias is on cuda:0\n",
            "model.layers.15.self_attn.o_proj.weight is on cuda:0\n",
            "model.layers.15.mlp.gate_proj.weight is on cuda:0\n",
            "model.layers.15.mlp.up_proj.weight is on cuda:0\n",
            "model.layers.15.mlp.down_proj.weight is on cuda:0\n",
            "model.layers.15.input_layernorm.weight is on cuda:0\n",
            "model.layers.15.post_attention_layernorm.weight is on cuda:0\n",
            "model.layers.16.self_attn.q_proj.weight is on meta\n",
            "model.layers.16.self_attn.q_proj.bias is on meta\n",
            "model.layers.16.self_attn.k_proj.weight is on meta\n",
            "model.layers.16.self_attn.k_proj.bias is on meta\n",
            "model.layers.16.self_attn.v_proj.weight is on meta\n",
            "model.layers.16.self_attn.v_proj.bias is on meta\n",
            "model.layers.16.self_attn.o_proj.weight is on meta\n",
            "model.layers.16.mlp.gate_proj.weight is on meta\n",
            "model.layers.16.mlp.up_proj.weight is on meta\n",
            "model.layers.16.mlp.down_proj.weight is on meta\n",
            "model.layers.16.input_layernorm.weight is on meta\n",
            "model.layers.16.post_attention_layernorm.weight is on meta\n",
            "model.layers.17.self_attn.q_proj.weight is on meta\n",
            "model.layers.17.self_attn.q_proj.bias is on meta\n",
            "model.layers.17.self_attn.k_proj.weight is on meta\n",
            "model.layers.17.self_attn.k_proj.bias is on meta\n",
            "model.layers.17.self_attn.v_proj.weight is on meta\n",
            "model.layers.17.self_attn.v_proj.bias is on meta\n",
            "model.layers.17.self_attn.o_proj.weight is on meta\n",
            "model.layers.17.mlp.gate_proj.weight is on meta\n",
            "model.layers.17.mlp.up_proj.weight is on meta\n",
            "model.layers.17.mlp.down_proj.weight is on meta\n",
            "model.layers.17.input_layernorm.weight is on meta\n",
            "model.layers.17.post_attention_layernorm.weight is on meta\n",
            "model.layers.18.self_attn.q_proj.weight is on meta\n",
            "model.layers.18.self_attn.q_proj.bias is on meta\n",
            "model.layers.18.self_attn.k_proj.weight is on meta\n",
            "model.layers.18.self_attn.k_proj.bias is on meta\n",
            "model.layers.18.self_attn.v_proj.weight is on meta\n",
            "model.layers.18.self_attn.v_proj.bias is on meta\n",
            "model.layers.18.self_attn.o_proj.weight is on meta\n",
            "model.layers.18.mlp.gate_proj.weight is on meta\n",
            "model.layers.18.mlp.up_proj.weight is on meta\n",
            "model.layers.18.mlp.down_proj.weight is on meta\n",
            "model.layers.18.input_layernorm.weight is on meta\n",
            "model.layers.18.post_attention_layernorm.weight is on meta\n",
            "model.layers.19.self_attn.q_proj.weight is on meta\n",
            "model.layers.19.self_attn.q_proj.bias is on meta\n",
            "model.layers.19.self_attn.k_proj.weight is on meta\n",
            "model.layers.19.self_attn.k_proj.bias is on meta\n",
            "model.layers.19.self_attn.v_proj.weight is on meta\n",
            "model.layers.19.self_attn.v_proj.bias is on meta\n",
            "model.layers.19.self_attn.o_proj.weight is on meta\n",
            "model.layers.19.mlp.gate_proj.weight is on meta\n",
            "model.layers.19.mlp.up_proj.weight is on meta\n",
            "model.layers.19.mlp.down_proj.weight is on meta\n",
            "model.layers.19.input_layernorm.weight is on meta\n",
            "model.layers.19.post_attention_layernorm.weight is on meta\n",
            "model.layers.20.self_attn.q_proj.weight is on meta\n",
            "model.layers.20.self_attn.q_proj.bias is on meta\n",
            "model.layers.20.self_attn.k_proj.weight is on meta\n",
            "model.layers.20.self_attn.k_proj.bias is on meta\n",
            "model.layers.20.self_attn.v_proj.weight is on meta\n",
            "model.layers.20.self_attn.v_proj.bias is on meta\n",
            "model.layers.20.self_attn.o_proj.weight is on meta\n",
            "model.layers.20.mlp.gate_proj.weight is on meta\n",
            "model.layers.20.mlp.up_proj.weight is on meta\n",
            "model.layers.20.mlp.down_proj.weight is on meta\n",
            "model.layers.20.input_layernorm.weight is on meta\n",
            "model.layers.20.post_attention_layernorm.weight is on meta\n",
            "model.layers.21.self_attn.q_proj.weight is on meta\n",
            "model.layers.21.self_attn.q_proj.bias is on meta\n",
            "model.layers.21.self_attn.k_proj.weight is on meta\n",
            "model.layers.21.self_attn.k_proj.bias is on meta\n",
            "model.layers.21.self_attn.v_proj.weight is on meta\n",
            "model.layers.21.self_attn.v_proj.bias is on meta\n",
            "model.layers.21.self_attn.o_proj.weight is on meta\n",
            "model.layers.21.mlp.gate_proj.weight is on meta\n",
            "model.layers.21.mlp.up_proj.weight is on meta\n",
            "model.layers.21.mlp.down_proj.weight is on meta\n",
            "model.layers.21.input_layernorm.weight is on meta\n",
            "model.layers.21.post_attention_layernorm.weight is on meta\n",
            "model.layers.22.self_attn.q_proj.weight is on meta\n",
            "model.layers.22.self_attn.q_proj.bias is on meta\n",
            "model.layers.22.self_attn.k_proj.weight is on meta\n",
            "model.layers.22.self_attn.k_proj.bias is on meta\n",
            "model.layers.22.self_attn.v_proj.weight is on meta\n",
            "model.layers.22.self_attn.v_proj.bias is on meta\n",
            "model.layers.22.self_attn.o_proj.weight is on meta\n",
            "model.layers.22.mlp.gate_proj.weight is on meta\n",
            "model.layers.22.mlp.up_proj.weight is on meta\n",
            "model.layers.22.mlp.down_proj.weight is on meta\n",
            "model.layers.22.input_layernorm.weight is on meta\n",
            "model.layers.22.post_attention_layernorm.weight is on meta\n",
            "model.layers.23.self_attn.q_proj.weight is on meta\n",
            "model.layers.23.self_attn.q_proj.bias is on meta\n",
            "model.layers.23.self_attn.k_proj.weight is on meta\n",
            "model.layers.23.self_attn.k_proj.bias is on meta\n",
            "model.layers.23.self_attn.v_proj.weight is on meta\n",
            "model.layers.23.self_attn.v_proj.bias is on meta\n",
            "model.layers.23.self_attn.o_proj.weight is on meta\n",
            "model.layers.23.mlp.gate_proj.weight is on meta\n",
            "model.layers.23.mlp.up_proj.weight is on meta\n",
            "model.layers.23.mlp.down_proj.weight is on meta\n",
            "model.layers.23.input_layernorm.weight is on meta\n",
            "model.layers.23.post_attention_layernorm.weight is on meta\n",
            "model.layers.24.self_attn.q_proj.weight is on meta\n",
            "model.layers.24.self_attn.q_proj.bias is on meta\n",
            "model.layers.24.self_attn.k_proj.weight is on meta\n",
            "model.layers.24.self_attn.k_proj.bias is on meta\n",
            "model.layers.24.self_attn.v_proj.weight is on meta\n",
            "model.layers.24.self_attn.v_proj.bias is on meta\n",
            "model.layers.24.self_attn.o_proj.weight is on meta\n",
            "model.layers.24.mlp.gate_proj.weight is on meta\n",
            "model.layers.24.mlp.up_proj.weight is on meta\n",
            "model.layers.24.mlp.down_proj.weight is on meta\n",
            "model.layers.24.input_layernorm.weight is on meta\n",
            "model.layers.24.post_attention_layernorm.weight is on meta\n",
            "model.layers.25.self_attn.q_proj.weight is on meta\n",
            "model.layers.25.self_attn.q_proj.bias is on meta\n",
            "model.layers.25.self_attn.k_proj.weight is on meta\n",
            "model.layers.25.self_attn.k_proj.bias is on meta\n",
            "model.layers.25.self_attn.v_proj.weight is on meta\n",
            "model.layers.25.self_attn.v_proj.bias is on meta\n",
            "model.layers.25.self_attn.o_proj.weight is on meta\n",
            "model.layers.25.mlp.gate_proj.weight is on meta\n",
            "model.layers.25.mlp.up_proj.weight is on meta\n",
            "model.layers.25.mlp.down_proj.weight is on meta\n",
            "model.layers.25.input_layernorm.weight is on meta\n",
            "model.layers.25.post_attention_layernorm.weight is on meta\n",
            "model.layers.26.self_attn.q_proj.weight is on meta\n",
            "model.layers.26.self_attn.q_proj.bias is on meta\n",
            "model.layers.26.self_attn.k_proj.weight is on meta\n",
            "model.layers.26.self_attn.k_proj.bias is on meta\n",
            "model.layers.26.self_attn.v_proj.weight is on meta\n",
            "model.layers.26.self_attn.v_proj.bias is on meta\n",
            "model.layers.26.self_attn.o_proj.weight is on meta\n",
            "model.layers.26.mlp.gate_proj.weight is on meta\n",
            "model.layers.26.mlp.up_proj.weight is on meta\n",
            "model.layers.26.mlp.down_proj.weight is on meta\n",
            "model.layers.26.input_layernorm.weight is on meta\n",
            "model.layers.26.post_attention_layernorm.weight is on meta\n",
            "model.layers.27.self_attn.q_proj.weight is on meta\n",
            "model.layers.27.self_attn.q_proj.bias is on meta\n",
            "model.layers.27.self_attn.k_proj.weight is on meta\n",
            "model.layers.27.self_attn.k_proj.bias is on meta\n",
            "model.layers.27.self_attn.v_proj.weight is on meta\n",
            "model.layers.27.self_attn.v_proj.bias is on meta\n",
            "model.layers.27.self_attn.o_proj.weight is on meta\n",
            "model.layers.27.mlp.gate_proj.weight is on meta\n",
            "model.layers.27.mlp.up_proj.weight is on meta\n",
            "model.layers.27.mlp.down_proj.weight is on meta\n",
            "model.layers.27.input_layernorm.weight is on meta\n",
            "model.layers.27.post_attention_layernorm.weight is on meta\n",
            "model.norm.weight is on meta\n",
            "lm_head.weight is on meta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IsoIweTLC6vc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "test_prompts = [\n",
        "    {\n",
        "        \"payload\": {\n",
        "            \"message_content\": \"How do I create a pandas DataFrame?\",\n",
        "            \"error_info\": \"not available\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"payload\": {\n",
        "            \"message_content\": \"How do I plot using matplotlib?\",\n",
        "            \"error_info\": \"Error count: 1, Error description: ModuleNotFoundError: No module named 'matplotlib'\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "test_file = \"test_queries.json\"\n",
        "with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(test_prompts, f, indent=2)\n",
        "\n",
        "# 3. Load the test data\n",
        "with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "batch_size = 4\n",
        "pad_to_multiple_of = 8\n",
        "formatted_prompts = [test_data[i:i+batch_size] for i in range(0, len(test_data), batch_size)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZPaVYUafjCE"
      },
      "outputs": [],
      "source": [
        "# Apply padding on the left since we are doing generation\n",
        "padding_side_default = tokenizer.padding_side\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Tokenize each batch\n",
        "tokenized_prompts = []\n",
        "for batch in formatted_prompts:\n",
        "    messages = [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"\"\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant and an expert Python data scientist. Before answering, take a deep breath and follow these guidelines:\n",
        "              1. **Understand the question fully** before responding.\n",
        "              2. **Check if retrieved context is relevant** to the question.\n",
        "                  - If it contains useful functions, libraries, or examples, use them.\n",
        "                  - If it is unrelated or incorrect, **rely on general knowledge** instead.\n",
        "              3. **Provide a clear explanation first**, before giving the code.\n",
        "              4. **Ensure correctness, completeness, and efficiency** in the code.\n",
        "              5. **Double-check variable names and constraints** before providing the answer.\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": query[\"payload\"][\"message_content\"] +\n",
        "                                       \"\\n<context>\\n\" +\n",
        "                                       \"\\n\\n\".join(ip_search([query[\"payload\"][\"message_content\"]], [\"text\"], \"demo_collection\")) +\n",
        "                                       \"\\n</context>\\n<error_info>\" +\n",
        "                                       query[\"payload\"][\"error_info\"] + \"</error_info>\"}\n",
        "        ]\n",
        "        for query in batch\n",
        "    ]\n",
        "    for query in batch:\n",
        "        print(query[\"payload\"][\"message_content\"])\n",
        "        print(query[\"payload\"][\"error_info\"])\n",
        "    # Generate template text\n",
        "    text_inputs = [\n",
        "        tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages\n",
        "    ]\n",
        "\n",
        "    # Tokenize the formatted prompts\n",
        "    tokenized_batch = tokenizer(\n",
        "        text_inputs, return_token_type_ids=False, padding=True, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized_prompts.append(tokenized_batch)\n",
        "\n",
        "# Restore the original padding behavior\n",
        "tokenizer.padding_side = padding_side_default\n",
        "model_name = model_name.replace('/', '-')\n",
        "\n",
        "generation_config = {\n",
        "    \"do_sample\": False,\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"num_beams\": 1\n",
        "}\n",
        "\n",
        "print(\"\\n=== GENERATED RESPONSES ===\\n\")\n",
        "for batch in tqdm(tokenized_prompts):\n",
        "    # Move the batch to the device\n",
        "    batch = batch.to(\"cuda\")\n",
        "    prompt_len = len(batch[\"input_ids\"][0])\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **batch,\n",
        "            **generation_config\n",
        "        )\n",
        "    generated_text = tokenizer.batch_decode(outputs[:, prompt_len:], skip_special_tokens=True)\n",
        "    for i, code in enumerate(generated_text):\n",
        "        stop_words = [\"</code>\", \"# SOLUTION END\"]\n",
        "        for stop_word in stop_words:\n",
        "            code = code.split(stop_word)[0]\n",
        "        print(f\"\\n🔹 **Prompt {cached_cnt + i}**\")\n",
        "        print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
        "        print(f\"📌 **Generated Code:**\\n{code}\")\n",
        "        print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
        "    cached_cnt += len(generated_text)  # Update count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aAneQqAEEM3"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import json\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from datasets import load_dataset\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Set model name and resume behavior directly in the code\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "# resume = True  # Set to True to resume from the last cached count\n",
        "\n",
        "# # Initialize the model and tokenizer\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name, device_map=\"auto\", resume_download=True, trust_remote_code=True\n",
        "# )\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"{name} is on {param.device}\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# if tokenizer.eos_token:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "# else:\n",
        "#     tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
        "\n",
        "# # Load dataset\n",
        "# ds1000 = list(load_dataset(\"xlangai/DS-1000\")[\"test\"])\n",
        "# prompts = [p[\"prompt\"] for p in ds1000]\n",
        "\n",
        "# # Check for cached answers if resume is enabled\n",
        "# cached_cnt = 0\n",
        "# #output_path = f'{model_name.replace(\"/\", \"-\")}-answers.jsonl'\n",
        "# output_path=f\"./data/{model_name.replace('/', '-')}-answers.jsonl\"\n",
        "# if resume and os.path.exists(output_path):\n",
        "#     cached_cnt = len(open(output_path, \"r\").readlines())\n",
        "\n",
        "# prompts = prompts[cached_cnt:]  # Resume from the last processed prompt if applicable\n",
        "\n",
        "# # Set parameters\n",
        "# batch_size = 2\n",
        "# pad_to_multiple_of = 8\n",
        "\n",
        "# # Split prompts into batches\n",
        "# formatted_prompts = [prompts[i: i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "# print(\"cached_cnt\", cached_cnt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNtN1kxzG_68"
      },
      "outputs": [],
      "source": [
        "# # Tokenize and process each batch\n",
        "# cached_cnt=0\n",
        "# tokenizer.padding_side = \"left\"\n",
        "# tokenized_prompts = []\n",
        "# for formatted_prompt in formatted_prompts:\n",
        "#     messages = [\n",
        "#         [\n",
        "#             {\"role\": \"system\", \"content\":  \"\"\"\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. Before answering, take a deep breath and answer based on the following guidelines:\n",
        "#               1. Use the provided context ONLY if relevant\n",
        "#               2. If the context doesn't help, rely on your general knowledge\n",
        "#               3. Answer the question, EXACTLY ACCORDING TO HOW IT IS ASKED, that is, only provide the neccessary code needed to add to the user's code for it to work.\n",
        "#               4, DO NOT give any explanation, only the code is required. For example, \"Problem:\\nHow do I get the dimensions of an array? For instance, this is (2, 2):\\na = np.array([[1,2],[3,4]])\\n\\nA:\\n<code>\\nimport numpy as np\\na = np.array([[1,2],[3,4]])\\n</code>\\nresult = ... \", your answer should be \"result = a.shape\".\n",
        "\n",
        "#               \"\"\"},\n",
        "#             {\"role\": \"user\", \"content\": prompt + \"\\n<context>\\n\" + \"\\n\\n\".join(ip_search([formatted_prompt[0]], [\"text\"], \"demo_collection\"))+ \"\\n</context>\\n\"}\n",
        "#         ]\n",
        "#         for prompt in formatted_prompt\n",
        "#     ]\n",
        "#     text_inputs = [\n",
        "#         tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages\n",
        "#     ]\n",
        "#     tokenized_batch = tokenizer(\n",
        "#         text_inputs, return_token_type_ids=False, padding=True, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\", max_length=2048\n",
        "#     )\n",
        "#     tokenized_prompts.append(tokenized_batch)\n",
        "# tokenizer.padding_side = \"right\"  # Reset padding side\n",
        "\n",
        "# # Generation configuration\n",
        "# generation_config = {\n",
        "#     \"do_sample\": False,\n",
        "#     \"max_new_tokens\": 1024,\n",
        "#     \"num_beams\": 1\n",
        "# }\n",
        "\n",
        "# # Run inference and save outputs\n",
        "\n",
        "# with open(output_path, 'a+') as f:\n",
        "#     for batch in tqdm(tokenized_prompts):\n",
        "#         batch = batch.to(\"cuda\")\n",
        "#         prompt_len = len(batch[\"input_ids\"][0])\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model.generate(**batch, **generation_config)\n",
        "#         generated_text = tokenizer.batch_decode(outputs[:, prompt_len:], skip_special_tokens=True)\n",
        "\n",
        "#         for code in generated_text:\n",
        "#             stop_words = [\"</code>\", \"# SOLUTION END\"]\n",
        "#             for stop_word in stop_words:\n",
        "#                 code = code.split(stop_word)[0]\n",
        "#             result = {\n",
        "#                 'id': cached_cnt,\n",
        "#                 'code': code,\n",
        "#                 'metadata': ds1000[cached_cnt]['metadata']\n",
        "#             }\n",
        "#             f.write(json.dumps(result) + '\\n')\n",
        "#             cached_cnt += 1\n",
        "\n",
        "# print(f\"Results saved in {output_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "72c0181e5b8a48d3b59c263c2e9b8f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e7ab4b2d3a04860bf395f7b32312436",
              "IPY_MODEL_356ec5ff3eba4c218591d0a03c737973",
              "IPY_MODEL_39ddefd341a446349db4662387a36ac8"
            ],
            "layout": "IPY_MODEL_6117840eef03464aa5b9075bd7f27afd"
          }
        },
        "1e7ab4b2d3a04860bf395f7b32312436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_698a90e6cadd454ca2568ce9b5270b76",
            "placeholder": "​",
            "style": "IPY_MODEL_4083184fdb4b41dfb680c3bbc4dd9858",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "356ec5ff3eba4c218591d0a03c737973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c429179e2f3f489b8fbae253e26304ea",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65161d1caa544787bc70236f68fc3215",
            "value": 4
          }
        },
        "39ddefd341a446349db4662387a36ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de77a196421845d7822ef020edbaf459",
            "placeholder": "​",
            "style": "IPY_MODEL_b5b23ea1b957409cb29e7a50cafcf746",
            "value": " 4/4 [01:15&lt;00:00, 16.03s/it]"
          }
        },
        "6117840eef03464aa5b9075bd7f27afd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "698a90e6cadd454ca2568ce9b5270b76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4083184fdb4b41dfb680c3bbc4dd9858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c429179e2f3f489b8fbae253e26304ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65161d1caa544787bc70236f68fc3215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de77a196421845d7822ef020edbaf459": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5b23ea1b957409cb29e7a50cafcf746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bda2e612b3ef4b03a87f6e5528fd88ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_befc1de95cde4e29a161f7b8cd8e9a49",
              "IPY_MODEL_49b4a785a32243f8b0faed23f491b33e",
              "IPY_MODEL_872ced7b880441d790d5bae28618c14b"
            ],
            "layout": "IPY_MODEL_eec9506f9b224ad3bae4942d42080b63"
          }
        },
        "befc1de95cde4e29a161f7b8cd8e9a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_984944cb6fc1419a9f93050d7f3bcc9c",
            "placeholder": "​",
            "style": "IPY_MODEL_4d974079a91f4cb3a9d0c6d34bcbc90f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "49b4a785a32243f8b0faed23f491b33e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fd8142671d946bcaf39ea76e515971a",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fba3671c47ba489f9f90548ada39bb2b",
            "value": 4
          }
        },
        "872ced7b880441d790d5bae28618c14b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b50fe291259741f69face03eebe016f8",
            "placeholder": "​",
            "style": "IPY_MODEL_ade4734d826a456791a00c7c1f1dd82f",
            "value": " 4/4 [01:20&lt;00:00, 16.82s/it]"
          }
        },
        "eec9506f9b224ad3bae4942d42080b63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "984944cb6fc1419a9f93050d7f3bcc9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d974079a91f4cb3a9d0c6d34bcbc90f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fd8142671d946bcaf39ea76e515971a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba3671c47ba489f9f90548ada39bb2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b50fe291259741f69face03eebe016f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ade4734d826a456791a00c7c1f1dd82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}